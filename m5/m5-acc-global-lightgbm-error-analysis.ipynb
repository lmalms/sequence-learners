{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                },
                {
                    "sourceId": 9669001,
                    "sourceType": "datasetVersion",
                    "datasetId": 5684895
                }
            ],
            "dockerImageVersionId": 30746,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import json\nimport gc\nimport math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Constants\n\n# Paths\nINPUT_BASE_PATH = \"/kaggle/input/\"\nRAW_DATA_INPUT_PATH = f\"{INPUT_BASE_PATH}/m5-forecasting-accuracy\"\nPROCESSED_DATA_INPUT_PATH = f\"{INPUT_BASE_PATH}/m5-acc\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\n# Timestamps\nMAX_TRAIN_TIMESTAMP = 1941\nSTART_TEST_TIMESTAMP = 1942\nSTART_TEST_WM_YR_WK = 11617",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Data Input",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def cast_category_types(dataset: pd.DataFrame) -> pd.DataFrame:\n    item_category_cols = [\"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    date_category_cols = [\"weekday\", \"month\", \"quarter\", \"year\"]\n    event_cols = [\n        \"event_name\", \"event_type\",\n        \"event_name_lag_-3\", \"event_type_lag_-3\",\n        \"event_name_lag_-2\", \"event_type_lag_-2\",\n        \"event_name_lag_-1\", \"event_type_lag_-1\",\n        \"event_name_lag_1\", \"event_type_lag_1\",\n        \"event_name_lag_2\", \"event_type_lag_2\",\n        \"event_name_lag_3\", \"event_type_lag_3\",\n        \"snap_CA\", \"snap_TX\", \"snap_WI\"\n    ]\n    sale_cols = [\"item_on_sale\"]\n    \n    all_cat_cols = item_category_cols + date_category_cols + event_cols + sale_cols\n    for category_col in all_cat_cols:\n        try:\n            dataset[category_col] = dataset[category_col].astype(\"category\")\n        except KeyError:\n            # print(f\"Column {category_col} does not exist. Skipping ...\")\n            pass\n    \n    return dataset\n\n\ndef cast_int_types(dataset: pd.DataFrame) -> pd.DataFrame:\n    int_cols = [\"d\", \"count\"]\n    for int_col in int_cols:\n        try:\n            dataset[int_col] = dataset[int_col].astype(np.int16)\n        except KeyError:\n            # print(f\"Column {int_col} does not exist. Skipping ...\")\n            pass\n    \n    return dataset\n\n\ndef cast_float_types(dataset: pd.DataFrame) -> pd.DataFrame:\n    item_id_cols = [\"item_id\"]\n    float_sales_cols = [\n        \"count_lag_28\", \"count_lag_29\", \"count_lag_30\", \"count_lag_31\",\n        \"count_lag_28_rolling_mean_window_7\", \"count_lag_28_rolling_std_window_7\", \"count_lag_28_rolling_kurt_window_7\",\n        \"count_lag_28_rolling_mean_window_14\", \"count_lag_28_rolling_std_window_14\", \"count_lag_28_rolling_kurt_window_14\",\n        \"count_lag_28_rolling_mean_window_21\", \"count_lag_28_rolling_std_window_21\", \"count_lag_28_rolling_kurt_window_21\",\n        \"count_lag_28_rolling_mean_window_28\", \"count_lag_28_rolling_std_window_28\", \"count_lag_28_rolling_kurt_window_28\",\n    ]\n    float_price_cols = [\n        \"sell_price\",\n        \"sell_price_diff_1\", \"sell_price_diff_2\", \"sell_price_diff_3\", \"sell_price_diff_7\",\n        \"sell_price_rolling_mean_window_7\", \"sell_price_rolling_std_window_7\", \"sell_price_rolling_kurt_window_7\",\n        \"sell_price_rolling_mean_window_14\", \"sell_price_rolling_std_window_14\", \"sell_price_rolling_kurt_window_14\",\n        \"sell_price_rolling_mean_window_21\", \"sell_price_rolling_std_window_21\", \"sell_price_rolling_kurt_window_21\",\n        \"sell_price_rolling_mean_window_28\", \"sell_price_rolling_std_window_28\", \"sell_price_rolling_kurt_window_28\",\n    ]\n    float_date_cols = [\n        \"weekday_sin\", \"weekday_cos\",\n        \"month_sin\", \"month_cos\",\n        \"quarter_sin\", \"quarter_cos\",\n    ]\n    float_cols = item_id_cols + float_sales_cols + float_price_cols + float_date_cols\n    for float_col in float_cols:\n        try:\n            dataset[float_col] = dataset[float_col].astype(np.float16)\n        except KeyError:\n            # print(f\"Column {int_col} does not exist. Skipping ...\")\n            pass\n    \n    return dataset\n\n# Cast to lower resolution types to save memory\ndef cast_data_types(dataset: pd.DataFrame) -> pd.DataFrame:\n    dataset = cast_category_types(dataset)\n    dataset = cast_int_types(dataset)\n    dataset = cast_float_types(dataset)\n    return dataset",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def drop_cols(dataset: pd.DataFrame) -> pd.DataFrame:\n    date_cols_to_drop = [\"date\", \"weekday\", \"month\", \"quarter\"]\n    event_cols_to_drop = [\n        \"event_type\",\n        \"event_type_lag_-3\", \"event_type_lag_-2\", \"event_type_lag_-1\",\n        \"event_type_lag_3\", \"event_type_lag_2\", \"event_type_lag_1\", \n    ]\n    sell_cols_to_drop = [\n        \"sell_price_rolling_mean_window_7\", \"sell_price_rolling_std_window_7\", \"sell_price_rolling_kurt_window_7\",\n        \"sell_price_rolling_mean_window_14\", \"sell_price_rolling_std_window_14\", \"sell_price_rolling_kurt_window_14\",\n        \"sell_price_rolling_mean_window_21\", \"sell_price_rolling_std_window_21\", \"sell_price_rolling_kurt_window_21\",\n        \"sell_price_rolling_mean_window_28\", \"sell_price_rolling_std_window_28\", \"sell_price_rolling_kurt_window_28\",\n    ]\n    count_cols_to_drop = [\n#         \"count_lag_28_rolling_mean_window_7\", \"count_lag_28_rolling_std_window_7\", \"count_lag_28_rolling_kurt_window_7\",\n#         \"count_lag_28_rolling_mean_window_14\", \"count_lag_28_rolling_std_window_14\", \"count_lag_28_rolling_kurt_window_14\",\n        \"count_lag_28_rolling_mean_window_21\", \"count_lag_28_rolling_std_window_21\", \"count_lag_28_rolling_kurt_window_21\",\n        \"count_lag_28_rolling_mean_window_28\", \"count_lag_28_rolling_std_window_28\", \"count_lag_28_rolling_kurt_window_28\",\n\n    ]\n\n    for column_set in (date_cols_to_drop, event_cols_to_drop, sell_cols_to_drop, count_cols_to_drop):\n        try:\n            dataset = dataset.drop(columns=column_set)\n        except KeyError:\n            # print(f\"Column '{col}' not found in axis. Skipping ...\")\n            pass\n\n        _ = gc.collect()\n    return dataset",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Load train data in chunks\ntrain_pq_file = pq.ParquetFile(f\"{PROCESSED_DATA_INPUT_PATH}/m5-acc-train.parquet\")\n\ntrain_data_set = pd.DataFrame()\nfor i, batch in tqdm(enumerate(train_pq_file.iter_batches(batch_size=131_072))):\n    train_batch_df = batch.to_pandas()\n    train_batch_df = drop_cols(train_batch_df)\n    train_batch_df = cast_data_types(train_batch_df)\n    \n    train_data_set = pd.concat([train_data_set, train_batch_df], ignore_index=True)\n    \n    del train_batch_df\n    _ = gc.collect()\n\ntrain_data_set = cast_data_types(train_data_set)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# LightGBM Model\nTrain the best performing LightGBM model.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "CATEGORICAL_FEATURES = [\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"year\",\n    \"event_name\",\n#     \"event_name_lag_-3\",\n    \"event_name_lag_-2\",\n    \"event_name_lag_-1\",\n    \"event_name_lag_1\",\n    \"event_name_lag_2\",\n#     \"event_name_lag_3\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"item_on_sale\",\n]\nCONTINOUS_FEATURES = [\n    \"item_id\",\n    \"weekday_sin\",\n    \"weekday_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"quarter_sin\",\n    \"quarter_cos\",\n    \"sell_price\",\n    \"sell_price_diff_1\",\n    \"sell_price_diff_2\",\n    \"sell_price_diff_3\",\n    \"sell_price_diff_7\",\n    \"count_lag_28\",\n    \"count_lag_29\",\n    \"count_lag_30\",\n    \"count_lag_31\",\n    \"count_lag_28_rolling_mean_window_7\", \"count_lag_28_rolling_std_window_7\", \"count_lag_28_rolling_kurt_window_7\",\n    \"count_lag_28_rolling_mean_window_14\", \"count_lag_28_rolling_std_window_14\", \"count_lag_28_rolling_kurt_window_14\",\n#     \"count_lag_28_rolling_mean_window_21\", \"count_lag_28_rolling_std_window_21\", \"count_lag_28_rolling_kurt_window_21\",\n#     \"count_lag_28_rolling_mean_window_28\", \"count_lag_28_rolling_std_window_28\", \"count_lag_28_rolling_kurt_window_28\",\n]\nFEATURES = CATEGORICAL_FEATURES + CONTINOUS_FEATURES\nLABEL = \"count\"\n\n# Parameters\nDATASET_PARAMETERS = {}\nTRAIN_PARAMETERS = {\n    \"objective\": \"tweedie\",\n    \"tweedie_variance_power\": 1.1,\n    \"learning_rate\": 0.0125,\n    \"num_leaves\": 2 ** 8 - 1,\n    \"max_bin\": 2 ** 7 - 1,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 0.8, \n    \"metric\": \"rmse\",\n    \"force_col_wise\": True,\n    \"seed\": 1,\n    \"histogram_pool_size\": 11_000,\n}",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Train test split\nhorizon = 28\nstart_t = MAX_TRAIN_TIMESTAMP - horizon\nend_t = MAX_TRAIN_TIMESTAMP\n\ntrain_df = train_data_set[train_data_set[\"d\"] < start_t]\nvalid_df = train_data_set[(train_data_set[\"d\"] >= start_t) & (train_data_set[\"d\"] <= end_t)]\nprint(f\"Train idx (start, end): ({train_df['d'].min()}, {train_df['d'].max()})\")\nprint(f\"Valid idx (start, end): ({valid_df['d'].min()}, {valid_df['d'].max()})\")\nprint(\"============\")\n\nX_train, y_train = train_df[FEATURES], train_df[LABEL]\nX_valid, y_valid = valid_df[FEATURES], valid_df[LABEL]\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_valid.shape}, y_test shape: {y_valid.shape}\")\n\ntrain_data = lgbm.Dataset(data=X_train, label=y_train)\nvalid_data = lgbm.Dataset(data=X_valid, label=y_valid)\n\ndel train_df\ngc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Train model\ntrain_valid_loss = {}\nmodel = lgbm.train(\n    TRAIN_PARAMETERS,\n    num_boost_round=1000,\n    train_set=train_data,\n    valid_sets=[train_data, valid_data],\n    callbacks=[\n        lgbm.early_stopping(stopping_rounds=5),\n        lgbm.log_evaluation(period=10),\n        lgbm.record_evaluation(train_valid_loss)\n    ]\n)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Forecast and save to onto valid df\ny_hat = model.predict(data=X_valid)\ny_hat = np.clip(y_hat, a_min=0, a_max=np.inf)\n\nforecast_df = valid_df[[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"d\"]].copy()\nforecast_df[\"count\"] = y_valid\nforecast_df[\"forecast\"] = y_hat",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## 1. Train / valid loss curves",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def learning_curve(\n    train_loss: list[float],\n    valid_loss: list[float],\n    ylabel: str,\n):\n    fig, ax = plt.subplots()\n    n_iter = np.arange(1, len(train_loss) + 1)\n    ax.plot(n_iter, train_loss, lw=2, label=\"train\")\n    ax.plot(n_iter, valid_loss, lw=2, label=\"valid\")\n    ax.set(xlabel=\"Iteration\", ylabel=ylabel)\n    ax.legend()\n    return ax",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "train_rmse = train_valid_loss[\"training\"][\"rmse\"]\nvalid_rmse = train_valid_loss[\"valid_1\"][\"rmse\"]\nlearning_curve(train_rmse, valid_rmse, \"RMSE\");",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## 2. Residual analysis",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def load_category_map(path: str) -> dict[int, str]:\n    with open(path, \"r\") as fp:\n        category_mapping = json.load(fp)\n    return category_mapping\n\ndef reverse_map(mapping):\n    return {v: k for k, v in mapping.items()}\n\nitem_id_map = reverse_map(load_category_map(f\"{PROCESSED_DATA_INPUT_PATH}/item_id_map.json\"))\ndept_id_map = reverse_map(load_category_map(f\"{PROCESSED_DATA_INPUT_PATH}/dept_id_map.json\"))\ncat_id_map = reverse_map(load_category_map(f\"{PROCESSED_DATA_INPUT_PATH}/cat_id_map.json\"))\nstore_id_map = reverse_map(load_category_map(f\"{PROCESSED_DATA_INPUT_PATH}/store_id_map.json\"))\nstate_id_map = reverse_map(load_category_map(f\"{PROCESSED_DATA_INPUT_PATH}/state_id_map.json\"))",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "### 2.1 Residuals vs. predicted values",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def residuals_plot(forecast_df: pd.DataFrame):\n    y_true = np.array(forecast_df[\"count\"])\n    y_pred = np.array(forecast_df[\"forecast\"])\n    fig, ax = plt.subplots()\n    ax.axhline(0, ls=\"--\", color=\"gray\", alpha=0.75)\n    ax.scatter(\n        np.array(y_pred),\n        np.array(y_true - y_pred),\n        alpha=0.3,\n    )\n    ax.set(xlabel=\"y_pred\", ylabel=\"y_true - y_pred\")\n    return ax\n\n\ndef residuals_plot_by_group(forecast_df: pd.DataFrame, group_by: str, category_map: dict[int, str]):\n    # Determine grid\n    group_values = forecast_df[group_by].unique()\n    if len(group_values) > 5:\n        n_rows = math.ceil(len(group_values) / 5)\n        n_cols = 5\n    else:\n        n_rows = 1\n        n_cols = len(group_values)\n    \n    # Plot\n    fig, ax = plt.subplots(\n        n_rows,\n        n_cols,\n        figsize=(4 * n_cols, 3 * n_rows),\n        sharex=True,\n        sharey=True,\n    )\n    ax = ax.flatten()\n    for i, group_value in enumerate(group_values):\n        group_df = forecast_df[forecast_df[group_by] == group_value]\n        ax[i].axhline(0, ls=\"--\", color=\"gray\", alpha=0.75)\n        ax[i].scatter(\n            np.array(group_df[\"forecast\"]),\n            np.array(group_df[\"count\"] - group_df[\"forecast\"]),\n            alpha=0.3,\n            label=category_map[group_value]\n        )\n        ax[i].legend()\n\n    for i in range(n_cols, 0, -1):\n        ax[-i].set(xlabel=\"y_pred\")\n    for i in range(0, len(ax), n_cols):\n        ax[i].set(ylabel=\"y_true - y_pred\")\n\n    fig.tight_layout();\n    return ax",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "residuals_plot(forecast_df);",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "residuals_plot_by_group(forecast_df, \"store_id\", store_id_map);",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "### 2.2 Residual distributions",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def residuals_dist_plot(forecast_df: pd.DataFrame):\n    y_true = np.array(forecast_df[\"count\"])\n    y_pred = np.array(forecast_df[\"forecast\"])\n    errors = np.array(y_true - y_pred)\n    fig, ax = plt.subplots()\n    freq, bins = np.histogram(errors, bins=100, density=True)\n    ax.bar(bins[:-1], freq, width=np.diff(bins), edgecolor=\"black\", align=\"edge\");\n    ax.set(xlabel=\"y_pred - y_true\", ylabel=\"Density\")\n    return ax\n\n\ndef residuals_dist_plot_by_group(forecast_df: pd.DataFrame, group_by: str, category_map: dict[int, str]):\n    # Determine grid\n    group_values = forecast_df[group_by].unique()\n    if len(group_values) > 5:\n        n_rows = math.ceil(len(group_values) / 5)\n        n_cols = 5\n    else:\n        n_rows = 1\n        n_cols = len(group_values)\n    \n    # Plot\n    fig, ax = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows), sharex=True)\n    ax = ax.flatten()\n    for i, group_value in enumerate(group_values):\n        group_df = forecast_df[forecast_df[group_by] == group_value]\n        y_true = np.array(group_df[\"count\"])\n        y_pred = np.array(group_df[\"forecast\"])\n        errors = np.array(y_true - y_pred)\n        freq, bins = np.histogram(errors, bins=100, density=True)\n        ax[i].bar(\n            bins[:-1],\n            freq,\n            width=np.diff(bins),\n            edgecolor=\"black\",\n            align=\"edge\",\n            label=category_map[group_value],\n        );\n        ax[i].legend()\n\n    for i in range(n_cols, 0, -1):\n        ax[-i].set(xlabel=\"y_true - y_pred\")\n    for i in range(0, len(ax), n_cols):\n        ax[i].set(ylabel=\"Density\")\n\n    fig.tight_layout();\n    return ax",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "residuals_dist_plot(forecast_df);",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "residuals_dist_plot_by_group(forecast_df, \"store_id\", store_id_map);",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "### 2.3 Correlation",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "",
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## 3. Feature importance",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "split_importance_df = pd.DataFrame(\n    data=list(zip(model.feature_importance(\"split\"), FEATURES)),\n    columns=[\"importance\", \"feature\"]\n)\n\ngain_importance_df = pd.DataFrame(\n    data=list(zip(model.feature_importance(\"gain\"), FEATURES)),\n    columns=[\"importance\", \"feature\"]\n)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "fig, ax = plt.subplots(1, 2, figsize=(15, 7.5), sharey=True)\n\ny_labels = np.array(split_importance_df[\"feature\"])\ny_positions = np.arange(len(y_labels))\nimportance = np.array(split_importance_df[\"importance\"])\nax[0].barh(y_positions, importance)\nax[0].set(yticks=y_positions, yticklabels=y_labels)\nax[0].set_xscale(\"log\")\nax[0].set_title(\"Split Importance\")\n\n\ny_labels = np.array(gain_importance_df[\"feature\"])\ny_positions = np.arange(len(y_labels))\nimportance = np.array(gain_importance_df[\"importance\"])\nax[1].barh(y_positions, importance, align='center')\nax[1].set(yticks=y_positions, yticklabels=y_labels)\nax[1].set_xscale(\"log\")\nax[1].set_title(\"Gain Importance\")\n\nfig.tight_layout();",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}