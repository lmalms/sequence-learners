{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30746,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "INPUT_BASE_PATH = \"/kaggle/input/m5-forecasting-accuracy\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\nCALENDAR_DATA = pd.read_csv(f\"{INPUT_BASE_PATH}/calendar.csv\")\n# SELL_PRICES = pd.read_csv(f\"{INPUT_BASE_PATH}/sell_prices.csv\")\n# SALES_TRAIN_EVALUATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_evaluation.csv\")\nSALES_TRAIN_VALIDATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_validation.csv\")\nSAMPLE_SUBMISSION = pd.read_csv(f\"{INPUT_BASE_PATH}/sample_submission.csv\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def get_product_id_mappings(df: pd.DataFrame):\n    id_to_int = {\n        product_id: i\n        for (i, product_id) in enumerate(df[\"id\"].unique(), start=1)\n    }\n    int_to_id = {v: k for (k,v) in id_to_int.items()}\n    return id_to_int, int_to_id\n\n\ndef flatten_sales_df(sales_df: pd.DataFrame, timestamp_cols: list[str]) -> pd.DataFrame:\n    sales_dfs = []\n    grouper = train_df.groupby(\n        [\n            \"item_id\",\n            \"dept_id\",\n            \"cat_id\",\n            \"store_id\",\n            \"state_id\"\n        ]\n    )\n    for key, sales_df in tqdm(grouper):\n        (item, dept, cat, store, state) = key\n        sales_df = sales_df[timestamp_cols].T.reset_index()\n        sales_df.columns = [\"d\", \"count\"]\n        sales_df[\"item_id\"] = item\n        sales_df[\"dept_id\"] = dept\n        sales_df[\"cat_id\"] = cat\n        sales_df[\"store_id\"] = store\n        sales_df[\"state_id\"] = state\n        sales_dfs.append(sales_df)\n    return pd.concat(sales_dfs)\n        \n\ndef merge_sales_and_calendar_data(sales_df: pd.DataFrame, calendar_data: pd.DataFrame) -> pd.DataFrame:\n    return sales_df.merge(right=calendar_data, on=\"d\", how=\"left\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "timestamp_cols = [f\"d_{i}\" for i in range(1, 1914)]\n\ntrain_df = SALES_TRAIN_VALIDATION.copy(deep=True)\ntrain_df = flatten_sales_df(train_df, timestamp_cols)\ntrain_df = merge_sales_and_calendar_data(train_df, CALENDAR_DATA)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# SALES BY STORE",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-07-22T20:37:25.945738Z",
                    "iopub.execute_input": "2024-07-22T20:37:25.946145Z",
                    "iopub.status.idle": "2024-07-22T20:37:25.986313Z",
                    "shell.execute_reply.started": "2024-07-22T20:37:25.946112Z",
                    "shell.execute_reply": "2024-07-22T20:37:25.985112Z"
                }
            }
        },
        {
            "cell_type": "code",
            "source": "sales_by_store = train_df.groupby([\"store_id\", \"date\"])[[\"count\"]].sum()\nstore_ids = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nn_samples = 500\n\nfig, ax = plt.subplots(5, 2, sharex=True, sharey=True, figsize=(15, 10))\nax = ax.flatten()\nfor i, store_id in enumerate(store_ids):\n    store_df = sales_by_store.loc[(store_id, slice(None))].copy().reset_index()\n    store_df = store_df.iloc[-n_samples:]\n    ax[i].plot(np.arange(n_samples), store_df[\"count\"].values, label=store_id)\n    \n    date_labels = store_df[\"date\"].values[::100]\n    x_ticks = np.arange(0, n_samples, 100)\n    ax[i].set_xticks(x_ticks)\n    ax[i].set_xticklabels(date_labels, rotation=45)\n    ax[i].legend()\n\nfig.tight_layout();",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# SALES BY CATEGORY",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-07-22T20:39:02.870765Z",
                    "iopub.execute_input": "2024-07-22T20:39:02.871155Z",
                    "iopub.status.idle": "2024-07-22T20:39:02.875995Z",
                    "shell.execute_reply.started": "2024-07-22T20:39:02.871125Z",
                    "shell.execute_reply": "2024-07-22T20:39:02.874832Z"
                }
            }
        },
        {
            "cell_type": "code",
            "source": "sales_by_category = train_df.groupby([\"cat_id\", \"date\"])[[\"count\"]].sum()\ncategory_ids = [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\n\nn_samples = 500\nfig, ax = plt.subplots(3, 1, figsize=(8, 6), sharex=True)\nfor i, cat_id in enumerate(category_ids):\n    cat_df = sales_by_category.loc[(cat_id, slice(None))].copy().reset_index()\n    cat_df = cat_df.iloc[-n_samples:]\n    ax[i].plot(np.arange(n_samples), cat_df[\"count\"].values, label=cat_id)\n    \n    date_labels = cat_df[\"date\"].values[::100]\n    x_ticks = np.arange(0, n_samples, 100)\n    ax[i].set_xticks(x_ticks)\n    ax[i].set_xticklabels(date_labels, rotation=45)\n\n    ax[i].legend()\nfig.tight_layout();",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# SALES BY CATEGORY & STORE",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "sales_by_store_and_cat = train_df.groupby([\"store_id\", \"cat_id\", \"date\"])[[\"count\"]].sum()\nstore_ids = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\ncategory_ids = [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\nn_samples = 500\n\nfig, ax = plt.subplots(5, 2, sharex=True, sharey=True, figsize=(15, 10))\nax = ax.flatten()\nfor i, store_id in enumerate(store_ids):\n    store_df = sales_by_store_and_cat.loc[store_id]\n    for cat_id in [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]:\n        cat_df = store_df.loc[cat_id].reset_index()\n        cat_df = cat_df.iloc[-n_samples:]\n        ax[i].plot(np.arange(n_samples), cat_df[\"count\"].values, label=(store_id, cat_id))\n\n    ax[i].legend(fontsize=\"small\", ncols=3)\n    \n    date_labels = cat_df[\"date\"].values[::100]\n    x_ticks = np.arange(0, n_samples, 100)\n    ax[i].set_xticks(x_ticks)\n    ax[i].set_xticklabels(date_labels, rotation=45)\n\nfig.tight_layout();",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# SALES BY STORE & DEPARTMENT",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "sales_by_store_and_dept = train_df.groupby([\"store_id\", \"dept_id\", \"date\"])[[\"count\"]].sum()\nstore_ids = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n\nn_samples = 500\n\nfig, ax = plt.subplots(5, 2, sharex=True, sharey=True, figsize=(15, 10))\nax = ax.flatten()\nfor i, store_id in enumerate(store_ids):\n    store_df = sales_by_store_and_dept.loc[store_id]\n    store_df = store_df.unstack(level=0).droplevel(level=0, axis=1)\n    store_df = store_df.iloc[-n_samples:]\n    ax[i] = store_df.plot(ax=ax[i])\n    \n    ax[i].legend(fontsize=\"x-small\", ncols=4)\n    ax[i].set_title(store_id)\n\nfig.tight_layout();",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}