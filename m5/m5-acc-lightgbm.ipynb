{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                },
                {
                    "sourceId": 9477181,
                    "sourceType": "datasetVersion",
                    "datasetId": 5684895
                }
            ],
            "dockerImageVersionId": 30746,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import json\nimport gc\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "SUBMISSION_RUN = True",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Constants\n\n# Paths\nINPUT_BASE_PATH = \"/kaggle/input/\"\nRAW_DATA_INPUT_PATH = f\"{INPUT_BASE_PATH}/m5-forecasting-accuracy\"\nPROCESSED_DATA_INPUT_PATH = f\"{INPUT_BASE_PATH}/m5-acc\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\n# Timestamps\nMAX_TRAIN_TIMESTAMP = 1941\nSTART_TEST_TIMESTAMP = 1942\nSTART_TEST_WM_YR_WK = 11617",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Data Input",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Load raw data\n\n# CALENDAR_DATA = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/calendar.csv\")\n# SELL_PRICES = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/sell_prices.csv\")\n# SALES_TRAIN_EVALUATION = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/sales_train_evaluation.csv\")\n\nSAMPLE_SUBMISSION = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/sample_submission.csv\")\nSUBMISSION_INDEX = SAMPLE_SUBMISSION.set_index(\"id\").index\nVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"validation\")]\nEVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"evaluation\")]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Load preprocessed data\ntrain_data_set = pd.read_parquet(f\"{PROCESSED_DATA_INPUT_PATH}/m5-acc-train.parquet\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Drop any columns that we dont need for training\ncols_to_drop = [\"av_store_dept_sales\", \"date\", \"wm_yr_wk\"]\ntry:\n    train_data_set = train_data_set.drop(columns=cols_to_drop)\nexcept KeyError:\n    print(f\"Columns not found in axis. Skipping ...\")\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Cast to lower resolution types to save memory\ndef cast_data_types(dataset: pd.DataFrame) -> pd.DataFrame:\n    # Category cols\n    item_category_cols = [\"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    date_category_cols = [\"wm_yr_wk\", \"weekday\", \"month\", \"year\", \"event_name\", \"event_type\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]\n    price_category_cols = [\"item_on_sale\"]\n    for category_col in item_category_cols + date_category_cols + price_category_cols:\n        try:\n            dataset[category_col] = dataset[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Column {category_col} does not exist. Skipping ...\")\n    \n    # Integer cols\n    int_cols = [\"d\", \"count\"]\n    for int_col in int_cols:\n        try:\n            dataset[int_col] = dataset[int_col].astype(np.int16)\n        except KeyError:\n            print(f\"Column {int_col} does not exist. Skipping ...\")\n    \n    # Float cols\n    float_sales_cols = [\n        \"item_id\",\n        \"count_lag_28\", \"count_lag_29\", \"count_lag_30\", \"count_lag_31\",\n        \"av_store_dept_sales_lag_28\", \"av_store_dept_sales_lag_29\", \"av_store_dept_sales_lag_30\", \"av_store_dept_sales_lag_31\"\n    ]\n    float_price_cols = [\n        \"sell_price\",\n        \"sell_price_diff_1\", \"sell_price_diff_2\", \"sell_price_diff_3\", \"sell_price_diff_7\",\n        \"av_item_state_sell_price\", \"av_dept_state_sell_price\"\n    ]\n    float_date_cols = [\n        \"weekday_sin\", \"weekday_cos\", \"month_sin\", \"month_cos\"\n    ]\n    float_cols = float_sales_cols + float_price_cols + float_date_cols\n    for float_col in float_cols:\n        try:\n            dataset[float_col] = dataset[float_col].astype(np.float16)\n        except KeyError:\n            print(f\"Column {int_col} does not exist. Skipping ...\")\n    \n    return dataset\n\ntrain_data_set = cast_data_types(train_data_set)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# LightGBM Model",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Rolling window validation\n\ndef time_series_split(\n    df: pd.DataFrame,\n    n_folds: int = 5,\n    horizon: int = 28,\n    overlap: int = 0,\n    max_timestamp: int = MAX_TRAIN_TIMESTAMP,\n):\n    min_timestamp = max_timestamp - n_folds * horizon + (n_folds - 1) * overlap\n    for fold_idx in range(n_folds):\n        start = min_timestamp + fold_idx * (horizon - overlap)\n        stop = start + horizon\n\n        train_data = df[df[\"d\"] < start]\n        valid_data = df[(df[\"d\"] >= start) & (df[\"d\"] < stop)]\n        \n        print(f\"Fold index: {fold_idx}\")\n        print(f\"Train idx (start, end): ({train_data['d'].min()}, {train_data['d'].max()})\")\n        print(f\"Valid idx (start, end): ({valid_data['d'].min()}, {valid_data['d'].max()})\")\n        print(\"==================\")\n        \n        yield train_data, valid_data\n        \n        \ndef train_predict_score(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_test: pd.DataFrame,\n    y_test: pd.Series,\n    train_parameters: dict,\n    dataset_parameters: dict | None = None,\n) -> tuple[float, float]:\n    dataset_parameters = dataset_parameters or {}\n    train_data = lgbm.Dataset(data=X_train, label=y_train, **dataset_parameters)\n    valid_data = lgbm.Dataset(data=X_test, label=y_test, **dataset_parameters)\n    model = lgbm.train(\n        train_parameters,\n        num_boost_round=3000,\n        train_set=train_data,\n        valid_sets=[train_data, valid_data],\n        callbacks=[\n            lgbm.early_stopping(stopping_rounds=5),\n            lgbm.log_evaluation(period=10),\n        ]\n    )\n    \n    y_hat = model.predict(X_test)\n    y_hat = np.clip(y_hat, a_min=0, a_max=np.inf)\n    rmse = np.sqrt(np.mean((y_test - y_hat) ** 2))\n\n    return rmse, model.best_iteration",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "CATEGORICAL_FEATURES = [\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"year\",\n    \"event_name\",\n    \"event_type\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"item_on_sale\",\n]\nCONTINOUS_FEATURES = [\n    \"item_id\",\n    \"weekday_sin\",\n    \"weekday_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"sell_price\",\n    \"sell_price_diff_1\",\n    \"sell_price_diff_2\",\n    \"sell_price_diff_3\",\n    \"sell_price_diff_7\",\n#     \"av_item_state_sell_price\",\n#     \"av_dept_state_sell_price\",\n    \"count_lag_28\",\n    \"count_lag_29\",\n    \"count_lag_30\",\n    \"count_lag_31\",\n    \"av_store_dept_sales_lag_28\",\n    \"av_store_dept_sales_lag_29\",\n    \"av_store_dept_sales_lag_30\",\n    \"av_store_dept_sales_lag_31\"\n]\nFEATURES = CATEGORICAL_FEATURES + CONTINOUS_FEATURES\nLABEL = \"count\"\n\n# Parameters\nDATASET_PARAMETERS = {}\nTRAIN_PARAMETERS = {\n    \"objective\": \"tweedie\",\n    'tweedie_variance_power': 1.1,\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 2 ** 7 - 1,\n    \"max_bin\": 2 ** 7 - 1,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.8,\n    \"metric\": \"rmse\",\n    \"force_col_wise\": True,\n}",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    scores = []\n    best_iterations = []\n    \n    for train, valid in tqdm(time_series_split(train_data_set, n_folds=5), total=5):\n    \n        X_train, y_train = train[FEATURES], train[LABEL]\n        X_test, y_test = valid[FEATURES], valid[LABEL]\n        score, best_iter = train_predict_score(\n            X_train=X_train,\n            y_train=y_train,\n            X_test=X_test,\n            y_test=y_test,\n            train_parameters=TRAIN_PARAMETERS,\n            dataset_parameters=DATASET_PARAMETERS,\n        )\n        scores.append(score)\n        best_iterations.append(best_iter)\n    \n    cv_results = {\n        \"scores\": scores,\n        \"mean_score\": np.mean(scores),\n        \"std_score\": np.std(scores),\n        \"best_iterations\": best_iterations,\n        \"mean_best_iteration\": np.mean(best_iterations)\n    }\n    \n    with open(f\"{OUTPUT_BASE_BATH}/cv_results.json\", \"w\") as f:\n        json.dump(cv_results, f)\n    \nelse:\n    X_train, y_train = train_data_set[FEATURES], train_data_set[LABEL]\n    train_data = lgbm.Dataset(data=X_train, label=y_train, **DATASET_PARAMETERS)\n    model = lgbm.train(\n        TRAIN_PARAMETERS,\n        num_boost_round=450,\n        train_set=train_data,\n        valid_sets=[train_data],\n        callbacks=[lgbm.log_evaluation(period=10)]\n    )",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Forecast & Submit",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def pivot_forecast_df(X_test: pd.DataFrame, y_hat: np.ndarray) -> pd.DataFrame:\n    forecast_df = pd.concat([X_test, pd.DataFrame(y_hat, columns=[\"count\"])], axis=1)\n    forecast_df = forecast_df.pivot(columns=\"d\", index=\"id\", values=\"count\").sort_index(axis=1)\n    forecast_df = forecast_df.rename_axis(None, axis=1).reset_index()\n    return forecast_df\n\ndef rename_forecast_columns(forecast_df: pd.DataFrame, forecast_horizon: int = 28) -> pd.DataFrame:\n    forecast_df = forecast_df.set_index(\"id\")\n    forecast_df.columns = [f\"F{i}\" for i in range(1, forecast_horizon + 1)]\n    return forecast_df.reset_index()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if SUBMISSION_RUN:\n    test_data_set = pd.read_parquet(f\"{PROCESSED_DATA_INPUT_PATH}/m5-acc-test.parquet\")\n    \n    # Drop any columns that we dont need for training\n    cols_to_drop = [\"av_store_dept_sales\", \"date\", \"wm_yr_wk\"]\n    for col in cols_to_drop:\n        try:\n            test_data_set = test_data_set.drop([col], axis=1)\n        except KeyError:\n            print(f\"Column '{col}' not found in axis. Skipping ...\")\n    _ = gc.collect()\n\n    test_data_set = cast_data_types(test_data_set)\n\n    y_hat = model.predict(test_data_set[FEATURES])\n    y_hat = np.clip(y_hat, a_min=0, a_max=np.inf)\n\n    forecast_df = pivot_forecast_df(test_data_set, y_hat)\n    forecast_df = rename_forecast_columns(forecast_df)\n    \n    # Merge with sample submissions\n    EVAL_SUBMISSION = EVAL_SUBMISSION[[\"id\"]].merge(forecast_df, on=\"id\", how=\"left\")\n    FINAL_SUBMISSIONS = pd.concat([VAL_SUBMISSION, EVAL_SUBMISSION])\n    FINAL_SUBMISSIONS = FINAL_SUBMISSIONS.set_index(\"id\").reindex(SUBMISSION_INDEX).reset_index()\n    \n    FINAL_SUBMISSIONS.to_csv(f\"{OUTPUT_BASE_BATH}/submission.csv\", index=False)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}