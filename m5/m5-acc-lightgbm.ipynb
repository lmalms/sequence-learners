{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30746,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import json\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:26.774890Z",
                    "iopub.execute_input": "2024-09-11T08:01:26.775338Z",
                    "iopub.status.idle": "2024-09-11T08:01:28.568016Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:26.775299Z",
                    "shell.execute_reply": "2024-09-11T08:01:28.566865Z"
                },
                "trusted": true
            },
            "execution_count": 1,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "SUBMISSION_RUN = False",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:28.573520Z",
                    "iopub.execute_input": "2024-09-11T08:01:28.573810Z",
                    "iopub.status.idle": "2024-09-11T08:01:28.579393Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:28.573785Z",
                    "shell.execute_reply": "2024-09-11T08:01:28.578188Z"
                },
                "trusted": true
            },
            "execution_count": 2,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "INPUT_BASE_PATH = \"/kaggle/input/m5-forecasting-accuracy\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\nCALENDAR_DATA = pd.read_csv(f\"{INPUT_BASE_PATH}/calendar.csv\")\nSELL_PRICES = pd.read_csv(f\"{INPUT_BASE_PATH}/sell_prices.csv\")\nSALES_TRAIN_EVALUATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_evaluation.csv\")\n\nSAMPLE_SUBMISSION = pd.read_csv(f\"{INPUT_BASE_PATH}/sample_submission.csv\")\nSUBMISSION_INDEX = SAMPLE_SUBMISSION.set_index(\"id\").index\nVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"validation\")]\nEVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"evaluation\")]",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:28.580794Z",
                    "iopub.execute_input": "2024-09-11T08:01:28.581195Z",
                    "iopub.status.idle": "2024-09-11T08:01:37.107345Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:28.581165Z",
                    "shell.execute_reply": "2024-09-11T08:01:37.105981Z"
                },
                "trusted": true
            },
            "execution_count": 3,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Constants\nMAX_TRAIN_TIMESTAMP = 1941\nSTART_TEST_TIMESTAMP = 1942\nSTART_TEST_WM_YR_WK = 11617",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:37.110960Z",
                    "iopub.execute_input": "2024-09-11T08:01:37.111475Z",
                    "iopub.status.idle": "2024-09-11T08:01:37.118151Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:37.111432Z",
                    "shell.execute_reply": "2024-09-11T08:01:37.116778Z"
                },
                "trusted": true
            },
            "execution_count": 4,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# PREPROCESSING & FEATURE ENGINEERING",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Extract categorical mappings\ndef get_unique_value_id_map(df: pd.DataFrame, col_name: str):\n    return {value_id: i for (i, value_id) in enumerate(df[col_name].unique())}\n\n\ndef map_category_ids(\n    df: pd.DataFrame,\n    column_name: str,\n    category_id_map: dict | None = None,\n    submission_run: bool = False\n) -> pd.DataFrame:\n    if category_id_map is None:\n        category_id_map = get_unique_value_id_map(df, column_name)\n    id_category_map = {v: k for (k, v) in category_id_map.items()}\n    df[column_name] = df[column_name].map(category_id_map)\n    return (category_id_map, None) if submission_run else (category_id_map, id_category_map)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:37.119777Z",
                    "iopub.execute_input": "2024-09-11T08:01:37.120231Z",
                    "iopub.status.idle": "2024-09-11T08:01:37.135488Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:37.120192Z",
                    "shell.execute_reply": "2024-09-11T08:01:37.134186Z"
                },
                "trusted": true
            },
            "execution_count": 5,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process sales data\ndef unpivot_sales_df(sales_df: pd.DataFrame, timestamp_cols: list[str]) -> pd.DataFrame:\n    sales_df = sales_df.melt(\n        id_vars=[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n        value_vars=timestamp_cols,\n        var_name=\"d\",\n        value_name=\"count\"\n    )\n    sales_df[\"d\"] = sales_df[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    return sales_df\n\n\ndef cast_sales_data_types(sales_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    for category_col in category_cols:\n        sales_df[category_col] = sales_df[category_col].astype(\"category\")\n        \n    column_downcast_map = [(\"d\", \"integer\"), (\"count\", \"integer\")]\n    for (col, dtype) in column_downcast_map:\n        sales_df[col] = pd.to_numeric(sales_df[col], downcast=dtype)\n    \n    return sales_df",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:37.137350Z",
                    "iopub.execute_input": "2024-09-11T08:01:37.137792Z",
                    "iopub.status.idle": "2024-09-11T08:01:37.147594Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:37.137762Z",
                    "shell.execute_reply": "2024-09-11T08:01:37.146425Z"
                },
                "trusted": true
            },
            "execution_count": 6,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process calendar data\ndef select_format_calendar_features(calendar_data: pd.DataFrame) -> pd.DataFrame:\n    # Drop columns\n    cols_to_drop = [\"weekday\", \"event_name_2\", \"event_type_2\"]\n    calendar_data = calendar_data.copy().drop(columns=cols_to_drop)\n    \n    # Format cols\n    calendar_data[\"d\"] = calendar_data[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    calendar_data = calendar_data.rename(columns={\n        \"event_name_1\": \"event_name\",\n        \"event_type_1\": \"event_type\",\n        \"wday\": \"weekday\"\n    })\n    \n    return calendar_data\n\n\ndef cast_calendar_data_types(calendar_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\n        \"weekday\",\n        \"month\",\n        \"year\",\n        \"wm_yr_wk\",\n        \"event_name\",\n        \"event_type\",\n        \"snap_CA\",\n        \"snap_TX\",\n        \"snap_WI\"\n    ]\n    for category_col in category_cols:\n        calendar_df[category_col] = calendar_df[category_col].astype(\"category\")\n    \n    calendar_df[\"d\"] = pd.to_numeric(calendar_df[\"d\"], downcast=\"integer\")\n    \n    return calendar_df",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:37.148939Z",
                    "iopub.execute_input": "2024-09-11T08:01:37.149322Z",
                    "iopub.status.idle": "2024-09-11T08:01:37.162276Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:37.149293Z",
                    "shell.execute_reply": "2024-09-11T08:01:37.160826Z"
                },
                "trusted": true
            },
            "execution_count": 7,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process sell price data\ndef cast_price_data_types(price_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\"item_id\", \"store_id\", \"wm_yr_wk\"]\n    for category_col in category_cols:\n        price_df[category_col] = price_df[category_col].astype(\"category\")\n        \n    price_df[\"sell_price\"] = pd.to_numeric(price_df[\"sell_price\"], downcast=\"float\")\n    \n    return price_df\n\n\ndef extract_state_and_dept_id(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    return (\n        sell_prices\n        .assign(state_id=sell_prices[\"store_id\"].str.extract(r\"(^[A-Z]+)\"))\n        .assign(dept_id=sell_prices[\"item_id\"].str.extract(r\"(^[A-Z]+_[0-9]+)\"))\n    )",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:37.163680Z",
                    "iopub.execute_input": "2024-09-11T08:01:37.164062Z",
                    "iopub.status.idle": "2024-09-11T08:01:37.179256Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:37.164026Z",
                    "shell.execute_reply": "2024-09-11T08:01:37.178065Z"
                },
                "trusted": true
            },
            "execution_count": 8,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndef merge_sales_calendar_price_data(\n    sales_df: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n    price_df: pd.DataFrame,\n    chunk_size: int = 500_000,\n) -> pd.DataFrame:\n    chunks = []\n    for chunk_start in tqdm(range(0, len(sales_df), chunk_size)):\n        sales_chunk = sales_df.iloc[chunk_start: chunk_start + chunk_size]\n        merged_chunk = (\n            sales_chunk.merge(\n                right=calendar_df,\n                on=\"d\",\n                how=\"left\",\n            ).merge(\n                right=price_df,\n                on=[\"item_id\", \"store_id\", \"wm_yr_wk\"],\n                how=\"left\",\n            )\n        )\n        chunks.append(merged_chunk)\n        \n#         if chunk_start > 10 * chunk_size: break\n            \n    return pd.concat(chunks)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:01:37.180665Z",
                    "iopub.execute_input": "2024-09-11T08:01:37.181078Z",
                    "iopub.status.idle": "2024-09-11T08:01:37.191757Z",
                    "shell.execute_reply.started": "2024-09-11T08:01:37.181043Z",
                    "shell.execute_reply": "2024-09-11T08:01:37.190632Z"
                },
                "trusted": true
            },
            "execution_count": 9,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Feature engineering\n\ndef fourier_features(data: pd.DataFrame) -> pd.DataFrame:\n    data[\"weekday_sin\"] = np.sin((2 * np.pi * data[\"weekday\"].astype(float)) / 7)\n    data[\"weekday_cos\"] = np.cos((2 * np.pi * data[\"weekday\"].astype(float)) / 7)\n    data[\"month_sin\"] = np.sin((2 * np.pi * data[\"month\"].astype(float)) / 12)\n    data[\"month_cos\"] = np.cos((2 * np.pi * data[\"month\"].astype(float)) / 12)\n    \n    for col in [\"weekday_sin\", \"weekday_cos\", \"month_sin\", \"month_cos\"]:\n        data[col] = pd.to_numeric(data[col], downcast=\"float\")\n    \n    return data\n\n\ndef av_item_state_prices(data: pd.DataFrame) -> pd.DataFrame:\n    av_price = (\n        data\n        .groupby([\"item_id\", \"state_id\", \"d\"], observed=True)\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_item_state_sell_price\"})\n        .reset_index()\n    )\n    \n    data = data.merge(right=av_price, on=[\"item_id\", \"state_id\", \"d\"], how=\"left\")\n    data[\"sell_price\"] = pd.to_numeric(data[\"sell_price\"], downcast=\"float\")\n    data[\"av_item_state_sell_price\"] = pd.to_numeric(data[\"av_item_state_sell_price\"], downcast=\"float\")\n    \n    # Fill missing sell prices with av\n    data[\"sell_price\"] = data[\"sell_price\"].fillna(data[\"av_item_state_sell_price\"])\n    \n    return data",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:02:53.136457Z",
                    "iopub.execute_input": "2024-09-11T08:02:53.136903Z",
                    "iopub.status.idle": "2024-09-11T08:02:53.148267Z",
                    "shell.execute_reply.started": "2024-09-11T08:02:53.136865Z",
                    "shell.execute_reply": "2024-09-11T08:02:53.147053Z"
                },
                "trusted": true
            },
            "execution_count": 11,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def merge_sales_and_price_data(sales_df: pd.DataFrame, sell_prices: pd.DataFrame) -> pd.DataFrame:\n    dedupe_item_state_prices = (\n        sell_prices[[\"item_id\", \"state_id\", \"wm_yr_wk\", \"av_item_state_sell_price\"]]\n        .drop_duplicates(subset=[\"item_id\", \"state_id\", \"wm_yr_wk\"])\n    )\n    dedupe_dept_state_prices = (\n        sell_prices[[\"dept_id\", \"state_id\", \"wm_yr_wk\", \"av_dept_state_sell_price\"]]\n        .drop_duplicates(subset=[\"dept_id\", \"state_id\", \"wm_yr_wk\"])\n    )\n    merged = sales_df.merge(\n        right=sell_prices[[\"item_id\", \"store_id\", \"wm_yr_wk\", \"sell_price\"]],\n        on=[\"item_id\", \"store_id\", \"wm_yr_wk\"],\n        how=\"left\",\n    )\n    merged = merged.merge(\n        right=dedupe_item_state_prices,\n        on=[\"item_id\", \"state_id\", \"wm_yr_wk\"],\n        how=\"left\",\n    )\n    merged = merged.merge(\n        right=dedupe_dept_state_prices,\n        on=[\"dept_id\", \"state_id\", \"wm_yr_wk\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef fill_missing_sell_prices(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"sell_price\"] = (\n        df[\"sell_price\"]\n        .fillna(df[\"av_item_state_sell_price\"])\n        .fillna(df[\"av_dept_state_sell_price\"])\n    )\n    df[\"av_item_state_sell_price\"] = (\n        df[\"av_item_state_sell_price\"]\n        .fillna(df[\"av_dept_state_sell_price\"])\n    )\n    return df\n    ",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# EVALUATION DATASET -- PRIVATE LEADERBOARD",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process sales data\n\n# Map item ids to categories\nitem_id_map, id_item_map = map_category_ids(SALES_TRAIN_EVALUATION, \"item_id\", submission_run=SUBMISSION_RUN)\ndept_id_map, id_dept_map = map_category_ids(SALES_TRAIN_EVALUATION, \"dept_id\", submission_run=SUBMISSION_RUN)\ncat_id_map, id_cat_map = map_category_ids(SALES_TRAIN_EVALUATION, \"cat_id\", submission_run=SUBMISSION_RUN)\nstore_id_map, id_store_map = map_category_ids(SALES_TRAIN_EVALUATION, \"store_id\", submission_run=SUBMISSION_RUN)\nstate_id_map, id_state_map = map_category_ids(SALES_TRAIN_EVALUATION, \"state_id\", submission_run=SUBMISSION_RUN)\n\n# Store category info for each item id\nITEM_ID_CATEGORIES = SALES_TRAIN_EVALUATION[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]\n\n# Convert sales table to long table format\nstart_t = 1 if SUBMISSION_RUN else 1\ntimestamp_cols = [f\"d_{i}\" for i in range(start_t, MAX_TRAIN_TIMESTAMP + 1)]\nsales_df = unpivot_sales_df(SALES_TRAIN_EVALUATION, timestamp_cols)\n\n# Cast datatypes\nsales_df = cast_sales_data_types(sales_df)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:03:05.051567Z",
                    "iopub.execute_input": "2024-09-11T08:03:05.052007Z",
                    "iopub.status.idle": "2024-09-11T08:04:05.994459Z",
                    "shell.execute_reply.started": "2024-09-11T08:03:05.051957Z",
                    "shell.execute_reply": "2024-09-11T08:04:05.992950Z"
                },
                "trusted": true
            },
            "execution_count": 12,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process calendar data\ncalendar_df = select_format_calendar_features(CALENDAR_DATA)\n\n# Map event categories\nevent_name_map, id_event_name_map = map_category_ids(calendar_df, \"event_name\", submission_run=SUBMISSION_RUN)\nevent_type_map, id_event_type_map = map_category_ids(calendar_df, \"event_type\", submission_run=SUBMISSION_RUN)\n\n# Cast datatypes\ncalendar_df = cast_calendar_data_types(calendar_df)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:04:05.996607Z",
                    "iopub.execute_input": "2024-09-11T08:04:05.997068Z",
                    "iopub.status.idle": "2024-09-11T08:04:06.021490Z",
                    "shell.execute_reply.started": "2024-09-11T08:04:05.997019Z",
                    "shell.execute_reply": "2024-09-11T08:04:06.020077Z"
                },
                "trusted": true
            },
            "execution_count": 13,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process price data\nitem_id_map, id_item_map = map_category_ids(SELL_PRICES, \"item_id\", item_id_map, SUBMISSION_RUN)\nstore_id_map, id_store_map = map_category_ids(SELL_PRICES, \"store_id\", store_id_map, SUBMISSION_RUN)\n\nprice_df = cast_price_data_types(SELL_PRICES)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:04:06.022823Z",
                    "iopub.execute_input": "2024-09-11T08:04:06.023184Z",
                    "iopub.status.idle": "2024-09-11T08:04:07.478711Z",
                    "shell.execute_reply.started": "2024-09-11T08:04:06.023153Z",
                    "shell.execute_reply": "2024-09-11T08:04:07.477529Z"
                },
                "trusted": true
            },
            "execution_count": 14,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndata = merge_sales_calendar_price_data(sales_df, calendar_df, price_df)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:04:07.481309Z",
                    "iopub.execute_input": "2024-09-11T08:04:07.481786Z",
                    "iopub.status.idle": "2024-09-11T08:08:43.768498Z",
                    "shell.execute_reply.started": "2024-09-11T08:04:07.481742Z",
                    "shell.execute_reply": "2024-09-11T08:08:43.767250Z"
                },
                "trusted": true
            },
            "execution_count": 15,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "100%|██████████| 119/119 [04:32<00:00,  2.29s/it]\n",
                    "output_type": "stream"
                }
            ]
        },
        {
            "cell_type": "code",
            "source": "# Feature engineering\ndata = fourier_features(data)\ndata = av_item_state_prices(data)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-11T08:10:27.250632Z",
                    "iopub.execute_input": "2024-09-11T08:10:27.251140Z",
                    "iopub.status.idle": "2024-09-11T08:11:28.217186Z",
                    "shell.execute_reply.started": "2024-09-11T08:10:27.251107Z",
                    "shell.execute_reply": "2024-09-11T08:11:28.215902Z"
                },
                "trusted": true
            },
            "execution_count": 16,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def get_item_state_level_sell_prices(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    return (\n        sell_prices\n        .groupby([\"item_id\", \"state_id\", \"wm_yr_wk\"], observed=True)\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_item_state_sell_price\"})\n    )\n\n\ndef get_dept_state_level_sell_prices(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    return (\n        sell_prices\n        .groupby([\"dept_id\", \"state_id\", \"wm_yr_wk\"], observed=True)\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_dept_state_sell_price\"})\n        .reset_index()\n    )\n\n\ndef add_average_item_dept_sell_prices(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    item_state_prices = get_item_state_level_sell_prices(sell_prices)\n    dept_state_prices = get_dept_state_level_sell_prices(sell_prices)\n    sell_prices = (\n        sell_prices.merge(\n            right=item_state_prices,\n            on=[\"item_id\", \"state_id\", \"wm_yr_wk\"],\n            how=\"left\"\n        ).merge(\n            right=dept_state_prices,\n            on=[\"dept_id\", \"state_id\", \"wm_yr_wk\"],\n            how=\"left\"\n        )\n    )\n    return sell_prices\n",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "item_state_prices = get_item_state_level_sell_prices(data)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "dept_state_prices = get_dept_state_level_sell_prices(data)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "item_state_prices.isna().sum()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "data[\"sell_price\"].isna().sum()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "data.head()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Save memory\ndel sales_df\n# price_df = price_df[price_df[\"wm_yr_wk\"] >= START_TEST_WM_YR_WK]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "df = pd.DataFrame({'A': [1, 1, 2, 1, 2], 'B': [np.nan, 2, 3, 4, 5], 'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "grouper = df.groupby('A')\nfor group in grouper:\n    break",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "group",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "merged[\"item_id\"].astype(int)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Add temporal features\ncalendar_df = fourier_features(calendar_df)",
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "\nprice_df = add_average_item_dept_sell_prices(price_df)\n\n\n\n# Cast price data\nprice_df = cast_price_data_types(price_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "price_df.head()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndata = merge_sales_and_calendar_data(sales_df, calendar_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "data = merge_sales_and_price_data(data, price_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "data.head()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "\n\n\n\n\n# Fill missing price data\ndata = fill_missing_sell_prices(data)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Map categories to intergers\nitem_id_map, id_item_map = map_category_ids(data, \"item_id\", SUBMISSION_RUN)\ndept_id_map, id_dept_map = map_category_ids(data, \"dept_id\", SUBMISSION_RUN)\ncat_id_map, id_cat_map = map_category_ids(data, \"cat_id\", SUBMISSION_RUN)\nstore_id_map, id_store_map = map_category_ids(data, \"store_id\", SUBMISSION_RUN)\nstate_id_map, id_state_map = map_category_ids(data, \"state_id\", SUBMISSION_RUN)\n\n\n# Set types\ndata = cast_types(data)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# LightGBM Model",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Rolling window validation\n\ndef time_series_split(\n    df: pd.DataFrame,\n    n_folds: int = 5,\n    horizon: int = 28,\n    overlap: int = 0,\n    max_timestamp: int = MAX_TIMESTAMP,\n):\n    min_timestamp = max_timestamp - n_folds * horizon + (n_folds - 1) * overlap\n    for fold_idx in range(n_folds):\n        start = min_timestamp + fold_idx * (horizon - overlap)\n        stop = start + horizon\n\n        train_data = df[df[\"d\"] < start].copy()\n        valid_data = df[(df[\"d\"] >= start) & (df[\"d\"] < stop)].copy()\n        \n        print(f\"Fold index: {fold_idx}\")\n        print(f\"Train idx (start, end): ({train_data['d'].min()}, {train_data['d'].max()})\")\n        print(f\"Valid idx (start, end): ({valid_data['d'].min()}, {valid_data['d'].max()})\")\n        print(\"==================\")\n        \n        yield train_data, valid_data\n        \n        \ndef train_predict_score(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_test: pd.DataFrame,\n    y_test: pd.Series,\n) -> tuple[float, float]:\n    train_data = lgbm.Dataset(data=X_train, label=y_train)\n    valid_data = lgbm.Dataset(data=X_test, label=y_test)\n    model = lgbm.train(\n        PARAMETERS,\n        num_boost_round=500,\n        train_set=train_data,\n        valid_sets=[valid_data],\n        callbacks=[\n            lgbm.early_stopping(stopping_rounds=5),\n            lgbm.log_evaluation(period=10),\n        ]\n    )\n    \n    y_hat = model.predict(X_test)\n    y_hat = np.clip(np.round(y_hat), a_min=0, a_max=np.inf)\n    y_hat = y_hat.astype(np.int32)\n    rmse = np.sqrt(np.mean((y_test - y_hat) ** 2))\n\n    return rmse, model.best_iteration",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "FEATURES = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"weekday\",\n    \"month\",\n    \"year\",\n    \"event_name\",\n    \"event_type\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    \"av_item_state_sell_price\",\n    \"av_dept_state_sell_price\",\n]\nLABEL = \"count\"\n\nPARAMETERS = {\n    \"objective\": \"poisson\",\n    \"learning_rate\": 0.1,\n    \"metric\": \"rmse\",\n    \"force_col_wise\": True,\n}",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    scores = []\n    best_iterations = []\n    \n    for train, valid in tqdm(time_series_split(data, n_folds=5), total=5):\n    \n        X_train, y_train = train[FEATURES], train[LABEL]\n        X_test, y_test = valid[FEATURES], valid[LABEL]\n        score, best_iter = train_predict_score(X_train, y_train, X_test, y_test)\n        scores.append(score)\n        best_iterations.append(best_iter)\n    \n    cv_results = {\n        \"scores\": scores,\n        \"mean_score\": np.mean(scores),\n        \"std_score\": np.std(scores),\n        \"best_iterations\": best_iterations,\n        \"mean_best_iteration\": np.mean(best_iterations)\n    }\n    \n    with open(f\"{OUTPUT_BASE_BATH}/cv_results.json\", \"w\") as f:\n        json.dump(cv_results, f)\n    \nelse:\n    train_data = lgbm.Dataset(data=data[FEATURES], label=data[LABEL])\n    model = lgbm.train(\n        PARAMETERS,\n        num_boost_round=400,\n        train_set=train_data,\n    )",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# FORECAST & SUBMISSION",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def construct_test_df(\n    d_start: int,\n    d_end: int,\n    item_category_df: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n    price_df: pd.DataFrame,\n) -> pd.DataFrame:\n    \n    d_range = list(range(d_start, d_end + 1))\n    d_range_df = pd.DataFrame(d_range, columns=[\"d\"])\n    test_df = pd.merge(item_id_categories, d_range_df, how=\"cross\")\n    test_df = merge_sales_and_calendar_data(test_df, calendar_df)\n    test_df = merge_sales_and_price_data(test_df, price_df)\n    return test_df\n\ndef map_test_df_columns(test_df: pd.DataFrame) -> pd.DataFrame:\n    test_df[\"item_id\"] = test_df[\"item_id\"].map(item_id_map)\n    test_df[\"dept_id\"] = test_df[\"dept_id\"].map(dept_id_map)\n    test_df[\"cat_id\"] = test_df[\"cat_id\"].map(cat_id_map)\n    test_df[\"store_id\"] = test_df[\"store_id\"].map(store_id_map)\n    test_df[\"state_id\"] = test_df[\"state_id\"].map(state_id_map)\n    test_df[\"event_name\"] = test_df[\"event_name\"].map(event_name_map)\n    test_df[\"event_type\"] = test_df[\"event_type\"].map(event_type_map)\n    return test_df\n\ndef pivot_forecast_df(X_test: pd.DataFrame, y_hat: np.ndarray) -> pd.DataFrame:\n    forecast_df = pd.concat([X_test, pd.DataFrame(y_hat, columns=[\"count\"])], axis=1)\n    forecast_df = forecast_df.pivot(columns=\"d\", index=\"id\", values=\"count\").sort_index(axis=1)\n    forecast_df = forecast_df.rename_axis(None, axis=1).reset_index()\n    return forecast_df\n\ndef rename_forecast_columns(forecast_df: pd.DataFrame, forecast_horizon: int = 28) -> pd.DataFrame:\n    forecast_df = forecast_df.set_index(\"id\")\n    forecast_df.columns = [f\"F{i}\" for i in range(1, forecast_horizon + 1)]\n    return forecast_df.reset_index()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if SUBMISSION_RUN:\n    test_d_start, test_d_end = 1942, 1969\n    test_df = construct_test_df(\n        d_start=test_d_start,\n        d_end=test_d_end,\n        item_category_df=item_id_categories,\n        calendar_df=calendar_df,\n        price_df=price_df,\n    )\n    test_df = map_test_df_columns(test_df)\n    test_df = cast_types(test_df)\n\n    y_hat = model.predict(test_df[FEATURES])\n    y_hat = np.clip(np.round(y_hat), a_min=0, a_max=np.inf)\n    y_hat = y_hat.astype(np.int32)\n\n    forecast_df = pivot_forecast_df(test_df, y_hat)\n    forecast_df = rename_forecast_columns(forecast_df)\n    \n    # Merge with sample submissions\n    EVAL_SUBMISSION = EVAL_SUBMISSION[[\"id\"]].merge(forecast_df, on=\"id\", how=\"left\")\n    FINAL_SUBMISSIONS = pd.concat([VAL_SUBMISSION, EVAL_SUBMISSION])\n    FINAL_SUBMISSIONS = FINAL_SUBMISSIONS.set_index(\"id\").reindex(SUBMISSION_INDEX).reset_index()\n    \n    FINAL_SUBMISSIONS.to_csv(f\"{OUTPUT_BASE_BATH}/submission.csv\", index=False)\n    ",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}