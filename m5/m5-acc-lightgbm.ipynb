{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30746,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "SUBMISSION_RUN = True",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "INPUT_BASE_PATH = \"/kaggle/input/m5-forecasting-accuracy\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\nCALENDAR_DATA = pd.read_csv(f\"{INPUT_BASE_PATH}/calendar.csv\")\n# SELL_PRICES = pd.read_csv(f\"{INPUT_BASE_PATH}/sell_prices.csv\")\nSALES_TRAIN_EVALUATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_evaluation.csv\")\n# SALES_TRAIN_VALIDATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_validation.csv\")\n\nSAMPLE_SUBMISSION = pd.read_csv(f\"{INPUT_BASE_PATH}/sample_submission.csv\")\nSUBMISSION_INDEX = SAMPLE_SUBMISSION.set_index(\"id\").index\nVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"validation\")]\nEVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"evaluation\")]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# PREPROCESSING & FEATURE ENGINEERING",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def get_unique_value_id_map(df: pd.DataFrame, col_name: str):\n    return {value_id: i for (i, value_id) in enumerate(df[col_name].unique())}\n\ndef get_item_id_map(sales_df: pd.DataFrame):\n    return get_unique_value_id_map(sales_df, \"item_id\")\n\ndef get_dept_id_map(sales_df: pd.DataFrame):\n    return get_unique_value_id_map(sales_df, \"dept_id\")\n\ndef get_cat_id_map(sales_df: pd.DataFrame):\n    return get_unique_value_id_map(sales_df, \"cat_id\")\n\ndef get_store_id_map(sales_df: pd.DataFrame):\n    return get_unique_value_id_map(sales_df, \"store_id\")\n\ndef get_state_id_map(sales_df: pd.DataFrame):\n    return get_unique_value_id_map(sales_df, \"state_id\")\n\n\ndef flatten_sales_df(sales_df: pd.DataFrame, timestamp_cols: list[str]) -> pd.DataFrame:\n    sales_dfs = []\n    grouper = sales_df.groupby(\n        [\n            \"item_id\",\n            \"dept_id\",\n            \"cat_id\",\n            \"store_id\",\n            \"state_id\"\n        ]\n    )\n    for key, sales_df in tqdm(grouper):\n        (item, dept, cat, store, state) = key\n        sales_df = sales_df[timestamp_cols].T.reset_index()\n        sales_df.columns = [\"d\", \"count\"]\n        sales_df = sales_df.assign(\n            d = sales_df[\"d\"].apply(lambda d: int(d.lstrip(\"d_\"))),\n            item_id = item,\n            dept_id = dept,\n            cat_id = cat,\n            store_id = store,\n            state_id = state,\n        )\n        sales_dfs.append(sales_df)\n    return pd.concat(sales_dfs)\n\n\n\ndef select_format_calendar_features(calendar_data: pd.DataFrame) -> pd.DataFrame:\n    # Drop columns\n    cols_to_drop = [\"wm_yr_wk\", \"weekday\", \"event_name_2\", \"event_type_2\"]\n    calendar_data = calendar_data.copy().drop(columns=cols_to_drop)\n    \n    # Format cols\n    calendar_data[\"d\"] = calendar_data[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    calendar_data = calendar_data.rename(columns={\n        \"event_name_1\": \"event_name\",\n        \"event_type_1\": \"event_type\",\n        \"wday\": \"weekday\"\n    })\n    \n    return calendar_data\n\ndef get_event_name_map(calendar_data: pd.DataFrame):\n    return get_unique_value_id_map(calendar_data, \"event_name\")\n\ndef get_event_type_map(calendar_data: pd.DataFrame):\n    return get_unique_value_id_map(calendar_data, \"event_type\")\n    \n\ndef merge_sales_and_calendar_data(sales_df: pd.DataFrame, calendar_data: pd.DataFrame) -> pd.DataFrame:\n    return sales_df.merge(right=calendar_data, on=\"d\", how=\"left\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# EVALUATION DATASET -- PRIVATE LEADERBOARD",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Store category info for each item id\nitem_id_categories = SALES_TRAIN_EVALUATION[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "start_t = 1 if SUBMISSION_RUN else 1000\ntrain_timestamp_cols = [f\"d_{i}\" for i in range(start_t, 1941)]\n\n# Process sales data\nitem_id_map = get_item_id_map(SALES_TRAIN_EVALUATION)\nid_item_map = {v: k for (k, v) in item_id_map.items()}\nSALES_TRAIN_EVALUATION[\"item_id\"] = SALES_TRAIN_EVALUATION[\"item_id\"].map(item_id_map)\n\ndept_id_map = get_dept_id_map(SALES_TRAIN_EVALUATION)\nid_dept_map = {v: k for (k, v) in dept_id_map.items()}\nSALES_TRAIN_EVALUATION[\"dept_id\"] = SALES_TRAIN_EVALUATION[\"dept_id\"].map(dept_id_map)\n\ncat_id_map = get_cat_id_map(SALES_TRAIN_EVALUATION)\nid_cat_map = {v: k for (k, v) in cat_id_map.items()}\nSALES_TRAIN_EVALUATION[\"cat_id\"] = SALES_TRAIN_EVALUATION[\"cat_id\"].map(cat_id_map)\n\nstore_id_map = get_store_id_map(SALES_TRAIN_EVALUATION)\nid_store_map = {v: k for (k, v) in store_id_map.items()}\nSALES_TRAIN_EVALUATION[\"store_id\"] = SALES_TRAIN_EVALUATION[\"store_id\"].map(store_id_map)\n\nstate_id_map = get_state_id_map(SALES_TRAIN_EVALUATION)\nid_state_map = {v: k for (k, v) in state_id_map.items()}\nSALES_TRAIN_EVALUATION[\"state_id\"] = SALES_TRAIN_EVALUATION[\"state_id\"].map(state_id_map)\n\ntrain_df = flatten_sales_df(SALES_TRAIN_EVALUATION, train_timestamp_cols)\n\n# Process calendar data\ncalendar_data = select_format_calendar_features(CALENDAR_DATA)\nevent_name_map = get_event_name_map(calendar_data)\nevent_type_map = get_event_type_map(calendar_data)\ncalendar_data[\"event_name\"] = calendar_data[\"event_name\"].map(event_name_map)\ncalendar_data[\"event_type\"] = calendar_data[\"event_type\"].map(event_type_map)\n\n# Merge sales and calendar data\ntrain_df = merge_sales_and_calendar_data(train_df, calendar_data)\n\n# Set types\ntrain_df = train_df.astype(\n    {\n        \"item_id\": \"float\",\n        \"dept_id\": \"category\",\n        \"cat_id\": \"category\",\n        \"store_id\": \"category\",\n        \"state_id\": \"category\",\n        \"weekday\": \"category\",\n        \"month\": \"category\",\n        \"year\": \"category\",\n        \"event_name\": \"category\",\n        \"event_type\": \"category\",\n    }\n)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## LightGBM Model",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Rolling window validation\n\ndef time_series_split(\n    df: pd.DataFrame,\n    n_folds: int = 5,\n    horizon: int = 28,\n    overlap: int = 14,\n    max_timestamp: int = 1941,\n):\n    min_timestamp = max_timestamp - n_folds * horizon + (n_folds - 1) * overlap\n    for fold_idx in range(n_folds):\n        start = min_timestamp + fold_idx * (horizon - overlap)\n        stop = start + horizon\n\n        train_data = df[df[\"d\"] < start].copy()\n        valid_data = df[(df[\"d\"] >= start) & (df[\"d\"] < stop)].copy()\n        \n        print(f\"Fold index: {fold_idx}\")\n        print(f\"Train idx (start, end): ({train_data['d'].min()}, {train_data['d'].max()})\")\n        print(f\"Valid idx (start, end): ({valid_data['d'].min()}, {valid_data['d'].max()})\")\n        print(\"==================\")\n        \n        yield train_data, valid_data\n        \n        \ndef train_predict_score(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_test: pd.DataFrame,\n    y_test: pd.Series,\n) -> float:\n    ...\n        ",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def item_level_validation_plot(\n    valid_df: pd.DataFrame,\n    item_id: int,\n):\n    valid_for_item = valid[valid[\"item_id\"] == item_id].copy()\n    store_ids = list(valid_for_item[\"store_id\"].unique())\n\n    fig, ax = plt.subplots(len(store_ids), 1, figsize=(8, 2 * len(store_ids)), sharex=True)\n    colors = cm.viridis(np.linspace(0, 1, len(store_ids)))\n\n    for i, store_id in enumerate(store_ids):\n        valid_for_store = valid_for_item[valid_for_item[\"store_id\"] == store_id]\n        ax[i].plot(\n            valid_for_store[\"date\"].values,\n            valid_for_store[\"count\"].values,\n            label=id_store_map[store_id],\n            color=colors[i]\n        )\n        ax[i].plot(\n            valid_for_store[\"date\"].values,\n            valid_for_store[\"forecast\"].values,\n            color=colors[i],\n            ls=\"--\",\n        )\n        ax[i].set(ylabel=f\"Store {id_store_map[store_id]}\")\n    ax[0].set(title=f\"Item {id_item_map[item_id]}\")\n    for tick in ax[-1].get_xticklabels():\n        tick.set_rotation(45)\n\n    fig.tight_layout();\n\n\ndef aggregate_validation_plot(\n    valid_df: pd.DataFrame,\n    state: str,  #[\"CA\", \"TX\", \"WI\"]\n    cat: str, # [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\n):\n    state_id = state_id_map[state]\n    cat_id = cat_id_map[cat]\n    \n    state_cat_df = valid_df[(valid_df[\"state_id\"] == state_id) & (valid_df[\"cat_id\"] == cat_id)]\n    g = state_cat_df.groupby([\"store_id\", \"dept_id\", \"date\"], observed=True)[[\"count\", \"forecast\"]].sum()\n    \n    store_ids = g.index.get_level_values(\"store_id\").unique()\n    fig, ax = plt.subplots(len(store_ids), 1, figsize=(8, 2 * len(store_ids)), sharex=True)\n\n    for i, store_id in enumerate(store_ids):\n        ts = g.loc[store_id]\n\n        dept_ids = ts.index.get_level_values(\"dept_id\").unique()\n        colors = cm.viridis(np.linspace(0, 1, len(dept_ids)))\n        \n        for j, dept_id in enumerate(dept_ids):\n            ax[i].plot(\n                ts.loc[dept_id].index,\n                ts.loc[dept_id][\"count\"].values,\n                label=id_dept_map[dept_id],\n                color=colors[j]\n            )\n            ax[i].plot(\n                ts.loc[dept_id].index,\n                ts.loc[dept_id][\"forecast\"].values,\n                color=colors[j],\n                ls=\"--\"\n            )\n        ax[i].set(ylabel=\"count\", title=id_store_map[store_id])\n        ax[i].legend()\n\n    for tick in ax[-1].get_xticklabels():\n        tick.set_rotation(45)\n\n    fig.tight_layout()\n    ",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "FEATURES = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"weekday\",\n    \"month\",\n    \"year\",\n    \"event_name\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n]\nLABEL = \"count\"\n\nPARAMETERS = {\n    \"objective\": \"poisson\",\n    \"learning_rate\": 0.1,\n    \"metric\": \"rmse\",\n}",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    for train, valid in time_series_split(train_df):\n        break\n    \n    train_data = lgbm.Dataset(data=train[FEATURES], label=train[LABEL])\n    valid_data = lgbm.Dataset(data=valid[FEATURES], label=valid[LABEL])\n    model = lgbm.train(\n        PARAMETERS,\n        num_boost_round=500,\n        train_set=train_data,\n        valid_sets=[valid_data],\n        callbacks=[\n            lgbm.early_stopping(stopping_rounds=5),\n            lgbm.log_evaluation(period=10)\n        ]\n    )\n    \n    y_hat = model.predict(valid[FEATURES])\n    y_hat = np.clip(np.round(y_hat), a_min=0, a_max=np.inf)\n    y_hat = y_hat.astype(np.int32)\n    valid[\"forecast\"] = y_hat\n    \n\nelse:\n    train_data = lgbm.Dataset(data=train_df[FEATURES], label=train_df[LABEL])\n    model = lgbm.train(\n        PARAMETERS,\n        num_boost_round=400,\n        train_set=train_data,\n    )",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    aggregate_validation_plot(valid, \"WI\", \"HOUSEHOLD\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    item_level_validation_plot(valid, 100)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# FORECAST",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def construct_test_df(\n    d_start: int,\n    d_end: int,\n    item_category_df: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n) -> pd.DataFrame:\n    \n    d_range = list(range(d_start, d_end + 1))\n    d_range_df = pd.DataFrame(d_range, columns=[\"d\"])\n    test_df = pd.merge(item_id_categories, d_range_df, how=\"cross\")\n    test_df = test_df.merge(calendar_df, on=\"d\", how=\"left\")\n    return test_df\n\ndef pivot_forecast_df(X_test: pd.DataFrame, y_hat: np.ndarray) -> pd.DataFrame:\n    forecast_df = pd.concat([X_test, pd.DataFrame(y_hat, columns=[\"count\"])], axis=1)\n    forecast_df = forecast_df.pivot(columns=\"d\", index=\"id\", values=\"count\").sort_index(axis=1)\n    forecast_df = forecast_df.rename_axis(None, axis=1).reset_index()\n    return forecast_df\n\ndef rename_forecast_columns(forecast_df: pd.DataFrame, forecast_horizon: int = 28) -> pd.DataFrame:\n    forecast_df = forecast_df.set_index(\"id\")\n    forecast_df.columns = [f\"F{i}\" for i in range(1, forecast_horizon + 1)]\n    return forecast_df.reset_index()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "test_d_start, test_d_end = 1942, 1969\ntest_df = construct_test_df(\n    d_start=test_d_start,\n    d_end=test_d_end,\n    item_category_df=item_id_categories,\n    calendar_df=calendar_data,\n)\n\ntest_df[\"item_id\"] = test_df[\"item_id\"].map(item_id_map)\ntest_df[\"dept_id\"] = test_df[\"dept_id\"].map(dept_id_map)\ntest_df[\"cat_id\"] = test_df[\"cat_id\"].map(cat_id_map)\ntest_df[\"store_id\"] = test_df[\"store_id\"].map(store_id_map)\ntest_df[\"state_id\"] = test_df[\"state_id\"].map(state_id_map)\ntest_df = test_df.astype(\n    {\n        \"item_id\": \"float\",\n        \"dept_id\": \"category\",\n        \"cat_id\": \"category\",\n        \"store_id\": \"category\",\n        \"state_id\": \"category\",\n        \"weekday\": \"category\",\n        \"month\": \"category\",\n        \"year\": \"category\",\n        \"event_name\": \"category\",\n        \"event_type\": \"category\",\n    }\n)\n\ny_hat = model.predict(test_df[FEATURES])\ny_hat = np.clip(np.round(y_hat), a_min=0, a_max=np.inf)\ny_hat = y_hat.astype(np.int32)\n\nforecast_df = pivot_forecast_df(test_df, y_hat)\nforecast_df = rename_forecast_columns(forecast_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge with sample submissions\nEVAL_SUBMISSION = EVAL_SUBMISSION[[\"id\"]].merge(forecast_df, on=\"id\", how=\"left\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# SUBMISSION",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "FINAL_SUBMISSIONS = pd.concat([VAL_SUBMISSION, EVAL_SUBMISSION])\nFINAL_SUBMISSIONS = FINAL_SUBMISSIONS.set_index(\"id\").reindex(SUBMISSION_INDEX).reset_index()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "FINAL_SUBMISSIONS.to_csv(f\"{OUTPUT_BASE_BATH}/submission.csv\", index=False)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}