{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30746,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:20.486228Z",
                    "iopub.execute_input": "2024-09-07T17:22:20.486748Z",
                    "iopub.status.idle": "2024-09-07T17:22:25.647248Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:20.486709Z",
                    "shell.execute_reply": "2024-09-07T17:22:25.646009Z"
                },
                "trusted": true
            },
            "execution_count": 1,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "SUBMISSION_RUN = False",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:26.375416Z",
                    "iopub.execute_input": "2024-09-07T17:22:26.381040Z",
                    "iopub.status.idle": "2024-09-07T17:22:26.390856Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:26.380970Z",
                    "shell.execute_reply": "2024-09-07T17:22:26.389250Z"
                },
                "trusted": true
            },
            "execution_count": 2,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "INPUT_BASE_PATH = \"/kaggle/input/m5-forecasting-accuracy\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\nCALENDAR_DATA = pd.read_csv(f\"{INPUT_BASE_PATH}/calendar.csv\")\nSELL_PRICES = pd.read_csv(f\"{INPUT_BASE_PATH}/sell_prices.csv\")\nSALES_TRAIN_EVALUATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_evaluation.csv\")\n\nSAMPLE_SUBMISSION = pd.read_csv(f\"{INPUT_BASE_PATH}/sample_submission.csv\")\nSUBMISSION_INDEX = SAMPLE_SUBMISSION.set_index(\"id\").index\nVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"validation\")]\nEVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"evaluation\")]",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:26.844824Z",
                    "iopub.execute_input": "2024-09-07T17:22:26.846349Z",
                    "iopub.status.idle": "2024-09-07T17:22:41.875306Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:26.846284Z",
                    "shell.execute_reply": "2024-09-07T17:22:41.873933Z"
                },
                "trusted": true
            },
            "execution_count": 3,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Constants\nMAX_TIMESTAMP = 1941",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:41.877458Z",
                    "iopub.execute_input": "2024-09-07T17:22:41.877880Z",
                    "iopub.status.idle": "2024-09-07T17:22:41.883466Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:41.877845Z",
                    "shell.execute_reply": "2024-09-07T17:22:41.882202Z"
                },
                "trusted": true
            },
            "execution_count": 4,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# PREPROCESSING & FEATURE ENGINEERING",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Extract categorical mappings\ndef get_unique_value_id_map(df: pd.DataFrame, col_name: str):\n    return {value_id: i for (i, value_id) in enumerate(df[col_name].unique())}\n\n\ndef map_category_ids(sales_df: pd.DataFrame, column_name: str, submission_run: bool):\n    category_id_map = get_unique_value_id_map(sales_df, column_name)\n    id_category_map = {v: k for (k, v) in category_id_map.items()}\n    sales_df[column_name] = sales_df[column_name].map(category_id_map)\n    return (category_id_map, None) if submission_run else (category_id_map, id_category_map)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:41.885104Z",
                    "iopub.execute_input": "2024-09-07T17:22:41.885536Z",
                    "iopub.status.idle": "2024-09-07T17:22:41.899982Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:41.885485Z",
                    "shell.execute_reply": "2024-09-07T17:22:41.898723Z"
                },
                "trusted": true
            },
            "execution_count": 5,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process sales data\ndef unpivot_sales_df(sales_df: pd.DataFrame, timestamp_cols: list[str]) -> pd.DataFrame:\n    sales_df = sales_df.melt(\n        id_vars=[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n        value_vars=timestamp_cols,\n        var_name=\"d\",\n        value_name=\"count\"\n    )\n    sales_df[\"d\"] = sales_df[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    return sales_df\n    ",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:41.903556Z",
                    "iopub.execute_input": "2024-09-07T17:22:41.904332Z",
                    "iopub.status.idle": "2024-09-07T17:22:41.916913Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:41.904258Z",
                    "shell.execute_reply": "2024-09-07T17:22:41.915544Z"
                },
                "trusted": true
            },
            "execution_count": 6,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process calendar data\ndef select_format_calendar_features(calendar_data: pd.DataFrame) -> pd.DataFrame:\n    # Drop columns\n    cols_to_drop = [\"weekday\", \"event_name_2\", \"event_type_2\"]\n    calendar_data = calendar_data.copy().drop(columns=cols_to_drop)\n    \n    # Format cols\n    calendar_data[\"d\"] = calendar_data[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    calendar_data = calendar_data.rename(columns={\n        \"event_name_1\": \"event_name\",\n        \"event_type_1\": \"event_type\",\n        \"wday\": \"weekday\"\n    })\n    \n    return calendar_data",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:41.918548Z",
                    "iopub.execute_input": "2024-09-07T17:22:41.918938Z",
                    "iopub.status.idle": "2024-09-07T17:22:41.929673Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:41.918906Z",
                    "shell.execute_reply": "2024-09-07T17:22:41.928155Z"
                },
                "trusted": true
            },
            "execution_count": 7,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process sell price data\ndef extract_state_and_dept_id(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    return (\n        sell_prices\n        .assign(state_id=sell_prices[\"store_id\"].str.extract(r\"(^[A-Z]+)\"))\n        .assign(dept_id=sell_prices[\"item_id\"].str.extract(r\"(^[A-Z]+_[0-9]+)\"))\n    )\n\n\ndef get_item_state_level_sell_prices(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    return (\n        sell_prices\n        .groupby([\"item_id\", \"state_id\", \"wm_yr_wk\"])\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_item_state_sell_price\"})\n    )\n\n\ndef get_dept_state_level_sell_prices(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    return (\n        sell_prices\n        .groupby([\"dept_id\", \"state_id\", \"wm_yr_wk\"])\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_dept_state_sell_price\"})\n        .reset_index()\n    )\n\n\ndef add_average_item_dept_sell_prices(sell_prices: pd.DataFrame) -> pd.DataFrame:\n    item_state_prices = get_item_state_level_sell_prices(sell_prices)\n    dept_state_prices = get_dept_state_level_sell_prices(sell_prices)\n    sell_prices = (\n        sell_prices.merge(\n            right=item_state_prices,\n            on=[\"item_id\", \"state_id\", \"wm_yr_wk\"],\n            how=\"left\"\n        ).merge(\n            right=dept_state_prices,\n            on=[\"dept_id\", \"state_id\", \"wm_yr_wk\"],\n            how=\"left\"\n        )\n    )\n    return sell_prices\n",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:41.931829Z",
                    "iopub.execute_input": "2024-09-07T17:22:41.933085Z",
                    "iopub.status.idle": "2024-09-07T17:22:41.945623Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:41.933041Z",
                    "shell.execute_reply": "2024-09-07T17:22:41.944193Z"
                },
                "trusted": true
            },
            "execution_count": 8,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndef merge_sales_and_calendar_data(sales_df: pd.DataFrame, calendar_data: pd.DataFrame) -> pd.DataFrame:\n    return sales_df.merge(right=calendar_data, on=\"d\", how=\"left\")\n\n    \ndef merge_sales_and_price_data(sales_df: pd.DataFrame, sell_prices: pd.DataFrame) -> pd.DataFrame:\n    dedupe_item_state_prices = (\n        sell_prices[[\"item_id\", \"state_id\", \"wm_yr_wk\", \"av_item_state_sell_price\"]]\n        .drop_duplicates(subset=[\"item_id\", \"state_id\", \"wm_yr_wk\"])\n    )\n    dedupe_dept_state_prices = (\n        sell_prices[[\"dept_id\", \"state_id\", \"wm_yr_wk\", \"av_dept_state_sell_price\"]]\n        .drop_duplicates(subset=[\"dept_id\", \"state_id\", \"wm_yr_wk\"])\n    )\n    merged = sales_df.merge(\n        right=sell_prices[[\"item_id\", \"store_id\", \"wm_yr_wk\", \"sell_price\"]],\n        on=[\"item_id\", \"store_id\", \"wm_yr_wk\"],\n        how=\"left\"\n    ).merge(\n        right=dedupe_item_state_prices,\n        on=[\"item_id\", \"state_id\", \"wm_yr_wk\"],\n        how=\"left\",\n    ).merge(\n        right=dedupe_dept_state_prices,\n        on=[\"dept_id\", \"state_id\", \"wm_yr_wk\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef fill_missing_sell_prices(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"sell_price\"] = (\n        df[\"sell_price\"]\n        .fillna(df[\"av_item_state_sell_price\"])\n        .fillna(df[\"av_dept_state_sell_price\"])\n    )\n    df[\"av_item_state_sell_price\"] = (\n        df[\"av_item_state_sell_price\"]\n        .fillna(df[\"av_dept_state_sell_price\"])\n    )\n    return df\n    \n    \ndef cast_types(df: pd.DataFrame) -> pd.DataFrame:\n    return df.astype(\n        {\n            \"item_id\": \"float\",\n            \"dept_id\": \"category\",\n            \"cat_id\": \"category\",\n            \"store_id\": \"category\",\n            \"state_id\": \"category\",\n            \"weekday\": \"category\",\n            \"month\": \"category\",\n            \"year\": \"category\",\n            \"event_name\": \"category\",\n            \"event_type\": \"category\",\n            \"sell_price\": \"float\",\n            \"av_item_state_sell_price\": \"float\",\n            \"av_dept_state_sell_price\": \"float\",\n        }\n    )\n    ",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:22:41.947838Z",
                    "iopub.execute_input": "2024-09-07T17:22:41.948268Z",
                    "iopub.status.idle": "2024-09-07T17:22:41.964713Z",
                    "shell.execute_reply.started": "2024-09-07T17:22:41.948230Z",
                    "shell.execute_reply": "2024-09-07T17:22:41.963279Z"
                },
                "trusted": true
            },
            "execution_count": 9,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# EVALUATION DATASET -- PRIVATE LEADERBOARD",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Store category info for each item id\nitem_id_categories = SALES_TRAIN_EVALUATION[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:23:06.472009Z",
                    "iopub.execute_input": "2024-09-07T17:23:06.472626Z",
                    "iopub.status.idle": "2024-09-07T17:23:06.484912Z",
                    "shell.execute_reply.started": "2024-09-07T17:23:06.472580Z",
                    "shell.execute_reply": "2024-09-07T17:23:06.483820Z"
                },
                "trusted": true
            },
            "execution_count": 10,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "start_t = 500 if SUBMISSION_RUN else 1000\ntimestamp_cols = [f\"d_{i}\" for i in range(start_t, MAX_TIMESTAMP + 1)]\n\n# Process sales data\nsales_df = unpivot_sales_df(SALES_TRAIN_EVALUATION, timestamp_cols)\n\n# Process calendar data\ncalendar_df = select_format_calendar_features(CALENDAR_DATA)\n\n# Process price data\nprice_df = extract_state_and_dept_id(SELL_PRICES)\nprice_df = add_average_item_dept_sell_prices(price_df)\n\n# Merge\ndata = merge_sales_and_calendar_data(sales_df, calendar_df)\ndata = merge_sales_and_price_data(data, price_df)\n\n# Fill missing price data\ndata = fill_missing_sell_prices(data)",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2024-09-07T17:23:10.340629Z",
                    "iopub.execute_input": "2024-09-07T17:23:10.341200Z",
                    "iopub.status.idle": "2024-09-07T17:26:38.968575Z",
                    "shell.execute_reply.started": "2024-09-07T17:23:10.341145Z",
                    "shell.execute_reply": "2024-09-07T17:26:38.966875Z"
                },
                "trusted": true
            },
            "execution_count": 11,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Map categories to intergers\nitem_id_map, id_item_map = map_category_ids(data, \"item_id\", SUBMISSION_RUN)\ndept_id_map, id_dept_map = map_category_ids(data, \"dept_id\", SUBMISSION_RUN)\ncat_id_map, id_cat_map = map_category_ids(data, \"cat_id\", SUBMISSION_RUN)\nstore_id_map, id_store_map = map_category_ids(data, \"store_id\", SUBMISSION_RUN)\nstate_id_map, id_state_map = map_category_ids(data, \"state_id\", SUBMISSION_RUN)\nevent_name_map, id_event_name_map = map_category_ids(data, \"event_name\", SUBMISSION_RUN)\nevent_type_map, id_event_type_map = map_category_ids(data, \"event_type\", SUBMISSION_RUN)\n\n# Set types\ndata = cast_types(data)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## LightGBM Model",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Rolling window validation\n\ndef time_series_split(\n    df: pd.DataFrame,\n    n_folds: int = 5,\n    horizon: int = 28,\n    overlap: int = 14,\n    max_timestamp: int = MAX_TIMESTAMP,\n):\n    min_timestamp = max_timestamp - n_folds * horizon + (n_folds - 1) * overlap\n    for fold_idx in range(n_folds):\n        start = min_timestamp + fold_idx * (horizon - overlap)\n        stop = start + horizon\n\n        train_data = df[df[\"d\"] < start].copy()\n        valid_data = df[(df[\"d\"] >= start) & (df[\"d\"] < stop)].copy()\n        \n        print(f\"Fold index: {fold_idx}\")\n        print(f\"Train idx (start, end): ({train_data['d'].min()}, {train_data['d'].max()})\")\n        print(f\"Valid idx (start, end): ({valid_data['d'].min()}, {valid_data['d'].max()})\")\n        print(\"==================\")\n        \n        yield train_data, valid_data\n        \n        \ndef train_predict_score(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_test: pd.DataFrame,\n    y_test: pd.Series,\n) -> float:\n    ...\n        ",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def item_level_validation_plot(\n    valid_df: pd.DataFrame,\n    item_id: int,\n):\n    valid_for_item = valid[valid[\"item_id\"] == item_id].copy()\n    store_ids = list(valid_for_item[\"store_id\"].unique())\n\n    fig, ax = plt.subplots(len(store_ids), 1, figsize=(8, 2 * len(store_ids)), sharex=True)\n    colors = cm.viridis(np.linspace(0, 1, len(store_ids)))\n\n    for i, store_id in enumerate(store_ids):\n        valid_for_store = valid_for_item[valid_for_item[\"store_id\"] == store_id]\n        ax[i].plot(\n            valid_for_store[\"date\"].values,\n            valid_for_store[\"count\"].values,\n            label=id_store_map[store_id],\n            color=colors[i]\n        )\n        ax[i].plot(\n            valid_for_store[\"date\"].values,\n            valid_for_store[\"forecast\"].values,\n            color=colors[i],\n            ls=\"--\",\n        )\n        ax[i].set(ylabel=f\"Store {id_store_map[store_id]}\")\n    ax[0].set(title=f\"Item {id_item_map[item_id]}\")\n    for tick in ax[-1].get_xticklabels():\n        tick.set_rotation(45)\n\n    fig.tight_layout();\n\n\ndef aggregate_validation_plot(\n    valid_df: pd.DataFrame,\n    state: str,  #[\"CA\", \"TX\", \"WI\"]\n    cat: str, # [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\n):\n    state_id = state_id_map[state]\n    cat_id = cat_id_map[cat]\n    \n    state_cat_df = valid_df[(valid_df[\"state_id\"] == state_id) & (valid_df[\"cat_id\"] == cat_id)]\n    g = state_cat_df.groupby([\"store_id\", \"dept_id\", \"date\"], observed=True)[[\"count\", \"forecast\"]].sum()\n    \n    store_ids = g.index.get_level_values(\"store_id\").unique()\n    fig, ax = plt.subplots(len(store_ids), 1, figsize=(8, 2 * len(store_ids)), sharex=True)\n\n    for i, store_id in enumerate(store_ids):\n        ts = g.loc[store_id]\n\n        dept_ids = ts.index.get_level_values(\"dept_id\").unique()\n        colors = cm.viridis(np.linspace(0, 1, len(dept_ids)))\n        \n        for j, dept_id in enumerate(dept_ids):\n            ax[i].plot(\n                ts.loc[dept_id].index,\n                ts.loc[dept_id][\"count\"].values,\n                label=id_dept_map[dept_id],\n                color=colors[j]\n            )\n            ax[i].plot(\n                ts.loc[dept_id].index,\n                ts.loc[dept_id][\"forecast\"].values,\n                color=colors[j],\n                ls=\"--\"\n            )\n        ax[i].set(ylabel=\"count\", title=id_store_map[store_id])\n        ax[i].legend()\n\n    for tick in ax[-1].get_xticklabels():\n        tick.set_rotation(45)\n\n    fig.tight_layout()\n\n    \n    \ndef residual_analysis():\n    # Residuals for different items / stores / states / cats\n    ...",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "FEATURES = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"weekday\",\n    \"month\",\n    \"year\",\n    \"event_name\",\n    \"event_type\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    \"av_item_state_sell_price\",\n    \"av_dept_state_sell_price\",\n]\nLABEL = \"count\"\n\nPARAMETERS = {\n    \"objective\": \"poisson\",\n    \"learning_rate\": 0.2,\n    \"metric\": \"rmse\",\n}",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    for train, valid in time_series_split(data):\n        break\n    \n    train_data = lgbm.Dataset(data=train[FEATURES], label=train[LABEL])\n    valid_data = lgbm.Dataset(data=valid[FEATURES], label=valid[LABEL])\n    model = lgbm.train(\n        PARAMETERS,\n        num_boost_round=600,\n        train_set=train_data,\n        valid_sets=[valid_data],\n        callbacks=[\n            lgbm.early_stopping(stopping_rounds=5),\n            lgbm.log_evaluation(period=10)\n        ]\n    )\n    \n    y_hat = model.predict(valid[FEATURES])\n    y_hat = np.clip(np.round(y_hat), a_min=0, a_max=np.inf)\n    y_hat = y_hat.astype(np.int32)\n    valid[\"forecast\"] = y_hat\n    \n\nelse:\n    train_data = lgbm.Dataset(data=data[FEATURES], label=data[LABEL])\n    model = lgbm.train(\n        PARAMETERS,\n        num_boost_round=350,\n        train_set=train_data,\n    )",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Error Analysis",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    aggregate_validation_plot(valid, \"WI\", \"HOUSEHOLD\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    item_level_validation_plot(valid, 100)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# FORECAST",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def construct_test_df(\n    d_start: int,\n    d_end: int,\n    item_category_df: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n    price_df: pd.DataFrame,\n) -> pd.DataFrame:\n    \n    d_range = list(range(d_start, d_end + 1))\n    d_range_df = pd.DataFrame(d_range, columns=[\"d\"])\n    test_df = pd.merge(item_id_categories, d_range_df, how=\"cross\")\n    test_df = merge_sales_and_calendar_data(test_df, calendar_df)\n    test_df = merge_sales_and_price_data(test_df, price_df)\n    return test_df\n\ndef map_test_df_columns(test_df: pd.DataFrame) -> pd.DataFrame:\n    test_df[\"item_id\"] = test_df[\"item_id\"].map(item_id_map)\n    test_df[\"dept_id\"] = test_df[\"dept_id\"].map(dept_id_map)\n    test_df[\"cat_id\"] = test_df[\"cat_id\"].map(cat_id_map)\n    test_df[\"store_id\"] = test_df[\"store_id\"].map(store_id_map)\n    test_df[\"state_id\"] = test_df[\"state_id\"].map(state_id_map)\n    test_df[\"event_name\"] = test_df[\"event_name\"].map(event_name_map)\n    test_df[\"event_type\"] = test_df[\"event_type\"].map(event_type_map)\n    return test_df\n\ndef pivot_forecast_df(X_test: pd.DataFrame, y_hat: np.ndarray) -> pd.DataFrame:\n    forecast_df = pd.concat([X_test, pd.DataFrame(y_hat, columns=[\"count\"])], axis=1)\n    forecast_df = forecast_df.pivot(columns=\"d\", index=\"id\", values=\"count\").sort_index(axis=1)\n    forecast_df = forecast_df.rename_axis(None, axis=1).reset_index()\n    return forecast_df\n\ndef rename_forecast_columns(forecast_df: pd.DataFrame, forecast_horizon: int = 28) -> pd.DataFrame:\n    forecast_df = forecast_df.set_index(\"id\")\n    forecast_df.columns = [f\"F{i}\" for i in range(1, forecast_horizon + 1)]\n    return forecast_df.reset_index()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "test_d_start, test_d_end = 1942, 1969\ntest_df = construct_test_df(\n    d_start=test_d_start,\n    d_end=test_d_end,\n    item_category_df=item_id_categories,\n    calendar_df=calendar_df,\n    price_df=price_df,\n)\n\ntest_df = map_test_df_columns(test_df)\ntest_df = cast_types(test_df)\n\n\ny_hat = model.predict(test_df[FEATURES])\ny_hat = np.clip(np.round(y_hat), a_min=0, a_max=np.inf)\ny_hat = y_hat.astype(np.int32)\n\nforecast_df = pivot_forecast_df(test_df, y_hat)\nforecast_df = rename_forecast_columns(forecast_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge with sample submissions\nEVAL_SUBMISSION = EVAL_SUBMISSION[[\"id\"]].merge(forecast_df, on=\"id\", how=\"left\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# SUBMISSION",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "FINAL_SUBMISSIONS = pd.concat([VAL_SUBMISSION, EVAL_SUBMISSION])\nFINAL_SUBMISSIONS = FINAL_SUBMISSIONS.set_index(\"id\").reindex(SUBMISSION_INDEX).reset_index()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "FINAL_SUBMISSIONS.to_csv(f\"{OUTPUT_BASE_BATH}/submission.csv\", index=False)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}