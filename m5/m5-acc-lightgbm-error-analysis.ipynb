{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                },
                {
                    "sourceId": 9467934,
                    "sourceType": "datasetVersion",
                    "datasetId": 5684895
                }
            ],
            "dockerImageVersionId": 30746,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import json\nimport gc\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Constants\n\n# Paths\nINPUT_BASE_PATH = \"/kaggle/input/\"\nRAW_DATA_INPUT_PATH = f\"{INPUT_BASE_PATH}/m5-forecasting-accuracy\"\nPROCESSED_DATA_INPUT_PATH = f\"{INPUT_BASE_PATH}/m5-acc\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\n# Timestamps\nMAX_TRAIN_TIMESTAMP = 1941\nSTART_TEST_TIMESTAMP = 1942\nSTART_TEST_WM_YR_WK = 11617",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Data Input",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Load raw data\n\n# CALENDAR_DATA = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/calendar.csv\")\n# SELL_PRICES = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/sell_prices.csv\")\n# SALES_TRAIN_EVALUATION = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/sales_train_evaluation.csv\")\n\nSAMPLE_SUBMISSION = pd.read_csv(f\"{RAW_DATA_INPUT_PATH}/sample_submission.csv\")\nSUBMISSION_INDEX = SAMPLE_SUBMISSION.set_index(\"id\").index\nVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"validation\")]\nEVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"evaluation\")]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Load preprocessed data\ntrain_data_set = pd.read_parquet(f\"{PROCESSED_DATA_INPUT_PATH}/m5-acc-train.parquet\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Drop any columns that we dont need for training or analysis\ncols_to_drop = [\"av_store_dept_sales\", \"wm_yr_wk\"]\ntry:\n    train_data_set = train_data_set.drop(columns=cols_to_drop)\nexcept KeyError:\n    print(f\"Columns not found in axis. Skipping ...\")\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Cast to lower resolution types to save memory\ndef cast_data_types(dataset: pd.DataFrame) -> pd.DataFrame:\n    item_category_cols = [\"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    date_category_cols = [\"wm_yr_wk\", \"weekday\", \"month\", \"year\", \"event_name\", \"event_type\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]\n    price_category_cols = [\"item_on_sale\"]\n    for category_col in item_category_cols + date_category_cols + price_category_cols:\n        try:\n            dataset[category_col] = dataset[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Column {category_col} does not exist. Skipping ...\")\n    \n    int_cols = [\"d\", \"count\"]\n    for int_col in int_cols:\n        try:\n            dataset[int_col] = pd.to_numeric(dataset[int_col], downcast=\"integer\")\n        except KeyError:\n            print(f\"Column {int_col} does not exist. Skipping ...\")\n            \n    float16_cols = [\"item_id\", \"sell_price\", \"weekday_sin\", \"weekday_cos\", \"month_sin\", \"month_cos\"]\n    for float_col in float16_cols:\n        try:\n            dataset[float_col] = dataset[float_col].astype(np.float16)\n        except KeyError:\n            print(f\"Column {int_col} does not exist. Skipping ...\")\n    \n    return dataset\n\ntrain_data_set = cast_data_types(train_data_set)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# LightGBM Model",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "CATEGORICAL_FEATURES = [\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"year\",\n    \"event_name\",\n    \"event_type\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"item_on_sale\",\n]\nCONTINOUS_FEATURES = [\n    \"item_id\",\n    \"weekday_sin\",\n    \"weekday_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"sell_price\",\n    \"sell_price_diff_1\",\n    \"sell_price_diff_2\",\n    \"sell_price_diff_3\",\n    \"sell_price_diff_7\",\n#     \"av_item_state_sell_price\",\n#     \"av_dept_state_sell_price\",\n    \"count_lag_28\",\n    \"count_lag_29\",\n    \"count_lag_30\",\n    \"count_lag_31\",\n    \"av_store_dept_sales_lag_28\",\n    \"av_store_dept_sales_lag_29\",\n    \"av_store_dept_sales_lag_30\",\n    \"av_store_dept_sales_lag_31\"\n]\nFEATURES = CATEGORICAL_FEATURES + CONTINOUS_FEATURES\nLABEL = \"count\"\n\n# Parameters\nTRAIN_PARAMETERS = {\n    \"objective\": \"tweedie\",\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 2 ** 7 - 1,\n    \"max_bin\": 2 ** 7 - 1,\n    \"metric\": \"rmse\",\n    \"force_col_wise\": True,\n}",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Train model\nhorizon = 28\nstart_t = MAX_TRAIN_TIMESTAMP - horizon\nend_t = MAX_TRAIN_TIMESTAMP\n\ntrain_df = train_data_set[train_data_set[\"d\"] < start_t]\nvalid_df = train_data_set[(train_data_set[\"d\"] >= start_t) & (train_data_set[\"d\"] <= end_t)]\nprint(f\"Train idx (start, end): ({train_df['d'].min()}, {train_df['d'].max()})\")\nprint(f\"Valid idx (start, end): ({valid_df['d'].min()}, {valid_df['d'].max()})\")\nprint(\"============\")\n\nX_train, y_train = train_df[FEATURES], train_df[LABEL]\nX_valid, y_valid = valid_df[FEATURES], valid_df[LABEL]\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_valid.shape}, y_test shape: {y_valid.shape}\")\n\ntrain_data = lgbm.Dataset(data=X_train, label=y_train)\nvalid_data = lgbm.Dataset(data=X_valid, label=y_valid)\n\nmodel = lgbm.train(\n    TRAIN_PARAMETERS,\n    num_boost_round=30,\n    train_set=train_data,\n    valid_sets=[valid_data],\n    callbacks=[\n        lgbm.early_stopping(stopping_rounds=5),\n        lgbm.log_evaluation(period=10),\n    ]\n)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Forecast and save to onto valid df\ny_hat = model.predict(data=X_valid)\ny_hat = np.clip(y_hat, a_min=0, a_max=np.inf)\ny_hat = y_hat.astype(np.int32)\n\nvalid_df.loc[:, \"forecast\"] = y_hat",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## LightGBM Model",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def item_level_validation_plot(\n    valid_df: pd.DataFrame,\n    item_id: int,\n):\n    \"\"\"\n    Plots forecast and ground truth sales for a given item id across all stores.\n    \n    :param valid_df: DataFrame containing forecast and ground truth sales data\n        by item and store id.\n    :param item_id: The item id to plot sales data for.\n    \"\"\"\n    valid_for_item = valid_df[valid_df[\"item_id\"] == item_id]\n    store_ids = list(valid_for_item[\"store_id\"].unique())\n\n    fig, ax = plt.subplots(len(store_ids), 1, figsize=(8, 2 * len(store_ids)), sharex=True)\n    colors = cm.viridis(np.linspace(0, 1, len(store_ids)))\n\n    for i, store_id in enumerate(store_ids):\n        valid_for_store = valid_for_item[valid_for_item[\"store_id\"] == store_id]\n        ax[i].plot(\n            valid_for_store[\"date\"].values,\n            valid_for_store[\"count\"].values,\n#             label=id_store_map[store_id],\n            color=colors[i]\n        )\n        ax[i].plot(\n            valid_for_store[\"date\"].values,\n            valid_for_store[\"forecast\"].values,\n            color=colors[i],\n            ls=\"--\",\n        )\n#         ax[i].set(ylabel=f\"Store {id_store_map[store_id]}\")\n    \n#     ax[0].set(title=f\"Item {id_item_map[item_id]}\")\n    for tick in ax[-1].get_xticklabels():\n        tick.set_rotation(45)\n\n    fig.tight_layout();\n\n\ndef aggregate_validation_plot(\n    valid_df: pd.DataFrame,\n    state: str,  #[\"CA\", \"TX\", \"WI\"]\n    cat: str, # [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\n):\n    state_id = state_id_map[state]\n    cat_id = cat_id_map[cat]\n    \n    state_cat_df = valid_df[(valid_df[\"state_id\"] == state_id) & (valid_df[\"cat_id\"] == cat_id)]\n    g = state_cat_df.groupby([\"store_id\", \"dept_id\", \"date\"], observed=True)[[\"count\", \"forecast\"]].sum()\n    \n    store_ids = g.index.get_level_values(\"store_id\").unique()\n    fig, ax = plt.subplots(len(store_ids), 1, figsize=(8, 2 * len(store_ids)), sharex=True)\n\n    for i, store_id in enumerate(store_ids):\n        ts = g.loc[store_id]\n\n        dept_ids = ts.index.get_level_values(\"dept_id\").unique()\n        colors = cm.viridis(np.linspace(0, 1, len(dept_ids)))\n        \n        for j, dept_id in enumerate(dept_ids):\n            ax[i].plot(\n                ts.loc[dept_id].index,\n                ts.loc[dept_id][\"count\"].values,\n                label=id_dept_map[dept_id],\n                color=colors[j]\n            )\n            ax[i].plot(\n                ts.loc[dept_id].index,\n                ts.loc[dept_id][\"forecast\"].values,\n                color=colors[j],\n                ls=\"--\"\n            )\n#         ax[i].set(ylabel=\"count\", title=id_store_map[store_id])\n        ax[i].legend()\n\n    for tick in ax[-1].get_xticklabels():\n        tick.set_rotation(45)\n\n    fig.tight_layout()\n\n    \n    \ndef residual_analysis():\n    # Residuals for different items / stores / states / cats\n    ...",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "item_id = 0\nitem_level_validation_plot(valid_df, item_id)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "",
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}