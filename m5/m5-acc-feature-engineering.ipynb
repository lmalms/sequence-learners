{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30761,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import gc\nimport json\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', None)",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Load raw datasets",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "INPUT_BASE_PATH = \"/kaggle/input/m5-forecasting-accuracy\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\nCALENDAR_DATA = pd.read_csv(f\"{INPUT_BASE_PATH}/calendar.csv\")\nSELL_PRICES = pd.read_csv(f\"{INPUT_BASE_PATH}/sell_prices.csv\")\nSALES_TRAIN_EVALUATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_evaluation.csv\")\n\nSAMPLE_SUBMISSION = pd.read_csv(f\"{INPUT_BASE_PATH}/sample_submission.csv\")\nSUBMISSION_INDEX = SAMPLE_SUBMISSION.set_index(\"id\").index\nVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"validation\")]\nEVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"evaluation\")]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Constants\nMAX_TRAIN_TIMESTAMP = 1941\nSTART_TEST_TIMESTAMP = 1942\nSTART_TEST_WM_YR_WK = 11617\n\n# Feature engineering constants\nSALES_LAG_PERIODS = [28, 29, 30, 31]\nSALES_ROLLING_WINDOWS = [7, 14, 21, 28]\nSALES_ROLLING_AGG_FUNCS = [\"mean\", \"std\", \"kurt\"]\n\nPRICE_LAG_PERIODS = [1, 2, 3, 7]\nPRICE_ROLLING_WINDOWS = [7, 14, 21, 28]\nPRICE_ROLLING_AGG_FUNCS = [\"mean\", \"std\", \"kurt\"]\n\nEVENT_LAG_PERIODS = [-3, -2, -1, 1, 2, 3]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Data preprocessing and feature engineering",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Extract categorical mappings\ndef get_unique_value_id_map(df: pd.DataFrame, col_name: str):\n    return {value_id: i for (i, value_id) in enumerate(df[col_name].unique())}\n\n\ndef map_category_ids(\n    df: pd.DataFrame,\n    column_name: str,\n    category_id_map: dict | None = None,\n    submission_run: bool = False\n) -> pd.DataFrame:\n    if category_id_map is None:\n        category_id_map = get_unique_value_id_map(df, column_name)\n    id_category_map = {v: k for (k, v) in category_id_map.items()}\n    df[column_name] = df[column_name].map(category_id_map)\n    return (category_id_map, None) if submission_run else (category_id_map, id_category_map)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Preprocess sales data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process sales data\ndef unpivot_sales_df(sales_df: pd.DataFrame, timestamp_cols: list[str]) -> pd.DataFrame:\n    sales_df = sales_df.melt(\n        id_vars=[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n        value_vars=timestamp_cols,\n        var_name=\"d\",\n        value_name=\"count\"\n    )\n    sales_df[\"d\"] = sales_df[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    sales_df = sales_df.sort_values(by=[\"store_id\", \"item_id\", \"d\"]).reset_index(drop=True)\n    return sales_df\n\n\ndef cast_sales_data_types(sales_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    for category_col in category_cols:\n        try:\n            sales_df[category_col] = sales_df[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Column {category_col} does not exist. Skipping.\")\n        \n    column_downcast_map = [(\"d\", np.int16), (\"count\", np.int16)]\n    for (col, dtype) in column_downcast_map:\n        try:\n            sales_df[col] = sales_df[col].astype(dtype)\n        except KeyError:\n            print(f\"Column {col} does not exist. Skipping.\")\n    \n    return sales_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Preprocess calendar data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process calendar data\ndef select_format_calendar_features(calendar_data: pd.DataFrame) -> pd.DataFrame:\n    # Drop columns\n    cols_to_drop = [\"weekday\", \"wday\", \"month\", \"year\", \"event_name_2\", \"event_type_2\"]\n    calendar_data = calendar_data.copy().drop(columns=cols_to_drop)\n    \n    # Format cols\n    calendar_data[\"d\"] = calendar_data[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    calendar_data = calendar_data.rename(columns={\n        \"event_name_1\": \"event_name\",\n        \"event_type_1\": \"event_type\",\n        \"wday\": \"weekday\"\n    })\n    \n    return calendar_data\n\n\ndef cast_calendar_data_types(calendar_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\n        \"weekday\",\n        \"month\",\n        \"year\",\n        \"wm_yr_wk\",\n        \"event_name\",\n        \"event_type\",\n        \"snap_CA\",\n        \"snap_TX\",\n        \"snap_WI\"\n    ]\n    for category_col in category_cols:\n        try:\n            calendar_df[category_col] = calendar_df[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Col {category_col} does not exist. Skipping ...\")\n    \n    try:\n        calendar_df[\"d\"] = calendar_df[\"d\"].astype(np.int16)\n    except KeyError:\n            print(f\"Col 'd' does not exist. Skipping ...\")\n    \n    return calendar_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Preprocess price data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process sell price data\ndef cast_price_data_types(price_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\"item_id\", \"store_id\", \"wm_yr_wk\"]\n    for category_col in category_cols:\n        try:\n            price_df[category_col] = price_df[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Col {category_col} does not exist. Skipping ...\")\n    \n    price_df[\"sell_price\"] = price_df[\"sell_price\"].astype(np.float16)\n    return price_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Merge raw datasets",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndef merge_sales_calendar_price_data(\n    sales_df: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n    price_df: pd.DataFrame,\n    chunk_size: int = 500_000,\n) -> pd.DataFrame:\n    chunks = []\n    for chunk_start in tqdm(range(0, len(sales_df), chunk_size)):\n        sales_chunk = sales_df.iloc[chunk_start: chunk_start + chunk_size]\n        merged_chunk = sales_chunk.merge(right=calendar_df, on=\"d\", how=\"left\")\n        merged_chunk = merged_chunk.merge(right=price_df, on=[\"item_id\", \"store_id\", \"wm_yr_wk\"], how=\"left\")\n        chunks.append(merged_chunk)\n    return pd.concat(chunks, ignore_index=True)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Feature engineering",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "### Datetime features",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Feature engineering funcs\ndef datetime_features(data: pd.DataFrame) -> pd.DataFrame:\n    dates = pd.to_datetime(data[\"date\"])\n    data[\"weekday\"] = dates.dt.weekday.astype(\"category\")\n    data[\"month\"] = dates.dt.month.astype(\"category\")\n    data[\"quarter\"] = dates.dt.quarter.astype(\"category\")\n    data[\"year\"] = dates.dt.year.astype(\"category\")\n    return data\n\n\ndef fourier_features(data: pd.DataFrame) -> pd.DataFrame:\n    data[\"weekday_sin\"] = np.sin((2 * np.pi * data[\"weekday\"].astype(float)) / 7)\n    data[\"weekday_cos\"] = np.cos((2 * np.pi * data[\"weekday\"].astype(float)) / 7)\n    \n    data[\"month_sin\"] = np.sin((2 * np.pi * data[\"month\"].astype(float)) / 12)\n    data[\"month_cos\"] = np.cos((2 * np.pi * data[\"month\"].astype(float)) / 12)\n    \n    data[\"quarter_sin\"] = np.sin((2 * np.pi * data[\"quarter\"].astype(float)) / 4)\n    data[\"quarter_cos\"] = np.cos((2 * np.pi * data[\"quarter\"].astype(float)) / 4)\n    \n    freqs = [\"weekday\", \"month\", \"quarter\"]\n    cols = [f\"{freq}_{trig}\" for freq in freqs for trig in (\"sin\", \"cos\")]\n    for col in cols:\n        data[col] = data[col].astype(np.float16)\n    \n    return data",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "### Item in stock feature",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def item_on_sale_feature(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Adds an indicator feature of whether an item is in stock right now.\n    An item is regarded out of stock if its 'sell_price' is NaN.\n    \n    :param data: Dataframe with 'sell_price' column.\n    :return: Dataframe with item_on_sale column added.\n    \"\"\"\n    # When price is NaN sale count is 0\n    item_on_sale = data[\"sell_price\"].notnull()\n    data[\"item_on_sale\"] = item_on_sale.astype(int).astype(\"category\")\n    return data",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "### Aggregate sales and price features",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def merge_in_chunks(\n    left: pd.DataFrame,\n    right: pd.DataFrame,\n    merge_keys: list[str],\n    target_col_from_right: str,\n    how: str = \"left\",\n    chunk_size: int = 500_000\n) -> pd.Series:\n    \"\"\"\n    Merge two dataframes in chucks and return the target column from the\n    right dataframe.\n    \n    :param left: Left, typically larger dataframe to join onto.\n    :param right: Right dataframe containing target feature to add to left.\n    :param merge_keys: The keys to merge on.\n    :param target_cols_from_right: The target column from the right dataframe\n        to return.\n    :param how: How to merge.\n    :param chunk_size: Number of rows to merge on each iteration.\n    :return: pd.Series. The target column from right. This can be inserted\n        directly into left outside of the function:\n        ```\n        left[target_col] = merge_in_chunks(...)\n        ```\n    \"\"\"\n    # Only select columns needed for the merge\n    left_slice_df = left[merge_keys]\n    \n    chunks = []\n    for chunk_start in tqdm(range(0, len(left_slice_df), chunk_size)):\n        left_chunk = left_slice_df.iloc[chunk_start: chunk_start + chunk_size]\n        merged_chunk = left_chunk.merge(right=right, on=merge_keys, how=how)\n        chunks.append(merged_chunk[target_col_from_right])\n     \n    return pd.concat(chunks, ignore_index=True)\n\n\ndef av_store_dept_sales(data: pd.DataFrame, chunk_size: int = 500_000) -> pd.DataFrame:\n    \"\"\"\n    Gets the average number of sales of all products in a given department and store.\n    \n    :param sales_df: Sales data frame containing 'store_id', 'dept_id', 'd', and 'count' columns.\n    :param chunk_size: Dataframe chunk_size to use for merging. Defaults to 500_000\n    :return: Dataframe with 'av_store_dept_sales' column added.\n    \"\"\"\n    av_store_dept_sales = (\n        data\n        .groupby([\"store_id\", \"dept_id\", \"d\"], observed=True)\n        [[\"count\"]]\n        .mean()\n        .rename(columns={\"count\": \"av_store_dept_sales\"})\n        .reset_index()\n    )\n    \n    # Merge into data in chunks\n    merged_sales_series = merge_in_chunks(\n        left=data,\n        right=av_store_dept_sales,\n        merge_keys=[\"store_id\", \"dept_id\", \"d\"],\n        target_col_from_right=\"av_store_dept_sales\",\n        how=\"left\",\n        chunk_size=chunk_size,\n    )\n    data[\"av_store_dept_sales\"] = merged_sales_series.astype(np.float16)\n    \n    return data\n\n\ndef av_item_state_prices(data: pd.DataFrame, chunk_size: int = 500_000) -> pd.DataFrame:\n    \"\"\"\n    Gets the average sell price for an item across states.\n    \n    :param data: Dataframe with 'item_id', 'state_id', 'd', and 'sell_price' columns.\n    :param chunk_size: Dataframe chunk_size to use for merging. Defaults to 500_000\n    :return: Dataframe with 'av_item_state_sell_price' column added.\n    \"\"\"\n    av_price = (\n        data\n        .groupby([\"item_id\", \"state_id\", \"d\"], observed=True)\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_item_state_sell_price\"})\n        .reset_index()\n    )\n    \n    # Merge into data in chunks\n    merged_price_series = merge_in_chunks(\n        left=data,\n        right=av_price,\n        merge_keys=[\"item_id\", \"state_id\", \"d\"],\n        target_col_from_right=\"av_item_state_sell_price\",\n        how=\"left\",\n        chunk_size=chunk_size,\n    )\n    data[\"av_item_state_sell_price\"] = merged_price_series.astype(np.float16)\n    \n    return data\n\n\ndef av_dept_state_price(data: pd.DataFrame, chunk_size: int = 500_000) -> pd.DataFrame:\n    \"\"\"\n    Gets the average sell price of all items across departments and states.\n    \n    :param data: Dataframe with 'dept_id', 'state_id', 'd' and 'sell_price' columns.\n    :param chunk_size: Dataframe chunk_size to use for merging. Defaults to 500_000.\n    :return: Dataframe with 'av_dept_state_sell_price' column added.\n    \"\"\"\n    av_price = (\n        data\n        .groupby([\"dept_id\", \"state_id\", \"d\"], observed=True)\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_dept_state_sell_price\"})\n        .reset_index()\n    )\n    \n    # Merge onto data in chunks\n    merged_price_series = merge_in_chunks(\n        left=data,\n        right=av_price,\n        merge_keys=[\"dept_id\", \"state_id\", \"d\"],\n        target_col_from_right=\"av_dept_state_sell_price\",\n        how=\"left\",\n        chunk_size=chunk_size,\n    )\n    data[\"av_dept_state_sell_price\"] = merged_price_series.astype(np.float16)\n    \n    return data",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "### Lagged and rolling features",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def lagged_features(\n    data: pd.DataFrame,\n    feature_columns: list[str],\n    periods: list[int]\n) -> pd.DataFrame:\n    lagged_dfs = []\n    for _, group_df in tqdm(data.groupby([\"item_id\", \"store_id\"], observed=True)):\n        group_df = group_df.sort_values(\"d\")\n        for p in periods:\n            lag_feature_names = [f\"{c}_lag_{p}\" for c in feature_columns]\n            group_df[lag_feature_names] = group_df[feature_columns].shift(p)\n        lagged_dfs.append(group_df)\n    all_lags = pd.concat(lagged_dfs, ignore_index=True)\n    return all_lags\n\n\ndef rolling_features(\n    data: pd.DataFrame,\n    feature_column: str,\n    windows: list[int],\n    agg_funcs: list[str]\n) -> pd.DataFrame:\n    rolling_dfs = []\n    for _, group_df in tqdm(data.groupby([\"item_id\", \"store_id\"], observed=True)):\n        group_df = group_df.sort_values(\"d\")\n        \n        window_dfs = []\n        for w in windows:\n            window_df = group_df[feature_column].rolling(w).aggregate(agg_funcs)\n            window_df = window_df.astype(np.float16)\n            \n            col_names = [f\"{feature_column}_rolling_{f}_window_{w}\" for f in agg_funcs]\n            window_df.columns = col_names\n            window_dfs.append(window_df)\n        \n        all_window_dfs = pd.concat(window_dfs, axis=1)\n        rolling_dfs.append(pd.concat([group_df, all_window_dfs], axis=1))\n    all_rolling_dfs = pd.concat(rolling_dfs, axis=0, ignore_index=True)\n    return all_rolling_dfs",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Lagged event features\n\ndef lagged_event_features(\n    data: pd.DataFrame,\n    periods: list[int],\n    no_event_name_category: int,\n    no_event_type_category: int\n) -> pd.DataFrame:\n\n    data = lagged_features(\n        data=data,\n        feature_columns=[\"event_name\", \"event_type\"],\n        periods=periods\n    )\n    \n    # Fill missing values\n    event_name_lag_cols = [f\"event_name_lag_{p}\" for p in periods]\n    data[event_name_lag_cols] = data[event_name_lag_cols].fillna(no_event_name_category)\n    data[event_name_lag_cols] = data[event_name_lag_cols].astype(\"category\")\n    \n    event_type_lag_cols = [f\"event_type_lag_{p}\" for p in periods]\n    data[event_type_lag_cols] = data[event_type_lag_cols].fillna(no_event_type_category)\n    data[event_type_lag_cols] = data[event_type_lag_cols].astype(\"category\")\n\n    return data",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Lagged and rolling target features\n\ndef lagged_target_features(data: pd.DataFrame, periods: list[int]) -> pd.DataFrame:\n    data = lagged_features(data=data, feature_columns=[\"count\"], periods=periods)\n    lagged_target_cols = [f\"count_lag_{p}\" for p in periods]\n    data[lagged_target_cols] = data[lagged_target_cols].astype(np.float16)\n    return data\n\n\ndef lagged_rolling_target_features(\n    data: pd.DataFrame,\n    windows: list[int],\n    agg_funcs: list[str],\n    lag: int = 28,\n) -> pd.DataFrame:\n    data = rolling_features(data, f\"count_lag_{lag}\", windows=windows, agg_funcs=agg_funcs)\n    return data",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Lagged and rolling price features\n\ndef diff_price_features(data: pd.DataFrame, periods: list[int]) -> pd.DataFrame:\n    diff_dfs = []\n    for _, group_df in tqdm(data.groupby([\"item_id\", \"store_id\"], observed=True)):\n        group_df = group_df.sort_values(\"d\")\n        for p in periods:\n            diff_series = group_df[\"sell_price\"].diff(p)\n            group_df[f\"sell_price_diff_{p}\"] = diff_series.astype(np.float16)\n        diff_dfs.append(group_df)\n    return pd.concat(diff_dfs, ignore_index=True)\n\n\ndef rolling_price_features(data: pd.DataFrame, windows: list[int], agg_funcs: list[str]) -> pd.DataFrame:\n    data = rolling_features(data, \"sell_price\", windows=windows, agg_funcs=agg_funcs)\n    return data",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Training data",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Sales data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process sales data\n\n# Map item ids to categories\nitem_id_map, id_item_map = map_category_ids(SALES_TRAIN_EVALUATION, \"item_id\")\ndept_id_map, id_dept_map = map_category_ids(SALES_TRAIN_EVALUATION, \"dept_id\")\ncat_id_map, id_cat_map = map_category_ids(SALES_TRAIN_EVALUATION, \"cat_id\")\nstore_id_map, id_store_map = map_category_ids(SALES_TRAIN_EVALUATION, \"store_id\")\nstate_id_map, id_state_map = map_category_ids(SALES_TRAIN_EVALUATION, \"state_id\")\n\n# Store category info for each item id\nITEM_ID_CATEGORIES = SALES_TRAIN_EVALUATION[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]\n\n# Convert sales table to long table format\ntimestamp_cols = [f\"d_{i}\" for i in range(1, MAX_TRAIN_TIMESTAMP + 1)]\nsales_df = unpivot_sales_df(SALES_TRAIN_EVALUATION, timestamp_cols)\n\n# The following ops are memory intensive so downcast datatypes to save on mem\nsales_df = cast_sales_data_types(sales_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Calendar data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process calendar data\ncalendar_df = select_format_calendar_features(CALENDAR_DATA)\n\n# Map event categories\nevent_name_map, id_event_name_map = map_category_ids(calendar_df, \"event_name\")\nevent_type_map, id_event_type_map = map_category_ids(calendar_df, \"event_type\")\n\n# Cast datatypes\ncalendar_df = cast_calendar_data_types(calendar_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Price data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process price data\nitem_id_map, id_item_map = map_category_ids(SELL_PRICES, \"item_id\", item_id_map)\nstore_id_map, id_store_map = map_category_ids(SELL_PRICES, \"store_id\", store_id_map)\n\nprice_df = cast_price_data_types(SELL_PRICES)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Combine and engineer new features",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndata = merge_sales_calendar_price_data(sales_df, calendar_df, price_df)\ndata = data.drop(\"wm_yr_wk\", axis=1)\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Feature engineering\n\n# Aggregate sales and price features\n# data = av_item_state_prices(data)\n# data = av_dept_state_price(data)\n# data = av_store_dept_sales(data)\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Add lagged event features\ndata = lagged_event_features(\n    data=data,\n    periods=EVENT_LAG_PERIODS,\n    no_event_name_category=event_name_map[np.nan],\n    no_event_type_category=event_type_map[np.nan],\n)\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Lagged and rolling price features\ndata = diff_price_features(\n    data=data,\n    periods=PRICE_LAG_PERIODS\n)\ndata = rolling_price_features(\n    data=data,\n    windows=PRICE_ROLLING_WINDOWS,\n    agg_funcs=PRICE_ROLLING_AGG_FUNCS\n)\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Lagged and rolling target features\ndata = lagged_target_features(\n    data=data,\n    periods=SALES_LAG_PERIODS\n)\ndata = lagged_rolling_target_features(\n    data=data,\n    windows=SALES_ROLLING_WINDOWS,\n    agg_funcs=SALES_ROLLING_AGG_FUNCS\n)\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Item in stock feature\ndata = item_on_sale_feature(data)\n\n# Datetime features\ndata = datetime_features(data)\ndata = fourier_features(data)\n\n_ = gc.collect()",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Save",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Save to parquet\ndata.to_parquet(f\"{OUTPUT_BASE_BATH}/m5-acc-train.parquet\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Save category maps\ndef save_category_map(category_map: dict, name: str) -> None:\n    file_name = f\"{OUTPUT_BASE_BATH}/{name}.json\"\n    with open(file_name, \"w\") as fp:\n        json.dump(category_map, fp)\n\nmaps_to_names = [\n    (item_id_map, \"item_id_map\"),\n    (dept_id_map, \"dept_id_map\"),\n    (cat_id_map, \"cat_id_map\"),\n    (store_id_map, \"store_id_map\"),\n    (state_id_map, \"state_id_map\"),\n    (event_name_map, \"event_name_map\"),\n    (event_type_map, \"event_type_map\")\n]\nfor category_map, map_name in maps_to_names:\n    save_category_map(category_map, map_name)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Test data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def construct_test_df(\n    d_start: int,\n    d_end: int,\n    item_category_ids: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n    price_df: pd.DataFrame,\n) -> pd.DataFrame:\n    d_range = list(range(d_start, d_end + 1))\n    d_range_df = pd.DataFrame(d_range, columns=[\"d\"])\n    test_df = pd.merge(ITEM_ID_CATEGORIES, d_range_df, how=\"cross\")\n    test_df = cast_sales_data_types(test_df)\n    test_df = merge_sales_calendar_price_data(test_df, calendar_df, price_df)\n    return test_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def add_lagged_event_features_from_training_set(\n    test_df: pd.DataFrame,\n    train_df: pd.DataFrame,\n    periods: list[int],\n    no_event_name_category: int,\n    no_event_type_category: int,\n) -> pd.DataFrame:\n    \n    # Select columns needed for merge\n    train_df_slice = train_df[[\"item_id\", \"store_id\", \"d\", \"event_name\", \"event_type\"]]\n    train_df_slice = train_df_slice[train_df_slice[\"d\"] >= START_TEST_TIMESTAMP - max(periods)]\n    test_df = pd.concat([test_df, train_df_slice])\n    \n    test_df = lagged_event_features(\n        data=test_df,\n        periods=periods,\n        no_event_name_category=no_event_name_category,\n        no_event_type_category=no_event_type_category\n    )\n    \n    test_df = test_df[test_df[\"d\"] >= START_TEST_TIMESTAMP]\n    return test_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def add_lagged_target_features_from_training_set(\n    test_df: pd.DataFrame,\n    train_df: pd.DataFrame,\n    periods: list[int],\n) -> pd.DataFrame:\n    # Select columns needed for merge\n    train_df_slice = train_df[[\"item_id\", \"store_id\", \"d\", \"count\"]]\n    train_df_slice = train_df_slice.rename(columns={\"d\": \"d_train\"})\n    \n    # Merge onto test\n    for p in tqdm(periods):\n        test_df[f\"d_lag_{p}\"] = test_df[\"d\"] - p\n        test_df = test_df.merge(\n            train_df_slice.rename(columns={\"count\": f\"count_lag_{p}\"}),\n            left_on=[\"item_id\", \"store_id\", f\"d_lag_{p}\"],\n            right_on=[\"item_id\", \"store_id\", \"d_train\"],\n            how=\"left\",\n        )\n        test_df[f\"count_lag_{p}\"] = test_df[f\"count_lag_{p}\"].astype(np.float16)\n        test_df = test_df.drop(columns=[f\"d_lag_{p}\", \"d_train\"])\n    return test_df\n\n\ndef add_lagged_rolling_target_features_from_training_set(\n    test_df: pd.DataFrame,\n    train_df: pd.DataFrame,\n    windows: list[int],\n    agg_funcs: list[str],\n    lag: int = 28,\n) -> pd.DataFrame:\n    # Select columns needed for merge\n    train_df_slice = train_df[[\"item_id\", \"store_id\", \"d\", \"count\"]]\n    \n    # Compute rolling features\n    rolling_features_df = rolling_features(\n        data=train_df_slice,\n        feature_column=\"count\",\n        windows=windows,\n        agg_funcs=agg_funcs,\n    )\n    \n    # Rename cols to include lag\n    rolling_feature_cols = [c for c in rolling_features_df if c.startswith(\"count_rolling_\")]\n    new_feature_col_names = [c.replace(\"count\", f\"count_lag_{lag}\") for c in rolling_feature_cols]\n    old_new_col_name_map = dict(zip(rolling_feature_cols, new_feature_col_names))\n    rolling_features_df = rolling_features_df.rename(columns=old_new_col_name_map)\n    \n    # Merge onto test\n    test_df[f\"d_lag_{lag}\"] = test_df[\"d\"] - lag\n    rolling_features_df = rolling_features_df.rename(columns={\"d\": \"d_train\"})\n    test_df = test_df.merge(\n        rolling_features_df.drop(\"count\", axis=1),\n        left_on=[\"item_id\", \"store_id\", f\"d_lag_{lag}\"],\n        right_on=[\"item_id\", \"store_id\", \"d_train\"],\n        how=\"left\",\n    )\n    test_df = test_df.drop(columns=[f\"d_lag_{lag}\", \"d_train\"])\n    return test_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "def add_sale_price_diffs_from_training_set(\n    test_df: pd.DataFrame,\n    train_df: pd.DataFrame,\n    periods: list[int],\n) -> pd.DataFrame:\n    \n    # Select data needed for merge\n    train_df_slice = train_df[[\"item_id\", \"store_id\", \"d\", \"sell_price\"]]\n    train_df_slice = train_df_slice[train_df_slice[\"d\"] >= START_TEST_TIMESTAMP - max(periods)]\n    test_df = pd.concat([test_df, train_df_slice])\n    \n    # Cast types\n    test_df[\"item_id\"] = test_df[\"item_id\"].astype(\"category\")\n    test_df[\"store_id\"] = test_df[\"store_id\"].astype(\"category\")\n    test_df[\"d\"] = test_df[\"d\"].astype(np.int16)\n    test_df[\"sell_price\"] = test_df[\"sell_price\"].astype(np.float16)\n    \n    # Compute diffs\n    test_df = diff_price_features(test_df, periods=periods)\n    test_df = test_df[test_df[\"d\"] >= START_TEST_TIMESTAMP]\n    return test_df\n\n\ndef add_rolling_price_features_from_training_set(\n    test_df: pd.DataFrame,\n    train_df: pd.DataFrame,\n    windows: list[int],\n    agg_funcs: list[str],\n) -> pd.DataFrame:\n    \n    # Select columns needed for merge\n    train_df_slice = data[[\"item_id\", \"store_id\", \"d\", \"sell_price\"]]\n    train_df_slice = train_df_slice[train_df_slice[\"d\"] >= START_TEST_TIMESTAMP - max(windows)]\n    test_df = pd.concat([test_df, train_df_slice])\n    \n    test_df[\"item_id\"] = test_df[\"item_id\"].astype(\"category\")\n    test_df[\"store_id\"] = test_df[\"store_id\"].astype(\"category\")\n    test_df[\"d\"] = test_df[\"d\"].astype(np.int16)\n    test_df[\"sell_price\"] = test_df[\"sell_price\"].astype(np.float16)\n    \n    # Compute rolling price features\n    test_df = rolling_price_features(test_df, windows=windows, agg_funcs=agg_funcs)\n    test_df = test_df[test_df[\"d\"] >= START_TEST_TIMESTAMP]\n    return test_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "test_d_start, test_d_end = 1942, 1969\ntest_df = construct_test_df(\n    d_start=test_d_start,\n    d_end=test_d_end,\n    item_category_ids=ITEM_ID_CATEGORIES,\n    calendar_df=calendar_df,\n    price_df=price_df,\n)\ntest_df = test_df.drop(\"wm_yr_wk\", axis=1)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Lagged events features\ntest_df = add_lagged_event_features_from_training_set(\n    test_df=test_df,\n    train_df=data,\n    periods=EVENT_LAG_PERIODS,\n    no_event_name_category=event_name_map[np.nan],\n    no_event_type_category=event_type_map[np.nan]\n)\n\n# Lagged sales features\ntest_df = add_lagged_target_features_from_training_set(\n    test_df=test_df,\n    train_df=data,\n    periods=SALES_LAG_PERIODS,\n)\ntest_df = add_lagged_rolling_target_features_from_training_set(\n    test_df=test_df,\n    train_df=data,\n    windows=SALES_ROLLING_WINDOWS,\n    agg_funcs=SALES_ROLLING_AGG_FUNCS,\n    lag=28\n)\n\n# Lagged price features\ntest_df = add_sale_price_diffs_from_training_set(\n    test_df=test_df,\n    train_df=data,\n    periods=PRICE_LAG_PERIODS,\n)\ntest_df = add_rolling_price_features_from_training_set(\n    test_df=test_df,\n    train_df=data,\n    windows=PRICE_ROLLING_WINDOWS,\n    agg_funcs=PRICE_ROLLING_AGG_FUNCS\n)\n\n# Item in stock feature\ntest_df = item_on_sale_feature(test_df)\n\n# Datetime features\ntest_df = datetime_features(test_df)\ntest_df = fourier_features(test_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "test_df.to_parquet(f\"{OUTPUT_BASE_BATH}/m5-acc-test.parquet\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}