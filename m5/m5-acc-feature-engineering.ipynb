{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 18599,
                    "databundleVersionId": 1236839,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30761,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import json\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport lightgbm as lgbm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Load raw datasets",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "INPUT_BASE_PATH = \"/kaggle/input/m5-forecasting-accuracy\"\nOUTPUT_BASE_BATH = \"/kaggle/working\"\n\nCALENDAR_DATA = pd.read_csv(f\"{INPUT_BASE_PATH}/calendar.csv\")\nSELL_PRICES = pd.read_csv(f\"{INPUT_BASE_PATH}/sell_prices.csv\")\nSALES_TRAIN_EVALUATION = pd.read_csv(f\"{INPUT_BASE_PATH}/sales_train_evaluation.csv\")\n\nSAMPLE_SUBMISSION = pd.read_csv(f\"{INPUT_BASE_PATH}/sample_submission.csv\")\nSUBMISSION_INDEX = SAMPLE_SUBMISSION.set_index(\"id\").index\nVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"validation\")]\nEVAL_SUBMISSION = SAMPLE_SUBMISSION[SAMPLE_SUBMISSION[\"id\"].str.contains(\"evaluation\")]",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Constants\nMAX_TRAIN_TIMESTAMP = 1941\nSTART_TEST_TIMESTAMP = 1942\nSTART_TEST_WM_YR_WK = 11617\n\nLAG_PERIODS = range(28, 32)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Utility functions for preprocessing and feature engineering",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Extract categorical mappings\ndef get_unique_value_id_map(df: pd.DataFrame, col_name: str):\n    return {value_id: i for (i, value_id) in enumerate(df[col_name].unique())}\n\n\ndef map_category_ids(\n    df: pd.DataFrame,\n    column_name: str,\n    category_id_map: dict | None = None,\n    submission_run: bool = False\n) -> pd.DataFrame:\n    if category_id_map is None:\n        category_id_map = get_unique_value_id_map(df, column_name)\n    id_category_map = {v: k for (k, v) in category_id_map.items()}\n    df[column_name] = df[column_name].map(category_id_map)\n    return (category_id_map, None) if submission_run else (category_id_map, id_category_map)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process sales data\ndef unpivot_sales_df(sales_df: pd.DataFrame, timestamp_cols: list[str]) -> pd.DataFrame:\n    sales_df = sales_df.melt(\n        id_vars=[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n        value_vars=timestamp_cols,\n        var_name=\"d\",\n        value_name=\"count\"\n    )\n    sales_df[\"d\"] = sales_df[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    sales_df = sales_df.sort_values(by=[\"store_id\", \"item_id\", \"d\"]).reset_index(drop=True)\n    return sales_df\n\n\ndef cast_sales_data_types(sales_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    for category_col in category_cols:\n        try:\n            sales_df[category_col] = sales_df[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Column {category_col} does not exist. Skipping.\")\n            pass\n        \n    column_downcast_map = [(\"item_id\", \"float\"), (\"d\", \"integer\"), (\"count\", \"integer\")]\n    for (col, dtype) in column_downcast_map:\n        try:\n            sales_df[col] = pd.to_numeric(sales_df[col], downcast=dtype)\n        except KeyError:\n            print(f\"Column {col} does not exist. Skipping.\")\n            pass\n    \n    return sales_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process calendar data\ndef select_format_calendar_features(calendar_data: pd.DataFrame) -> pd.DataFrame:\n    # Drop columns\n    cols_to_drop = [\"weekday\", \"event_name_2\", \"event_type_2\"]\n    calendar_data = calendar_data.copy().drop(columns=cols_to_drop)\n    \n    # Format cols\n    calendar_data[\"d\"] = calendar_data[\"d\"].apply(lambda d: int(d.lstrip(\"d_\")))\n    calendar_data = calendar_data.rename(columns={\n        \"event_name_1\": \"event_name\",\n        \"event_type_1\": \"event_type\",\n        \"wday\": \"weekday\"\n    })\n    \n    return calendar_data\n\n\ndef cast_calendar_data_types(calendar_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\n        \"weekday\",\n        \"month\",\n        \"year\",\n        \"wm_yr_wk\",\n        \"event_name\",\n        \"event_type\",\n        \"snap_CA\",\n        \"snap_TX\",\n        \"snap_WI\"\n    ]\n    for category_col in category_cols:\n        try:\n            calendar_df[category_col] = calendar_df[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Col {category_col} does not exist. Skipping ...\")\n    \n    try:\n        calendar_df[\"d\"] = pd.to_numeric(calendar_df[\"d\"], downcast=\"integer\")\n    except KeyError:\n            print(f\"Col 'd' does not exist. Skipping ...\")\n    \n    return calendar_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Process sell price data\ndef cast_price_data_types(price_df: pd.DataFrame) -> pd.DataFrame:\n    category_cols = [\"store_id\", \"wm_yr_wk\"]\n    for category_col in category_cols:\n        try:\n            price_df[category_col] = price_df[category_col].astype(\"category\")\n        except KeyError:\n            print(f\"Col {category_col} does not exist. Skipping ...\")\n    \n    for float_col in [\"item_id\", \"sell_price\"]:\n        try:\n            price_df[float_col] = pd.to_numeric(price_df[float_col], downcast=\"float\")\n        except KeyError:\n            print(f\"Col {float_col} does not exist. Skipping ...\")\n    \n    return price_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndef merge_sales_calendar_price_data(\n    sales_df: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n    price_df: pd.DataFrame,\n    chunk_size: int = 500_000,\n) -> pd.DataFrame:\n    chunks = []\n    for chunk_start in tqdm(range(0, len(sales_df), chunk_size)):\n        sales_chunk = sales_df.iloc[chunk_start: chunk_start + chunk_size]\n        merged_chunk = (\n            sales_chunk.merge(\n                right=calendar_df,\n                on=\"d\",\n                how=\"left\",\n            ).merge(\n                right=price_df,\n                on=[\"item_id\", \"store_id\", \"wm_yr_wk\"],\n                how=\"left\",\n            )\n        )\n        chunks.append(merged_chunk)\n    return pd.concat(chunks, ignore_index=True)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Feature engineering funcs\n\ndef fourier_features(data: pd.DataFrame) -> pd.DataFrame:\n    data[\"weekday_sin\"] = np.sin((2 * np.pi * data[\"weekday\"].astype(float)) / 7)\n    data[\"weekday_cos\"] = np.cos((2 * np.pi * data[\"weekday\"].astype(float)) / 7)\n    data[\"month_sin\"] = np.sin((2 * np.pi * data[\"month\"].astype(float)) / 12)\n    data[\"month_cos\"] = np.cos((2 * np.pi * data[\"month\"].astype(float)) / 12)\n    \n    for col in [\"weekday_sin\", \"weekday_cos\", \"month_sin\", \"month_cos\"]:\n        data[col] = pd.to_numeric(data[col], downcast=\"float\")\n    \n    return data\n\n\ndef av_store_dept_sales(sales_df: pd.DataFrame, chunk_size: int = 500_000) -> pd.DataFrame:\n    av_store_dept_sales = (\n        sales_df\n        .groupby([\"store_id\", \"dept_id\", \"d\"], observed=True)\n        [[\"count\"]]\n        .mean()\n        .rename(columns={\"count\": \"av_store_dept_sales\"})\n        .reset_index()\n    )\n    \n    # Merge in chunks then concatenate\n    store_dept_d_df = data[[\"store_id\", \"dept_id\", \"d\"]]\n    chunks = []\n    for chunk_start in tqdm(range(0, len(store_dept_d_df), chunk_size)):\n        store_dept_d_chunk = store_dept_d_df.iloc[chunk_start: chunk_start + chunk_size]\n        merged_chunk = store_dept_d_chunk.merge(\n            right=av_store_dept_sales,\n            on=[\"store_id\", \"dept_id\", \"d\"],\n            how=\"left\"\n        )\n        chunks.append(merged_chunk[\"av_store_dept_sales\"])\n    merged_av_sales = pd.concat(chunks, ignore_index=True)\n    data[\"av_store_dept_sales\"] = merged_av_sales\n    \n    # Cast col types\n    for sales_col in [\"count\", \"av_store_dept_sales\"]:\n        data[sales_col] = pd.to_numeric(data[sales_col], downcast=\"float\")\n    \n    return data\n\n\ndef av_item_state_prices(\n    data: pd.DataFrame,\n    chunk_size: int = 500_000,\n) -> pd.DataFrame:\n    av_price = (\n        data\n        .groupby([\"item_id\", \"state_id\", \"d\"], observed=True)\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_item_state_sell_price\"})\n        .reset_index()\n    )\n    \n    # Merge in chunks then concatenate\n    item_state_d_df = data[[\"item_id\", \"state_id\", \"d\"]]\n    chunks = []\n    for chunk_start in tqdm(range(0, len(item_state_d_df), chunk_size)):\n        item_state_d_chunk = item_state_d_df.iloc[chunk_start: chunk_start + chunk_size]\n        merged_chunk = item_state_d_chunk.merge(\n            right=av_price,\n            on=[\"item_id\", \"state_id\", \"d\"],\n            how=\"left\"\n        )\n        chunks.append(merged_chunk[\"av_item_state_sell_price\"])\n    merged_av_price = pd.concat(chunks, ignore_index=True)\n    data[\"av_item_state_sell_price\"] = merged_av_price\n    \n    # Cast col types\n    for price_col in [\"sell_price\", \"av_item_state_sell_price\"]:\n        data[price_col] = pd.to_numeric(data[price_col], downcast=\"float\")\n    \n    return data\n\n\ndef av_dept_state_price(\n    data: pd.DataFrame,\n    chunk_size: int = 500_000\n) -> pd.DataFrame:\n    av_price = (\n        data\n        .groupby([\"dept_id\", \"state_id\", \"d\"], observed=True)\n        [[\"sell_price\"]]\n        .mean()\n        .rename(columns={\"sell_price\": \"av_dept_state_sell_price\"})\n        .reset_index()\n    )\n    \n    # Merge in chunks then concatenate\n    dept_state_d_df = data[[\"dept_id\", \"state_id\", \"d\"]]\n    chunks = []\n    for chunk_start in tqdm(range(0, len(dept_state_d_df), chunk_size)):\n        dept_state_d_chunk = dept_state_d_df.iloc[chunk_start: chunk_start + chunk_size]\n        merged_chunk = dept_state_d_chunk.merge(\n            right=av_price,\n            on=[\"dept_id\", \"state_id\", \"d\"],\n            how=\"left\"\n        )\n        chunks.append(merged_chunk[\"av_dept_state_sell_price\"])\n    merged_av_price = pd.concat(chunks, ignore_index=True)\n    data[\"av_dept_state_sell_price\"] = merged_av_price\n    \n    # Cast col types\n    for price_col in [\"sell_price\", \"av_dept_state_sell_price\"]:\n        data[price_col] = pd.to_numeric(data[price_col], downcast=\"float\")\n    \n    return data\n\n\ndef lagged_sales_features(sales_df: pd.DataFrame, periods: list[int]) -> pd.DataFrame:\n    # TODO: Consider applying the same merge + concat approach here\n    # rather than storing the whole df in lagged_dfs\n    lagged_dfs = []\n    for _, group_df in tqdm(sales_df.groupby([\"item_id\", \"store_id\"], observed=True)):\n        group_df = group_df.sort_values(\"d\")\n        for p in periods:\n            lag_series = group_df[\"count\"].shift(p)\n            group_df[f\"count_lag_{p}\"] = pd.to_numeric(lag_series, downcast=\"float\")\n        lagged_dfs.append(group_df)\n    return pd.concat(lagged_dfs, ignore_index=True)\n\n\ndef cast_lagged_sales_features(sales_df: pd.DataFrame, periods: list[int]) -> pd.DataFrame:\n    lag_column_names = [f\"count_lag_{p}\" for p in periods]\n    for column_name in lag_column_names:\n        sales_df[column_name] = pd.to_numeric(sales_df[column_name], downcast=\"float\")\n    return sales_df\n\n\ndef lagged_av_store_dept_sales_features(sales_df: pd.DataFrame, periods: list[int]) -> pd.DataFrame:\n    # TODO: Consider applying the same merge + concat approach here\n    # rather than storing the whole df in lagged_dfs\n    lagged_dfs = []\n    for _, group_df in tqdm(sales_df.groupby([\"item_id\", \"store_id\"], observed=True)):\n        group_df = group_df.sort_values(\"d\")\n        for p in periods:\n            lag_series = group_df[\"av_store_dept_sales\"].shift(p)\n            group_df[f\"av_store_dept_sales_lag_{p}\"] = pd.to_numeric(lag_series, downcast=\"float\")\n        lagged_dfs.append(group_df)\n    return pd.concat(lagged_dfs, ignore_index=True)\n\n\ndef cast_lagged_av_sales_features(sales_df: pd.DataFrame, periods: list[int]) -> pd.DataFrame:\n    lag_column_names = [\"av_store_dept_sales\"] + [f\"av_store_dept_sales_lag_{p}\" for p in periods]\n    for column_name in lag_column_names:\n        sales_df[column_name] = pd.to_numeric(sales_df[column_name], downcast=\"float\")\n    return sales_df\n\n\ndef lagged_sell_price_diff_features(\n    data: pd.DataFrame,\n    price_column: str,\n    periods: list[int],\n) -> pd.DataFrame:\n    # TODO: Consider applying the same merge + concat approach here\n    # rather than storing the whole df in lagged_dfs\n    lagged_dfs = []\n    for _, group_df in tqdm(data.groupby([\"item_id\", \"store_id\"], observed=True)):\n        group_df = group_df.sort_values(\"d\")\n        for p in periods:\n            lag_price_column = f\"{price_column}_diff_{p}\"\n            lag_series = group_df[price_column].diff(p)\n            group_df[lag_price_column] = pd.to_numeric(lag_series, downcast=\"float\")\n        lagged_dfs.append(group_df)\n    return pd.concat(lagged_dfs, ignore_index=True)\n",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Training data",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Sales data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process sales data\n\n# Map item ids to categories\nitem_id_map, id_item_map = map_category_ids(SALES_TRAIN_EVALUATION, \"item_id\", submission_run=False)\ndept_id_map, id_dept_map = map_category_ids(SALES_TRAIN_EVALUATION, \"dept_id\", submission_run=False)\ncat_id_map, id_cat_map = map_category_ids(SALES_TRAIN_EVALUATION, \"cat_id\", submission_run=False)\nstore_id_map, id_store_map = map_category_ids(SALES_TRAIN_EVALUATION, \"store_id\", submission_run=False)\nstate_id_map, id_state_map = map_category_ids(SALES_TRAIN_EVALUATION, \"state_id\", submission_run=False)\n\n# Store category info for each item id\nITEM_ID_CATEGORIES = SALES_TRAIN_EVALUATION[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]]\n\n# Convert sales table to long table format\ntimestamp_cols = [f\"d_{i}\" for i in range(1, MAX_TRAIN_TIMESTAMP + 1)]\nsales_df = unpivot_sales_df(SALES_TRAIN_EVALUATION, timestamp_cols)\n\n# The following ops are memory intensive so downcast datatypes to save on mem\nsales_df = cast_sales_data_types(sales_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Calendar data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process calendar data\ncalendar_df = select_format_calendar_features(CALENDAR_DATA)\n\n# Map event categories\nevent_name_map, id_event_name_map = map_category_ids(calendar_df, \"event_name\", submission_run=False)\nevent_type_map, id_event_type_map = map_category_ids(calendar_df, \"event_type\", submission_run=False)\n\n# Cast datatypes\ncalendar_df = cast_calendar_data_types(calendar_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Price data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Process price data\nitem_id_map, id_item_map = map_category_ids(SELL_PRICES, \"item_id\", item_id_map, submission_run=False)\nstore_id_map, id_store_map = map_category_ids(SELL_PRICES, \"store_id\", store_id_map, submission_run=False)\n\nprice_df = cast_price_data_types(SELL_PRICES)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Combine and engineer new features",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "data = pd.read_parquet(f\"{OUTPUT_BASE_BATH}/m5-acc-train-intermediate-2.parquet\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "data = cast_sales_data_types(data)\ndata = cast_calendar_data_types(data)\ndata = cast_price_data_types(data)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "data = lagged_sell_price_features(data, \"sell_price\", [1])",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Merge\ndata = merge_sales_calendar_price_data(sales_df, calendar_df, price_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "# Feature engineering\ndata = av_item_state_prices(data)\ndata = av_dept_state_price(data)\ndata = av_store_dept_sales(data)\n\n# Add lagged sales data\ndata = lagged_sales_features(data, periods=LAG_PERIODS)\ndata = lagged_av_store_dept_sales_features(data, periods=LAG_PERIODS)\n\n# Fourier features\ndata = fourier_features(data)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Save",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Save to parquet\ndata.to_parquet(f\"{OUTPUT_BASE_BATH}/m5-acc-train.parquet\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "# Test data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def construct_test_df(\n    d_start: int,\n    d_end: int,\n    item_category_ids: pd.DataFrame,\n    calendar_df: pd.DataFrame,\n    price_df: pd.DataFrame,\n) -> pd.DataFrame:\n    d_range = list(range(d_start, d_end + 1))\n    d_range_df = pd.DataFrame(d_range, columns=[\"d\"])\n    test_df = pd.merge(ITEM_ID_CATEGORIES, d_range_df, how=\"cross\")\n    test_df = cast_sales_data_types(test_df)\n    test_df = merge_sales_calendar_price_data(test_df, calendar_df, price_df)\n    return test_df\n\n\ndef add_lagged_sales_features_from_training_set(\n    test_df: pd.DataFrame,\n    train_df: pd.DataFrame,\n    periods: list[int],\n) -> pd.DataFrame:\n    # Select columns needed for merge\n    lagged_train_cols = [f\"count_lag_{p}\" for p in periods]\n    train_df_slice = train_df[[\"item_id\", \"store_id\", \"d\"] + lagged_train_cols]\n    train_df_slice = train_df_slice.rename(columns={\"d\": \"d_train\"})\n    \n    # Merge onto test\n    for p in tqdm(periods):\n        test_df[f\"d_lag_{p}\"] = test_df[\"d\"] - p\n        test_df = test_df.merge(\n            train_df_slice[[\"item_id\", \"store_id\", \"d_train\", f\"count_lag_{p}\"]],\n            left_on=[\"item_id\", \"store_id\", f\"d_lag_{p}\"],\n            right_on=[\"item_id\", \"store_id\", \"d_train\"],\n            how=\"left\",\n        )\n        test_df = test_df.drop(columns=[f\"d_lag_{p}\", \"d_train\"])\n    return test_df\n\n\ndef add_lagged_av_sales_features_from_training_set(\n    test_df: pd.DataFrame,\n    train_df: pd.DataFrame,\n    periods: list[int],\n) -> pd.DataFrame:\n    # Select columns needed for merge\n    lagged_train_cols = [f\"av_store_dept_sales_lag_{p}\" for p in periods]\n    train_df_slice = train_df[[\"item_id\", \"store_id\", \"d\"] + lagged_train_cols]\n    train_df_slice = train_df_slice.rename(columns={\"d\": \"d_train\"})\n    \n    # Merge onto test\n    for p in tqdm(periods):\n        test_df[f\"d_lag_{p}\"] = test_df[\"d\"] - p\n        test_df = test_df.merge(\n            train_df_slice[[\"item_id\", \"store_id\", \"d_train\", f\"av_store_dept_sales_lag_{p}\"]],\n            left_on=[\"item_id\", \"store_id\", f\"d_lag_{p}\"],\n            right_on=[\"item_id\", \"store_id\", \"d_train\"],\n            how=\"left\",\n        )\n        test_df = test_df.drop(columns=[f\"d_lag_{p}\", \"d_train\"])\n    return test_df",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "test_d_start, test_d_end = 1942, 1969\ntest_df = construct_test_df(\n    d_start=test_d_start,\n    d_end=test_d_end,\n    item_category_ids=ITEM_ID_CATEGORIES,\n    calendar_df=calendar_df,\n    price_df=price_df,\n)\n\ntest_df = add_lagged_sales_features_from_training_set(\n    test_df=test_df,\n    train_df=data,\n    periods=LAG_PERIODS,\n)\ntest_df = add_lagged_av_sales_features_from_training_set(\n    test_df=test_df,\n    train_df=data,\n    periods=LAG_PERIODS,\n)\n\n# Feature engineering\ntest_df = av_item_state_prices(test_df)\ntest_df = av_dept_state_price(test_df)\ntest_df = fourier_features(test_df)",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": "test_df.to_parquet(f\"{OUTPUT_BASE_BATH}/m5-acc-test.parquet\")",
            "metadata": {
                "trusted": true
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}