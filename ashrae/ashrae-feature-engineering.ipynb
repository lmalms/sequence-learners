{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 9994,
                    "databundleVersionId": 752467,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30786,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "from datetime import datetime, date\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import savgol_filter\nfrom tqdm import tqdm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Constants\nINPUT_DATA_PATH = \"/kaggle/input/ashrae-energy-prediction\"\n\nMIN_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-01-01 00:00:00\")\nMAX_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-12-31 23:00:00\")\nMIN_TEST_TIMESTAMP = pd.Timestamp(\"2017-01-01 00:00:00\")\nMAX_TEST_TIMESTAMP = pd.Timestamp('2018-12-31 23:00:00')\nDATA_RESOLUTION = \"1h\"\n\nWEATHER_FEATURE_COLUMNS = [\n    'air_temperature',\n    'cloud_coverage',\n    'dew_temperature',\n    'precip_depth_1_hr',\n    'sea_level_pressure',\n    'wind_direction',\n    'wind_speed'\n]",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "def cast_readings_data(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"building_id\"] = df[\"building_id\"].astype(\"category\")\n    df[\"meter_id\"] = df[\"meter_id\"].astype(\"category\")\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    try:\n        df[\"meter_reading\"] = df[\"meter_reading\"].astype(np.float32)\n    except KeyError:\n        pass\n    return df\n\n\ndef cast_weather_data(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"site_id\"] = df[\"site_id\"].astype(\"category\")\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    for col in WEATHER_FEATURE_COLUMNS:\n        df[col] = df[col].astype(np.float32)\n    return df\n\n\ndef cast_buildings_data(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"site_id\"] = df[\"site_id\"].astype(\"category\")\n    df[\"building_id\"] = df[\"building_id\"].astype(\"category\")\n    df[\"primary_use\"] = df[\"primary_use\"].astype(\"category\")\n    for col in [\"square_feet\", \"year_built\", \"floor_count\"]:\n        df[col] = df[col].astype(np.float32)\n    return df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Train data",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "### Load raw data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Meter readings\nreadings_df_train = pd.read_csv(\n    f\"{INPUT_DATA_PATH}/train.csv\",\n    header=0,\n    names=[\"building_id\", \"meter_id\", \"timestamp\", \"meter_reading\"],\n)\nreadings_df_train = cast_readings_data(readings_df_train)\n\n# Weather\nweather_df_train = pd.read_csv(f\"{INPUT_DATA_PATH}/weather_train.csv\")\nweather_df_train = cast_weather_data(weather_df_train)\n\n# Buildings\nbuildings_df = pd.read_csv(f\"{INPUT_DATA_PATH}/building_metadata.csv\")\nbuildings_df = cast_buildings_data(buildings_df)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "### Filtering & Outlier Removal",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2025-01-30T07:58:23.982880Z",
                    "iopub.execute_input": "2025-01-30T07:58:23.983382Z",
                    "iopub.status.idle": "2025-01-30T07:58:23.989539Z",
                    "shell.execute_reply.started": "2025-01-30T07:58:23.983342Z",
                    "shell.execute_reply": "2025-01-30T07:58:23.988093Z"
                }
            }
        },
        {
            "cell_type": "code",
            "source": "def _find_constant_streaks(\n    df: pd.DataFrame,\n    streak_length: int = 25,\n    target_column: str = \"meter_reading\"\n):\n    # Compute diffs on target col\n    df = df.copy()\n    df = df.sort_values(\"timestamp\")\n    df[\"target_col_diff\"] = df[target_column].diff()\n    \n    # First find any periods of constant meter readings\n    streaks = []\n    current_streak_start = 0\n    for idx, row in df.iterrows():\n        if pd.isna(row[\"target_col_diff\"]):\n            continue\n        \n        elif row[\"target_col_diff\"] == 0:\n            # Start a new streak if not already a running streak\n            current_streak_start = current_streak_start or idx - 1\n        \n        else:\n            # Streak finished\n            # Save if there is currently a running streak\n            if current_streak_start is not None:\n                streaks.append((current_streak_start, idx - 1))\n    \n            # Reset\n            current_streak_start = None\n            \n\n    # Only keep streaks with length >= streak_length\n    filtered_streaks = []\n    for start, end in streaks:\n        streak_df = df.loc[start: end]\n        assert (streak_df[\"target_col_diff\"].dropna() == 0).all(), print(start, end)\n        if len(streak_df) >= streak_length:\n            start_t = streak_df[\"timestamp\"].min().to_pydatetime()\n            end_t = streak_df[\"timestamp\"].max().to_pydatetime()\n            filtered_streaks.append((start_t, end_t))\n    \n    return filtered_streaks\n\n\ndef find_constant_streaks(\n    readings_df: pd.DataFrame,\n    meter_id: int,\n    building_id: int,\n    streak_length: int = 25,\n    target_column: str = \"meter_reading\"\n) -> list[tuple[datetime | None, datetime | None]]:\n    bm_df = readings_df[\n        (readings_df[\"building_id\"] == building_id)\n        & (readings_df[\"meter_id\"] == meter_id)\n    ]\n    return _find_constant_streaks(bm_df, streak_length, target_column)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "def construct_keep_filter(\n    timestamps: pd.Series,\n    to_keep: list[tuple[datetime | None, datetime | None]],\n):\n    filter_ = pd.Series(\n        data=np.full(shape=(len(timestamps, )), fill_value=False),\n        index=timestamps.index,\n    )\n    for start, end in to_keep:\n    \n        match (start, end):\n            case None, None:\n                pass\n            case (datetime(), None):\n                period_filter = timestamps >= start\n            case (None, datetime()):\n                period_filter = timestamps <= end\n            case (datetime(), datetime()):\n                period_filter = (timestamps >= start) & (timestamps <= end)\n        \n        filter_ |= period_filter\n    \n    return filter_\n\n\ndef keep_filter(\n    data: pd.DataFrame,\n    to_keep: list[tuple[datetime | None, datetime | None]]\n):\n    \"\"\"\n    Filter data to only include observations included in to_keep filters.\n    Filters are tuples of datetimes specifying start and end timestamps\n    (inclusive) of periods to be included.\n    \"\"\"\n    keep_filter = construct_keep_filter(data[\"timestamp\"], to_keep)\n    return data.loc[keep_filter]\n\n\ndef remove_filter(\n    data: pd.DataFrame,\n    to_remove: list[tuple[datetime | None, datetime | None]]\n):\n    \"\"\"\n    Filter data by removing all observations included in the to_remove filter.\n    Only observations outside of the filter will be retained in the final data.\n    Filters are tuples of datetimes specifying start and end timestamps\n    (inclusive) of periods to be removed.\n    \"\"\"\n    remove_filter = construct_keep_filter(data[\"timestamp\"], to_remove)\n    return data.loc[~remove_filter]",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "#### Electricity data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def filter_electricity_data(\n    readings_df: pd.DataFrame,\n    to_keep_filters: dict[int, list[tuple[datetime | None, datetime | None]]],\n    to_remove_filters: dict[int, list[tuple[datetime | None, datetime | None]]],\n):\n    meter_id = 0\n    \n    to_drop_index_values = np.array([])\n    filtered_bm_dfs = []\n\n    # To keep filters\n    for b_id, b_filter in tqdm(to_keep_filters.items()):\n        bm_df = readings_df[\n            (readings_df[\"building_id\"] == b_id)\n            & (readings_df[\"meter_id\"] == meter_id)\n        ]\n        to_drop_index_values = np.concatenate([to_drop_index_values, bm_df.index])\n        \n        bm_df_filtered = keep_filter(bm_df, b_filter)\n        filtered_bm_dfs.append(bm_df_filtered)\n\n    # To remove filters\n    for b_id, b_filter in tqdm(to_remove_filters.items()):\n        bm_df = readings_df[\n            (readings_df[\"building_id\"] == b_id)\n            & (readings_df[\"meter_id\"] == meter_id)\n        ]\n        to_drop_index_values = np.concatenate([to_drop_index_values, bm_df.index])\n        \n        bm_df_filtered = remove_filter(bm_df, b_filter)\n        filtered_bm_dfs.append(bm_df_filtered)\n    \n    readings_df = readings_df.drop(index=to_drop_index_values)\n    filtered_bm_df = pd.concat(filtered_bm_dfs, axis=0)\n    readings_df = pd.concat([readings_df, filtered_bm_df], axis=0)\n    \n    return readings_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Specify keep filters for electricity \nto_keep_electricity = {\n    \n    # Keep everything after May 21\n    i: [(datetime(2016, 5, 21), None)]\n    for i in (\n        list(range(29)) \n        + list(range(30, 45)) \n        + list(range(47, 53)) \n        + list(range(54, 105))\n    )\n} | {\n    \n    # Some building specific filters\n    29: [(datetime(2016, 8, 10), None)],\n    45: [(datetime(2016, 7, 1), None)],\n    53: [(datetime(2016, 12, 15), None)],  # Only keeping the final timestamp\n    106: [(datetime(2016, 11, 1), None)],\n    180: [(None, datetime(2016, 2, 17, 10))],\n    218: [(None, datetime(2016, 2, 17, 10))],\n    604: [(datetime(2016, 12, 1), None)],\n    740: [(datetime(2016, 12, 31), None)], # Only keeping the final timestamp\n    803: [(None, datetime(2016, 9, 24))],\n    857: [(None, datetime(2016, 4, 13))],\n    1113: [(datetime(2016, 7, 27, 10), None)],\n    1153: [(datetime(2016, 1, 20, 14), None)],\n    1264: [(None, datetime(2016, 8, 23))],\n    1345: [(None, datetime(2016, 2, 11))],\n    46: [(None, datetime(2016, 3, 1)), (datetime(2016, 5, 21), None)],\n}",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Specify remove filters for electricity\nto_remove_electricity = {\n    b_id: find_constant_streaks(readings_df_train, meter_id=0, building_id=b_id)\n    for b_id in tqdm(\n        [105]\n        + list(range(107, 128))\n        + list(range(136, 156))\n        + [177]\n        + list(range(245, 255))\n        + [269, 278, 376, 537, 545, 577, 681, 693, 723, 733, 738, 799, 802, 874]\n        + list(range(875, 885))\n        + [886, 897]\n        + list(range(905, 946))\n        + list(range(954, 997))\n        + [1066, 1079, 1096, 1098, 1128, 1154, 1157, 1160, 1169, 1177, 1185, 1202, 1221, 1225, 1226]\n        + list(range(1228, 1281))\n        + list(range(1282, 1314))\n        + list(range(1315, 1325))\n        + [1359]\n    )\n}",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Apply filters\nreadings_df_train = filter_electricity_data(\n    readings_df_train,\n    to_keep_filters=to_keep_electricity,\n    to_remove_filters=to_remove_electricity,\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Test that filters are working\n\n# meter_id = 0\n\n# # building_id = np.random.choice(a=list(to_keep_electricity.keys()))\n# # building_id_filter = to_keep_electricity[building_id]\n\n# building_id = np.random.choice(a=list(to_remove_electricity.keys()))\n# building_id_filter = to_remove_electricity[building_id]\n\n# print(building_id, building_id_filter)\n\n# bm_df_before = readings_df_train_before[\n#     (readings_df_train_before[\"building_id\"] == building_id)\n#     & (readings_df_train_before[\"meter_id\"] == meter_id)\n# ]\n# bm_df_after = readings_df_train_after[\n#     (readings_df_train_after[\"building_id\"] == building_id)\n#     & (readings_df_train_after[\"meter_id\"] == meter_id)\n# ]\n\n# fig, ax = plt.subplots(2, 1, figsize=(15, 4), sharex=True, sharey=True)\n# ax[0].plot(\n#     bm_df_before[\"timestamp\"].values,\n#     bm_df_before[\"meter_reading\"].values,\n#     label=f\"Before filter\"\n# )\n# ax[0].legend()\n# ax[1].plot(\n#     bm_df_after[\"timestamp\"].values,\n#     bm_df_after[\"meter_reading\"].values,\n#     label=f\"After filter\"\n# )\n# ax[1].legend();\n\n# for start, end in building_id_filter:\n#     if start is not None: ax[1].axvline(start, color=\"red\", lw=2)\n#     if end is not None: ax[1].axvline(end, color=\"red\", lw=2)\n    \n# fig.tight_layout();",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "### Weather data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Reindex weather data such that every site has a measurement\n# for each training timestamp\n\n\ndef reindex_weather_data(\n    weather_df: pd.DataFrame,\n    start_timestamp: pd.Timestamp,\n    end_timestamp: pd.Timestamp,\n    freq: str = \"1h\"\n) -> pd.DataFrame:\n    weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n    timestamps = pd.date_range(\n        start_timestamp,\n        end_timestamp,\n        freq=freq,\n        inclusive=\"both\"\n    )\n    timestamps = pd.DatetimeIndex(timestamps, name=\"timestamp\")\n    site_dfs = []\n    for site_id, site_df in weather_df.groupby(\"site_id\", observed=True):\n        site_df = site_df.set_index(\"timestamp\").reindex(timestamps).reset_index()\n        site_df[\"site_id\"] = site_df[\"site_id\"].fillna(value=site_id)\n        site_dfs.append(site_df)\n\n    weather_df = pd.concat(site_dfs, ignore_index=True)\n    return weather_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "weather_df_train = reindex_weather_data(\n    weather_df=weather_df_train,\n    start_timestamp=MIN_TRAIN_TIMESTAMP,\n    end_timestamp=MAX_TRAIN_TIMESTAMP,\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Functionality for imputing missing weather data\n\ndef interpolate(weather_df: pd.DataFrame, column: str) -> pd.DataFrame:\n    weather_df[column] = weather_df[column].interpolate(\"linear\", limit=12)\n    weather_df[column] = weather_df[column].ffill(limit=2)\n    weather_df[column] = weather_df[column].bfill(limit=2)\n    return weather_df\n\n\ndef _mean_weather_by_date_and_site(weather_df, column) -> pd.DataFrame:\n    mean_values = (\n        weather_df\n        .groupby([\"date\", \"site_id\"])[[column]]\n        .mean()\n        .reset_index()\n    )\n    return mean_values\n\n\ndef _merge_onto_weather_df(weather_df, right, right_suffix) -> pd.DataFrame:\n    weather_df = weather_df.merge(\n        right=right,\n        how=\"left\",\n        on=[\"date\", \"site_id\"],\n        suffixes=(\"\", right_suffix)\n    )\n    return weather_df\n\n\ndef impute_with_same_day_mean(weather_df: pd.DataFrame, column: str) -> pd.DataFrame:\n    # Compute same day mean values and merge onto weather df\n    mean_values = _mean_weather_by_date_and_site(weather_df, column)\n    weather_df = _merge_onto_weather_df(weather_df, mean_values, \"_mean\")\n    \n    # Fill with means from same day\n    weather_df[column] = weather_df[column].fillna(weather_df[f\"{column}_mean\"])\n    weather_df = weather_df.drop(f\"{column}_mean\", axis=1)\n    return weather_df\n\n\ndef ffill_mean_by_date(weather_df: pd.DataFrame, column: str) -> pd.DataFrame:\n    # Compute same day mean values and merge onto weather df\n    mean_values = _mean_weather_by_date_and_site(weather_df, column)\n    \n    # ffill by site and date\n    site_dfs = []\n    for site_id, site_df in mean_values.groupby(\"site_id\"):\n        site_df = site_df.sort_values(\"date\")\n        site_df[column] = site_df[column].ffill().bfill()\n        site_dfs.append(site_df)\n    mean_values = pd.concat(site_dfs, ignore_index=True)\n    \n    # Merge back onto main and fill with mean values\n    weather_df = _merge_onto_weather_df(weather_df, mean_values, \"_mean\")\n    weather_df[column] = weather_df[column].fillna(weather_df[f\"{column}_mean\"])\n    weather_df = weather_df.drop(f\"{column}_mean\", axis=1)\n    return weather_df\n\n\ndef fill_missing_weather_data(weather_df: pd.DataFrame) -> pd.DataFrame:\n    weather_df[\"date\"] = weather_df[\"timestamp\"].dt.date\n\n    for column in WEATHER_FEATURE_COLUMNS:\n        weather_df = interpolate(weather_df, column)\n        weather_df = impute_with_same_day_mean(weather_df, column)\n        weather_df = ffill_mean_by_date(weather_df, column)\n\n    weather_df = weather_df.drop(columns=[\"date\"])\n    \n    return weather_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Weather data feature engineering\n# Lagged / rolling features. Compute these before merging to make merges\n# less memory intensive.\n\ndef add_smoothed_weather_feature(df: pd.DataFrame, feature: str) -> pd.DataFrame:\n    site_dfs = []\n    for site_id, site_df in df.groupby(\"site_id\", observed=True):\n        site_df = site_df[[\"site_id\", \"timestamp\", feature]]\n        site_df = site_df.sort_values(\"timestamp\").drop_duplicates(keep=\"first\")\n        site_df[f\"{feature}_smoothed\"] = savgol_filter(\n            np.array(site_df[feature]),\n            window_length=12,\n            polyorder=2,\n        )\n        site_df[f\"{feature}_smoothed\"] = site_df[f\"{feature}_smoothed\"].astype(np.float32)\n        site_dfs.append(site_df)\n    site_dfs = pd.concat(site_dfs, ignore_index=True).drop(columns=[feature])\n    df = df.merge(right=site_dfs, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    return df\n\n\ndef add_lagged_weather_feature(df: pd.DataFrame, feature: str, lag: int) -> pd.DataFrame:\n    site_dfs = []\n    for site_id, site_df in df.groupby(\"site_id\", observed=True):\n        site_df = site_df[[\"site_id\", \"timestamp\", feature]]\n        site_df = site_df.sort_values(\"timestamp\").drop_duplicates(keep=\"first\")\n        lag_series = site_df[feature].shift(lag).astype(np.float32)\n        site_df[f\"{feature}_lag_{lag}\"] = lag_series\n        site_dfs.append(site_df)\n    site_dfs = pd.concat(site_dfs, ignore_index=True).drop(columns=[feature])\n    df = df.merge(right=site_dfs, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    return df\n    \n\ndef add_rolling_mean_weather_feature(df: pd.DataFrame, feature: str, window: int) -> pd.DataFrame:\n    site_dfs = []\n    for site_id, site_df in df.groupby(\"site_id\", observed=True):\n        site_df = site_df[[\"site_id\", \"timestamp\", feature]]\n        site_df = site_df.sort_values(\"timestamp\").drop_duplicates(keep=\"first\")\n        rolling_series = site_df[feature].rolling(window).mean().astype(np.float32)\n        site_df[f\"{feature}_rolling_{window}\"] = rolling_series\n        site_dfs.append(site_df)\n    site_dfs = pd.concat(site_dfs, ignore_index=True).drop(columns=[feature])\n    df = df.merge(right=site_dfs, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    return df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "WEATHER_LAGS = [1, 2]\n\n# Air temperature\nfeature_name = \"air_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_train = add_lagged_weather_feature(weather_df_train, feature_name, lag)\n\n# Dew temperature\nfeature_name = \"dew_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_train = add_lagged_weather_feature(weather_df_train, feature_name, lag)\n\n# Sea level pressure\nfeature_name = \"sea_level_pressure\"\nfor lag in WEATHER_LAGS:\n    weather_df_train = add_lagged_weather_feature(weather_df_train, feature_name, lag)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Merge",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def merge_dfs(readings_df: pd.DataFrame, buildings_df: pd.DataFrame, weather_df: pd.DataFrame) -> pd.DataFrame:\n\n    # Merge\n    merged_df = pd.merge(left=readings_df, right=buildings_df, how=\"left\", on=\"building_id\")\n    merged_df = pd.merge(left=merged_df, right=weather_df, how=\"left\", on=[\"site_id\", \"timestamp\"])\n\n    return merged_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "train_df = merge_dfs(readings_df_train, buildings_df, weather_df_train)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Feature engineering",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2025-01-08T07:28:05.619410Z",
                    "iopub.execute_input": "2025-01-08T07:28:05.620589Z",
                    "iopub.status.idle": "2025-01-08T07:28:07.073796Z",
                    "shell.execute_reply.started": "2025-01-08T07:28:05.620539Z",
                    "shell.execute_reply": "2025-01-08T07:28:07.072645Z"
                }
            }
        },
        {
            "cell_type": "code",
            "source": "def kbtu_to_kwh(df: pd.DataFrame) -> pd.DataFrame:\n    mask = (df[\"building_id\"] == 0) & (df[\"meter_id\"] == 0)\n    df.loc[mask, \"meter_reading\"] = df.loc[mask, \"meter_reading\"] * 0.2931\n    return df\n\n\ndef add_periodic_features(df: pd.DataFrame, feature: str, period: int) -> pd.DataFrame:\n    df[f\"{feature}_sin\"] = np.sin(2 * np.pi * df[feature] / period)\n    df[f\"{feature}_sin\"] = df[f\"{feature}_sin\"].astype(np.float32)\n    \n    df[f\"{feature}_cos\"] = np.cos(2 * np.pi * df[feature] / period).astype(np.float32)\n    df[f\"{feature}_cos\"] = df[f\"{feature}_cos\"].astype(np.float32)\n    \n    return df\n\n\ndef add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype(np.uint8)\n    df = add_periodic_features(df, \"hour\", 24)\n\n    df[\"day_of_week\"] = df[\"timestamp\"].dt.weekday.astype(np.uint8)\n    df = add_periodic_features(df, \"day_of_week\", 7)\n\n    df[\"month\"] = df[\"timestamp\"].dt.month.astype(np.uint8)\n    df = add_periodic_features(df, \"month\", 12)\n\n    is_weekend = (df[\"timestamp\"].dt.weekday >= 5)\n    df[\"is_weekend\"] = is_weekend.astype(np.uint8)\n    \n    return df\n\n\ndef add_building_age_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"building_age_years\"] = df[\"timestamp\"].dt.year - df[\"year_built\"]\n    return df\n\n\ndef add_building_area_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"building_area_square_feet\"] = df[\"square_feet\"] * df[\"floor_count\"]\n    return df\n\n\ndef saturation_vapour_pressure(temperature: pd.Series) -> pd.Series:\n    return 6.1094 * np.exp(17.625 * temperature / (temperature + 243.04))\n\n\ndef add_relative_humidity_feature(df: pd.DataFrame) -> pd.DataFrame:\n    svp_air_temp = saturation_vapour_pressure(df[\"air_temperature\"])\n    svp_dew_temp = saturation_vapour_pressure(df[\"dew_temperature\"])\n    rh = 100 * svp_dew_temp / svp_air_temp\n    df[\"relative_humidity\"] = rh.astype(np.float32)\n    return df\n\n\ndef add_cold_chill_feature(df: pd.DataFrame) -> pd.DataFrame:\n    # Cold chill only defined for temps below 10C and wind speeds above 1.3 m/s\n    mask = (df[\"air_temperature\"] <= 10.0) & (df[\"wind_speed\"] >= 1.3)\n    air_temp = df.loc[mask, \"air_temperature\"]\n    wind_speed = df.loc[mask, \"wind_speed\"]\n    cold_chill = (\n        13.12 \n        +  0.6215 * air_temp\n        - 11.37 * (3.6 * wind_speed) ** 0.16\n        + 0.3965 * air_temp * (3.6 * wind_speed) ** 0.16\n    )\n    df.loc[mask, \"cold_chill\"] = cold_chill.astype(np.float32)\n    return df\n\n\ndef add_apparent_temperature_feature(df: pd.DataFrame) -> pd.DataFrame:\n    mask = df[\"air_temperature\"].between(10, 27, inclusive=\"both\")\n    air_temp = df.loc[mask, \"air_temperature\"]\n    wind_speed = df.loc[mask, \"wind_speed\"]\n    humidity = df.loc[mask, \"relative_humidity\"] / 100\n    pressure = humidity * 6.105 * np.exp((17.27 * air_temp) / (air_temp + 237.7))\n    apparent_temp = air_temp + 0.33 * pressure - 0.7 * wind_speed - 4\n    df.loc[mask, \"apparent_temperature\"] = apparent_temp.astype(np.float32)\n    return df\n\n\ndef add_heat_index_feature(df: pd.DataFrame) -> pd.DataFrame:\n    mask = df[\"air_temperature\"] >= 27\n    air_temp = df.loc[mask, \"air_temperature\"]\n    humidity = df.loc[mask, \"relative_humidity\"]\n    heat_index = (\n        - 8.7847 \n        + 1.6114 * air_temp \n        + 2.3385 * humidity\n        - 0.1461 * air_temp * humidity\n        - 0.0123 * air_temp ** 2 \n        - 0.0164 * humidity ** 2\n        + 2.212e-03 * air_temp ** 2 * humidity\n        + 7.255e-04 * air_temp * humidity ** 2\n        - 3.582e-06 * air_temp ** 2 * humidity ** 2\n    )\n    df.loc[mask, \"heat_index\"] = heat_index.astype(np.float32)\n    return df\n\n\ndef cooling_degree_days(df: pd.DataFrame) -> pd.DataFrame:\n    # https://www.investopedia.com/terms/c/colddegreeday.asp\n    ...\n\n\ndef heating_degree_days():\n    ...",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Timestamp features\ntrain_df = add_temporal_features(train_df)\n\n# Meter reading features\ntrain_df = kbtu_to_kwh(train_df)\n\n# Building features\ntrain_df = add_building_age_feature(train_df)\ntrain_df = add_building_area_feature(train_df)\n\n# Weather features\ntrain_df = add_relative_humidity_feature(train_df)\ntrain_df = add_cold_chill_feature(train_df)\ntrain_df = add_apparent_temperature_feature(train_df)\ntrain_df = add_heat_index_feature(train_df)\ntrain_df = add_periodic_features(train_df, \"wind_direction\", 360)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "train_df.to_parquet(\"train_df.parquet\")",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Test data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Meter readings\nreadings_df_test = pd.read_csv(\n    f\"{INPUT_DATA_PATH}/test.csv\",\n    header=0,\n    names=[\"row_id\", \"building_id\", \"meter_id\", \"timestamp\"],\n)\nreadings_df_test = cast_readings_data(readings_df_test)\nreadings_df_test[\"row_id\"] = readings_df_test[\"row_id\"].astype(np.uint32)\n\n# Weather\nweather_df_test = pd.read_csv(f\"{INPUT_DATA_PATH}/weather_test.csv\")\nweather_df_test = cast_weather_data(weather_df_test)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Reindex weather data\nweather_df_test = reindex_weather_data(\n    weather_df=weather_df_test,\n    start_timestamp=MIN_TEST_TIMESTAMP,\n    end_timestamp=MAX_TEST_TIMESTAMP,\n)\n\n# Concat train and test weather data\ntrain_timestamp_cutoff = MIN_TEST_TIMESTAMP - pd.Timedelta(\"1d\")  # More than enough\ntimestamp_mask = weather_df_train[\"timestamp\"] >= train_timestamp_cutoff\nweather_cols = [\"timestamp\", \"site_id\"] + WEATHER_FEATURE_COLUMNS\nweather_df_test = pd.concat(\n    [\n        weather_df_test,\n        weather_df_train[timestamp_mask][weather_cols]\n        \n    ],\n    axis=0,\n    ignore_index=True\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Compute lagged / rolling weather features\nWEATHER_LAGS = [1, 2]\n\n# Air temperature\nfeature_name = \"air_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_test = add_lagged_weather_feature(weather_df_test, feature_name, lag)\n\n# Dew temperature\nfeature_name = \"dew_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_test = add_lagged_weather_feature(weather_df_test, feature_name, lag)\n\n# Sea level pressure\nfeature_name = \"sea_level_pressure\"\nfor lag in WEATHER_LAGS:\n    weather_df_test = add_lagged_weather_feature(weather_df_test, feature_name, lag)\n\n\nweather_df_test = weather_df_test.sort_values([\"site_id\", \"timestamp\"]).reset_index(drop=True)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Merge all dfs\ntest_df = merge_dfs(readings_df_test, buildings_df, weather_df_test)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "### Feature engineering",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Timestamp features\ntest_df = add_temporal_features(test_df)\n\n# Building features\ntest_df = add_building_age_feature(test_df)\ntest_df = add_building_area_feature(test_df)\n\n# Weather features\ntest_df = add_relative_humidity_feature(test_df)\ntest_df = add_cold_chill_feature(test_df)\ntest_df = add_apparent_temperature_feature(test_df)\ntest_df = add_heat_index_feature(test_df)\ntest_df = add_periodic_features(test_df, \"wind_direction\", 360)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "test_df.to_parquet(\"test_df.parquet\")",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}