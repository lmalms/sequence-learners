{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceId": 9994,
                    "databundleVersionId": 752467,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 30786,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "from datetime import datetime, date\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import savgol_filter\nfrom tqdm import tqdm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Constants\nINPUT_DATA_PATH = \"/kaggle/input/ashrae-energy-prediction\"\n\nMIN_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-01-01 00:00:00\")\nMAX_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-12-31 23:00:00\")\nMIN_TEST_TIMESTAMP = pd.Timestamp(\"2017-01-01 00:00:00\")\nMAX_TEST_TIMESTAMP = pd.Timestamp('2018-12-31 23:00:00')\nDATA_RESOLUTION = \"1h\"\n\nWEATHER_FEATURE_COLUMNS = [\n    'air_temperature',\n    'cloud_coverage',\n    'dew_temperature',\n    'precip_depth_1_hr',\n    'sea_level_pressure',\n    'wind_direction',\n    'wind_speed'\n]\nWEATHER_LAGS = [1, 2, 3, 4, 5]\nWEATHER_ROLLING_WINDOWS = [12, 24]",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "def cast_readings_data(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"building_id\"] = df[\"building_id\"].astype(\"category\")\n    df[\"meter_id\"] = df[\"meter_id\"].astype(\"category\")\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    try:\n        df[\"meter_reading\"] = df[\"meter_reading\"].astype(np.float32)\n    except KeyError:\n        pass\n    return df\n\n\ndef cast_weather_data(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"site_id\"] = df[\"site_id\"].astype(\"category\")\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    for col in WEATHER_FEATURE_COLUMNS:\n        df[col] = df[col].astype(np.float32)\n    return df\n\n\ndef cast_buildings_data(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"site_id\"] = df[\"site_id\"].astype(\"category\")\n    df[\"building_id\"] = df[\"building_id\"].astype(\"category\")\n    df[\"primary_use\"] = df[\"primary_use\"].astype(\"category\")\n    for col in [\"square_feet\", \"year_built\", \"floor_count\"]:\n        df[col] = df[col].astype(np.float32)\n    return df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Train data",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "### Load raw data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Meter readings\nreadings_df_train = pd.read_csv(\n    f\"{INPUT_DATA_PATH}/train.csv\",\n    header=0,\n    names=[\"building_id\", \"meter_id\", \"timestamp\", \"meter_reading\"],\n)\nreadings_df_train = cast_readings_data(readings_df_train)\n\n# Weather\nweather_df_train = pd.read_csv(f\"{INPUT_DATA_PATH}/weather_train.csv\")\nweather_df_train = cast_weather_data(weather_df_train)\n\n# Buildings\nbuildings_df = pd.read_csv(f\"{INPUT_DATA_PATH}/building_metadata.csv\")\nbuildings_df = cast_buildings_data(buildings_df)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "### Filtering & Outlier Removal",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2025-01-30T07:58:23.982880Z",
                    "iopub.execute_input": "2025-01-30T07:58:23.983382Z",
                    "iopub.status.idle": "2025-01-30T07:58:23.989539Z",
                    "shell.execute_reply.started": "2025-01-30T07:58:23.983342Z",
                    "shell.execute_reply": "2025-01-30T07:58:23.988093Z"
                }
            }
        },
        {
            "cell_type": "code",
            "source": "def _find_constant_streaks(\n    df: pd.DataFrame,\n    streak_length: int = 25,\n    target_column: str = \"meter_reading\"\n):\n    # Compute diffs on target col\n    df = df.copy()\n    df = df.sort_values(\"timestamp\")\n    df[\"target_col_diff\"] = df[target_column].diff()\n    \n    # First find any periods of constant meter readings\n    streaks = []\n    current_streak_start = 0\n    for idx, row in df.iterrows():\n        if pd.isna(row[\"target_col_diff\"]):\n            continue\n        \n        elif row[\"target_col_diff\"] == 0:\n            # Start a new streak if not already a running streak\n            current_streak_start = current_streak_start or idx - 1\n        \n        else:\n            # Streak finished\n            # Save if there is currently a running streak\n            if current_streak_start is not None:\n                streaks.append((current_streak_start, idx - 1))\n    \n            # Reset\n            current_streak_start = None\n            \n\n    # Only keep streaks with length >= streak_length\n    filtered_streaks = []\n    for start, end in streaks:\n        streak_df = df.loc[start: end]\n        assert (streak_df[\"target_col_diff\"].dropna() == 0).all(), print(start, end)\n        if len(streak_df) >= streak_length:\n            start_t = streak_df[\"timestamp\"].min().to_pydatetime()\n            end_t = streak_df[\"timestamp\"].max().to_pydatetime()\n            filtered_streaks.append((start_t, end_t))\n    \n    return filtered_streaks\n\n\ndef find_constant_streaks(\n    readings_df: pd.DataFrame,\n    meter_id: int,\n    building_id: int,\n    streak_length: int = 25,\n    target_column: str = \"meter_reading\"\n) -> list[tuple[datetime | None, datetime | None]]:\n    bm_df = readings_df[\n        (readings_df[\"building_id\"] == building_id)\n        & (readings_df[\"meter_id\"] == meter_id)\n    ]\n    return _find_constant_streaks(bm_df, streak_length, target_column)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "def construct_keep_filter(\n    timestamps: pd.Series,\n    to_keep: list[tuple[datetime | None, datetime | None]],\n):\n    filter_ = pd.Series(\n        data=np.full(shape=(len(timestamps, )), fill_value=False),\n        index=timestamps.index,\n    )\n    for start, end in to_keep:\n    \n        match (start, end):\n            case None, None:\n                pass\n            case (datetime(), None):\n                period_filter = timestamps >= start\n            case (None, datetime()):\n                period_filter = timestamps <= end\n            case (datetime(), datetime()):\n                period_filter = (timestamps >= start) & (timestamps <= end)\n            case _:\n                print(start, end)\n                print(\"Unrecognised filter pattern. Skipping ...\")\n                continue\n        \n        filter_ |= period_filter\n    \n    return filter_\n\n\ndef keep_filter(\n    data: pd.DataFrame,\n    to_keep: list[tuple[datetime | None, datetime | None]]\n):\n    \"\"\"\n    Filter data to only include observations included in to_keep filters.\n    Filters are tuples of datetimes specifying start and end timestamps\n    (inclusive) of periods to be included.\n    \"\"\"\n    keep_filter = construct_keep_filter(data[\"timestamp\"], to_keep)\n    return data.loc[keep_filter]\n\n\ndef remove_filter(\n    data: pd.DataFrame,\n    to_remove: list[tuple[datetime | None, datetime | None]]\n):\n    \"\"\"\n    Filter data by removing all observations included in the to_remove filter.\n    Only observations outside of the filter will be retained in the final data.\n    Filters are tuples of datetimes specifying start and end timestamps\n    (inclusive) of periods to be removed.\n    \"\"\"\n    remove_filter = construct_keep_filter(data[\"timestamp\"], to_remove)\n    return data.loc[~remove_filter]",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "def filter_readings_data(\n    readings_df: pd.DataFrame,\n    meter_id: int,\n    to_keep_filters: dict[int, list[tuple[datetime | None, datetime | None]]] | None = None,\n    to_remove_filters: dict[int, list[tuple[datetime | None, datetime | None]]] | None = None,\n):\n    to_keep_filters = to_keep_filters or {}\n    to_remove_filters = to_remove_filters or {}\n    \n    to_drop_index_values = np.array([])\n    filtered_bm_dfs = []\n\n    # To keep filters\n    for b_id, b_filter in tqdm(to_keep_filters.items()):\n        bm_df = readings_df[\n            (readings_df[\"building_id\"] == b_id)\n            & (readings_df[\"meter_id\"] == meter_id)\n        ]\n        to_drop_index_values = np.concatenate([to_drop_index_values, bm_df.index])\n        \n        bm_df_filtered = keep_filter(bm_df, b_filter)\n        filtered_bm_dfs.append(bm_df_filtered)\n\n    # To remove filters\n    for b_id, b_filter in tqdm(to_remove_filters.items()):\n        bm_df = readings_df[\n            (readings_df[\"building_id\"] == b_id)\n            & (readings_df[\"meter_id\"] == meter_id)\n        ]\n        to_drop_index_values = np.concatenate([to_drop_index_values, bm_df.index])\n        \n        bm_df_filtered = remove_filter(bm_df, b_filter)\n        filtered_bm_dfs.append(bm_df_filtered)\n    \n    readings_df = readings_df.drop(index=to_drop_index_values)\n    filtered_bm_df = pd.concat(filtered_bm_dfs, axis=0)\n    readings_df = pd.concat([readings_df, filtered_bm_df], axis=0)\n    \n    return readings_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "#### Electricity data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "to_keep_electricity = {\n    \n    # Keep everything after May 21\n    i: [(datetime(2016, 5, 21), None)]\n    for i in (\n        list(range(29)) \n        + list(range(30, 45)) \n        + list(range(47, 53)) \n        + list(range(54, 105))\n    )\n} | {\n    # Some building specific filters\n    29: [(datetime(2016, 8, 10), None)],\n    45: [(datetime(2016, 7, 1), None)],\n    53: [(datetime(2016, 12, 15), None)],\n    106: [(datetime(2016, 11, 1), None)],\n    180: [(None, datetime(2016, 2, 17, 10))],\n    218: [(None, datetime(2016, 2, 17, 10))],\n    604: [(datetime(2016, 12, 1), None)],\n    740: [(datetime(2016, 12, 31), None)],\n    803: [(None, datetime(2016, 9, 24))],\n    857: [(None, datetime(2016, 4, 13))],\n    1113: [(datetime(2016, 7, 27, 10), None)],\n    1153: [(datetime(2016, 1, 20, 14), None)],\n    1264: [(None, datetime(2016, 8, 23))],\n    1345: [(None, datetime(2016, 2, 11))],\n    46: [(None, datetime(2016, 3, 1)), (datetime(2016, 5, 21), None)],\n}\n\nto_remove_electricity = {\n    b_id: find_constant_streaks(readings_df_train, meter_id=0, building_id=b_id)\n    for b_id in tqdm(\n        [105]\n        + list(range(107, 128))\n        + list(range(136, 156))\n        + [177]\n        + list(range(245, 255))\n        + [269, 278, 376, 537, 545, 577, 681, 693, 723, 733, 738, 799, 802, 874 ]\n        + list(range(875, 885))\n        + [886, 897]\n        + list(range(905, 946))\n        + list(range(954, 997))\n        + [1066, 1079, 1096, 1098, 1128, 1154, 1157, 1160, 1169, 1177, 1185, 1202 ]\n        + [1221, 1225, 1226]\n        + list(range(1228, 1281))\n        + list(range(1282, 1314))\n        + list(range(1315, 1325))\n        + [1359]\n    )\n}",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Apply filters\nreadings_df_train = filter_readings_data(\n    readings_df_train,\n    meter_id=0,\n    to_keep_filters=to_keep_electricity,\n    to_remove_filters=to_remove_electricity,\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "#### Chilled Water",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "to_keep_chilled_water = {\n    43: [(None, datetime(2016, 4, 4, 19)), (datetime(2016, 6, 6, 11), None)],\n    60: [(datetime(2016, 4, 29, 10, 0), None)],\n    162: [(None, datetime(2016, 9, 13, 14)), (datetime(2016, 10, 10, 8), None)],\n    192: [(None, datetime(2016, 5, 9, 13))],\n    195: [(None, datetime(2016, 3, 17, 12)), (datetime(2016, 3, 22), None)],\n    236: [(None, datetime(2016, 1, 24, 2)), (datetime(2016, 3, 21, 12), None)],\n    258: [(None, datetime(2016, 8, 29, 10)), (datetime(2016, 9, 8, 13), datetime(2016, 9, 19, 6)), (datetime(2016, 12, 12, 9,), None)],\n    264: [(datetime(2016, 2, 8, 10), None)],\n    290: [(None, datetime(2016, 8, 19, 1)), (datetime(2016, 9, 9, 7), datetime(2016, 9, 14, 17)), (datetime(2016, 9, 23, 1), datetime(2016, 10, 8, 14)), (datetime(2016, 10, 14, 9), None)],\n    765: [(datetime(2016, 4, 22, 12), None)],\n    778: [(datetime(2016, 9, 8, 9), datetime(2016, 10, 20))],\n    780: [(None, datetime(2016, 8, 2))],\n    \n    # Filters for same / similar October pattern\n    770: [(None, datetime(2016, 10, 4, 10, 0)), (datetime(2016, 10, 10, 11, 0), None)],\n    777: [(None, datetime(2016, 10, 4, 9)), (datetime(2016, 10, 10, 8), None)],\n    787: [(None, datetime(2016, 10, 4, 9)), (datetime(2016, 10, 10, 8), None)],\n\n    # Filters for same July and October pattern\n    880: [(None, datetime(2016, 3, 18, 23)), (datetime(2016, 5, 18, 10), datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 6), None)],\n    954: [(datetime(2016, 8, 8, 11), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 8), None)],\n    990: [(None, datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 8), None)],\n    \n    1167: [(None, datetime(2016, 5, 18, 16)), (datetime(2016, 6, 25, 7), None)],\n    1225: [(None, datetime(2016, 8, 23, 12)), (datetime(2016, 10, 11, 13), None)],\n    1226: [(None, datetime(2016, 8, 23, 12)), (datetime(2016, 10, 20, 12), None)],\n    1232: [(None, datetime(2016, 6, 23, 18)), (datetime(2016, 8, 31, 19), None)],\n    1244: [(None, datetime(2016, 7, 13, 16)), (datetime(2016, 8, 31, 19), None)],\n    1246: [(datetime(2016, 3, 2, 19), None)],\n    1272: [(None, datetime(2016, 9, 28, 9)), (datetime(2016, 10, 20, 12), None)],\n    1273: [(None, datetime(2016, 5, 31, 16)), (datetime(2016, 6, 16, 23), None)],\n} | {\n    # Filters for same July and October pattern\n    building_id: [(None, datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 6), None)]\n    for building_id in (\n        [910, 920, 923, 926, 927, 929, 931, 934, 955, 957]\n        + [961, 963, 965, 967, 969, 973, 976, 989]\n        + [993, 994, 996]\n    )\n}\n\nto_remove_chilled_water = {\n    b_id: find_constant_streaks(readings_df_train, meter_id=1, building_id=b_id, streak_length=35)\n    for b_id in tqdm(\n        [7, 75, 97, 98, 163, 167, 171, 172, 177, 188, 190, 191, 195, 200]\n        + [207, 231, 233, 235, 260, 265, 267, 748, 750, 752, 755, 763, 776 ]\n        + [786, 789, 790, 792, 801]      \n\n        # Filters for same July and October pattern\n        + [874, 890, 893, 894, 895, 896, 898, 899, 911, 915, 916, 917, 918 ]\n        + [929, 932, 933, 935, 942, 951, 952, 953, 957, 958, 959, 960, 961 ]\n        + [962, 964, 965, 966, 968, 971, 972, 974, 975, 978, 979, 980, 981 ]\n        + [983, 987, 991, 992, 994, 995, 997 ]\n\n        # Filters for same mid July pattern. All ids contain that period in addition to\n        # other constant streak periods\n        + [1223, 1225, 1226, 1227, 1229, 1230, 1233, 1234, 1235, 1236, 1238 ]\n        + [1239, 1240, 1241, 1242, 1243, 1246, 1247, 1248, 1249, 1250, 1251 ]\n        + [1252, 1253, 1255, 1258, 1259, 1260, 1262, 1263, 1264, 1266, 1267 ]\n        + [1280, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294 ]\n        + [1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1306, 1307 ]\n        + [1308, 1309, 1310, 1311, 1312]\n    )\n}",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Apply filters\nreadings_df_train = filter_readings_data(\n    readings_df_train,\n    meter_id=1,\n    to_keep_filters=to_keep_chilled_water,\n    to_remove_filters=to_remove_chilled_water,\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "#### Steam",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "to_keep_steam = {\n    751: [(datetime(2016, 2, 3, 7), None)],\n    758: [(None, datetime(2016, 3, 7, 14))],\n    759: [(None, datetime(2016, 1, 14, 23)), (datetime(2016, 2, 2, 8), None)],\n    762: [(None, datetime(2016, 2, 25))],\n    766: [(datetime(2016, 2, 3, 7), None)],\n    772: [(datetime(2016, 2, 25, 7), datetime(2016, 3, 16, 7)), (datetime(2016, 12, 16, 23), None)],\n    783: [(datetime(2016, 12, 9, 14), None)]\n} | {\n    # Same July / October pattern\n    b_id: [(None, datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 6), None)]\n    for b_id in (\n        [880, 889, 890, 894, 901, 905, 906, 910, 911, 913, 914 ]\n        + [917, 921, 924, 928, 933, 951, 953, 955, 964, 968, 971 ]\n        + [973, 976, 979, 981, 987, 992, 995, 997]\n    )\n} | {\n    # Same July / October pattern but slightly different dtes\n    b_id : [(None, datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 12), None)]\n    for b_id in (\n        [885, 886, 887, 888, 895, 896, 893, 898, 899, 900]\n        + [903, 907, 908, 912, 915, 916, 918, 920, 922, 926 ]\n        + [929, 931, 932, 934, 946, 948, 949, 952, 956, 957 ]\n        + [958, 959, 960, 961, 963, 965, 966, 967, 969, 972 ] \n        + [974, 978, 980, 989, 991, 996]\n    )\n} | {\n    # Same early June pattern\n    b_id: [(None, datetime(2016, 5, 31, 16)), (datetime(2016, 6, 2, 18), None)]\n    for b_id in [1358, 1385, 1386, 1387, 1425, 1427]\n}| {\n    927: [(None, datetime(2016, 5, 26)), (datetime(2016, 6, 1, 4), datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 6), None)],\n    945: [(None, datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 6), datetime(2016, 12, 2)), (datetime(2016, 12, 5, 12), None)],\n    954: [(datetime(2016, 8, 8, 10), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 12), None)],\n    983: [(None, datetime(2016, 5, 10)), (datetime(2016, 5, 17, 12), datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 12), None)],\n    993: [(None, datetime(2016, 7, 1, 16)), (datetime(2016, 7, 5, 4), datetime(2016, 10, 15)), (datetime(2016, 10, 17, 12), datetime(2016, 12, 28))],   \n    1072: [(datetime(2016, 7, 25, 12), None)],\n    1176: [(datetime(2016, 2, 12, 10), None)],\n    1189: [(datetime(2016, 2, 12, 10), None)],\n    1250: [(datetime(2016, 12, 21, 15), None)],\n    1392: [(None, datetime(2016, 5, 31, 16)), (datetime(2016, 6, 2, 18), datetime(2016, 7, 5, )), (datetime(2016, 7, 9, ), None)],\n    1426: [(None, datetime(2016, 5, 31, 16)), (datetime(2016, 6, 2, 18), datetime(2016, 7, 7, 13)), (datetime(2016, 8, 1, 12), None)],\n} \n\nto_remove_steam = {\n    b_id: find_constant_streaks(readings_df_train, meter_id=2, building_id=b_id, streak_length=35)\n    for b_id in tqdm(\n        [750, 776, 784, 876, 886, 888, 889, 890, 894, 895, 907, 910, 912 ]\n        + [916, 917, 921, 925, 928, 929, 932, 933, 942, 951, 955, 961, 962 ]\n        + [965, 972, 973, 976, 978, 996, 1088, 1098, 1111, 1129, 1140 ]\n        + [1158, 1174, 1176, 1189]\n\n        # All contain same mid July pattern\n        + [1225, 1226, 1238, 1239, 1241, 1243, 1245, 1247, 1248, 1249, 1254 ]\n        + [1256, 1258, 1263, 1283, 1284, 1285, 1286, 1287, 1289, 1290, 1291 ]\n        + [1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1301, 1303, 1305 ]\n        + [1307, 1308, 1309, 1310 ]\n\n        # Same early June pattern\n        + [1329, 1336, 1337, 1338, 1339, 1341, 1342, 1343, 1344, 1345, 1346 ]\n        + [1347, 1350, 1351, 1354, 1355, 1360, 1361, 1363, 1364, 1366, 1367 ]\n        + [1373, 1375, 1377, 1378, 1379, 1381, 1382, 1383, 1384, 1391, 1396 ]\n        + [1405, 1406, 1409, 1414, 1417, 1418, 1420, 1424, 1430, 1431, 1433 ]\n        + [1434, 1437, 1438]\n    )\n}",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Apply filters\nreadings_df_train = filter_readings_data(\n    readings_df_train,\n    meter_id=2,\n    to_keep_filters=to_keep_steam,\n    to_remove_filters=to_remove_steam,\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "#### Hot Water",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "to_keep_hot_water = {\n    163: [(None, datetime(2016, 1, 28, 19))],\n    176: [(None, datetime(2016, 2, 10, 12)), (datetime(2016, 5, 5, 18), None)],\n    195: [(None, datetime(2016, 2, 9, 15)), (datetime(2016, 12, 7, 13), None)],\n    200: [(None, datetime(2016, 3, 19, 17))],\n    220: [(None, datetime(2016, 1, 6, 22)), (datetime(2016, 1, 14), None)],\n    226: [(None, datetime(2016, 9, 30, 9)), (datetime(2016, 10, 25, 6), None)],\n    236: [(None, datetime(2016, 11, 20))],\n    279: [(datetime(2016, 12, 31), None)],\n    287: [(datetime(2016, 12, 31), None)],\n\n}\n\nto_remove_hot_water = {\n    b_id: find_constant_streaks(readings_df_train, meter_id=3, building_id=b_id, streak_length=35)\n    for b_id in tqdm(\n        [113, 117, 119, 121, 138, 175, 192, 203, 284, 1223, 1224, 1227 ]\n        + [1228, 1229, 1230, 1231, 1233, 1234, 1235, 1236, 1240, 1242, 1244 ]\n        + [1246, 1251, 1252, 1253, 1255, 1259, 1262, 1265, 1266, 1267, 1269 ]\n        + [1270, 1271, 1273, 1274, 1294, 1295, 1296, 1297, 1298, 1300, 1301 ]\n        + [1311, 1312, 1317, 1318, 1319, 1321, 1321, 1322, 1323]\n    )\n}",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Apply filters\nreadings_df_train = filter_readings_data(\n    readings_df_train,\n    meter_id=3,\n    to_keep_filters=to_keep_hot_water,\n    to_remove_filters=to_remove_hot_water,\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "### Weather data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Reindex weather data such that every site has a measurement\n# for each training timestamp\n\n\ndef reindex_weather_data(\n    weather_df: pd.DataFrame,\n    start_timestamp: pd.Timestamp,\n    end_timestamp: pd.Timestamp,\n    freq: str = \"1h\"\n) -> pd.DataFrame:\n    weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"])\n    timestamps = pd.date_range(\n        start_timestamp,\n        end_timestamp,\n        freq=freq,\n        inclusive=\"both\"\n    )\n    timestamps = pd.DatetimeIndex(timestamps, name=\"timestamp\")\n    site_dfs = []\n    for site_id, site_df in weather_df.groupby(\"site_id\", observed=True):\n        site_df = site_df.set_index(\"timestamp\").reindex(timestamps).reset_index()\n        site_df[\"site_id\"] = site_df[\"site_id\"].fillna(value=site_id)\n        site_dfs.append(site_df)\n\n    weather_df = pd.concat(site_dfs, ignore_index=True)\n    return weather_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "weather_df_train = reindex_weather_data(\n    weather_df=weather_df_train,\n    start_timestamp=MIN_TRAIN_TIMESTAMP,\n    end_timestamp=MAX_TRAIN_TIMESTAMP,\n)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Functionality for imputing missing weather data\n\ndef interpolate(weather_df: pd.DataFrame, column: str) -> pd.DataFrame:\n    weather_df[column] = weather_df[column].interpolate(\"linear\", limit=12)\n    weather_df[column] = weather_df[column].ffill(limit=2)\n    weather_df[column] = weather_df[column].bfill(limit=2)\n    return weather_df\n\n\ndef _mean_weather_by_date_and_site(weather_df, column) -> pd.DataFrame:\n    mean_values = (\n        weather_df\n        .groupby([\"date\", \"site_id\"], observed=True)[[column]]\n        .mean()\n        .reset_index()\n    )\n    return mean_values\n\n\ndef _merge_onto_weather_df(weather_df, right, right_suffix) -> pd.DataFrame:\n    weather_df = weather_df.merge(\n        right=right,\n        how=\"left\",\n        on=[\"date\", \"site_id\"],\n        suffixes=(\"\", right_suffix)\n    )\n    return weather_df\n\n\ndef impute_with_same_day_mean(weather_df: pd.DataFrame, column: str) -> pd.DataFrame:\n    # Compute same day mean values and merge onto weather df\n    mean_values = _mean_weather_by_date_and_site(weather_df, column)\n    weather_df = _merge_onto_weather_df(weather_df, mean_values, \"_mean\")\n    \n    # Fill with means from same day\n    weather_df[column] = weather_df[column].fillna(weather_df[f\"{column}_mean\"])\n    weather_df = weather_df.drop(f\"{column}_mean\", axis=1)\n    return weather_df\n\n\ndef ffill_mean_by_date(weather_df: pd.DataFrame, column: str) -> pd.DataFrame:\n    # Compute same day mean values and merge onto weather df\n    mean_values = _mean_weather_by_date_and_site(weather_df, column)\n    \n    # ffill by site and date\n    site_dfs = []\n    for site_id, site_df in mean_values.groupby(\"site_id\", observed=True):\n        site_df = site_df.sort_values(\"date\")\n        site_df[column] = site_df[column].ffill().bfill()\n        site_dfs.append(site_df)\n    mean_values = pd.concat(site_dfs, ignore_index=True)\n    \n    # Merge back onto main and fill with mean values\n    weather_df = _merge_onto_weather_df(weather_df, mean_values, \"_mean\")\n    weather_df[column] = weather_df[column].fillna(weather_df[f\"{column}_mean\"])\n    weather_df = weather_df.drop(f\"{column}_mean\", axis=1)\n    return weather_df\n\n\ndef fill_missing_weather_data(weather_df: pd.DataFrame) -> pd.DataFrame:\n    weather_df[\"date\"] = weather_df[\"timestamp\"].dt.date\n\n    for column in WEATHER_FEATURE_COLUMNS:\n        weather_df = interpolate(weather_df, column)\n        weather_df = impute_with_same_day_mean(weather_df, column)\n        weather_df = ffill_mean_by_date(weather_df, column)\n\n    weather_df = weather_df.drop(columns=[\"date\"])\n    \n    return weather_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Weather data feature engineering\n# Lagged / rolling features. Compute these before merging to make merges\n# less memory intensive.\n\ndef add_smoothed_weather_feature(df: pd.DataFrame, feature: str) -> pd.DataFrame:\n    site_dfs = []\n    for site_id, site_df in df.groupby(\"site_id\", observed=True):\n        site_df = site_df[[\"site_id\", \"timestamp\", feature]]\n        site_df = site_df.sort_values(\"timestamp\").drop_duplicates(keep=\"first\")\n        site_df[f\"{feature}_smoothed\"] = savgol_filter(\n            np.array(site_df[feature]),\n            window_length=12,\n            polyorder=2,\n        )\n        site_df[f\"{feature}_smoothed\"] = site_df[f\"{feature}_smoothed\"].astype(np.float32)\n        site_dfs.append(site_df)\n    site_dfs = pd.concat(site_dfs, ignore_index=True).drop(columns=[feature])\n    df = df.merge(right=site_dfs, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    return df\n\n\ndef add_lagged_weather_feature(df: pd.DataFrame, feature: str, lag: int) -> pd.DataFrame:\n    site_dfs = []\n    for site_id, site_df in df.groupby(\"site_id\", observed=True):\n        site_df = site_df[[\"site_id\", \"timestamp\", feature]]\n        site_df = site_df.sort_values(\"timestamp\").drop_duplicates(keep=\"first\")\n        lag_series = site_df[feature].shift(lag).astype(np.float32)\n        site_df[f\"{feature}_lag_{lag}\"] = lag_series\n        site_dfs.append(site_df)\n    site_dfs = pd.concat(site_dfs, ignore_index=True).drop(columns=[feature])\n    df = df.merge(right=site_dfs, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    return df\n    \n\ndef add_rolling_mean_weather_feature(df: pd.DataFrame, feature: str, window: int) -> pd.DataFrame:\n    site_dfs = []\n    for site_id, site_df in df.groupby(\"site_id\", observed=True):\n        site_df = site_df[[\"site_id\", \"timestamp\", feature]]\n        site_df = site_df.sort_values(\"timestamp\").drop_duplicates(keep=\"first\")\n        rolling_series = site_df[feature].rolling(window).mean().astype(np.float32)\n        site_df[f\"{feature}_rolling_mean_{window}\"] = rolling_series\n        site_dfs.append(site_df)\n    site_dfs = pd.concat(site_dfs, ignore_index=True).drop(columns=[feature])\n    df = df.merge(right=site_dfs, on=[\"site_id\", \"timestamp\"], how=\"left\")\n    return df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Fill missing weather data\nweather_df_train = fill_missing_weather_data(weather_df_train)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Air temperature\nfeature_name = \"air_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_train = add_lagged_weather_feature(weather_df_train, feature_name, lag)\n\nfor window in WEATHER_ROLLING_WINDOWS:\n    weather_df_train = add_rolling_mean_weather_feature(weather_df_train, feature_name, window)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Dew temperature\nfeature_name = \"dew_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_train = add_lagged_weather_feature(weather_df_train, feature_name, lag)\n\nfor window in WEATHER_ROLLING_WINDOWS:\n    weather_df_train = add_rolling_mean_weather_feature(weather_df_train, feature_name, window)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Sea level pressure\nfeature_name = \"sea_level_pressure\"\nfor lag in WEATHER_LAGS:\n    weather_df_train = add_lagged_weather_feature(weather_df_train, feature_name, lag)\n\nfor window in WEATHER_ROLLING_WINDOWS:\n    weather_df_train = add_rolling_mean_weather_feature(weather_df_train, feature_name, window)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Merge",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def merge_dfs(readings_df: pd.DataFrame, buildings_df: pd.DataFrame, weather_df: pd.DataFrame) -> pd.DataFrame:\n\n    # Merge\n    merged_df = pd.merge(left=readings_df, right=buildings_df, how=\"left\", on=\"building_id\")\n    merged_df = pd.merge(left=merged_df, right=weather_df, how=\"left\", on=[\"site_id\", \"timestamp\"])\n\n    return merged_df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "train_df = merge_dfs(readings_df_train, buildings_df, weather_df_train)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Feature engineering",
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2025-01-08T07:28:05.619410Z",
                    "iopub.execute_input": "2025-01-08T07:28:05.620589Z",
                    "iopub.status.idle": "2025-01-08T07:28:07.073796Z",
                    "shell.execute_reply.started": "2025-01-08T07:28:05.620539Z",
                    "shell.execute_reply": "2025-01-08T07:28:07.072645Z"
                }
            }
        },
        {
            "cell_type": "code",
            "source": "def kbtu_to_kwh(df: pd.DataFrame) -> pd.DataFrame:\n    mask = (df[\"building_id\"] == 0) & (df[\"meter_id\"] == 0)\n    df.loc[mask, \"meter_reading\"] = df.loc[mask, \"meter_reading\"] * 0.2931\n    return df\n\n\ndef add_periodic_features(df: pd.DataFrame, feature: str, period: int) -> pd.DataFrame:\n    df[f\"{feature}_sin\"] = np.sin(2 * np.pi * df[feature] / period)\n    df[f\"{feature}_sin\"] = df[f\"{feature}_sin\"].astype(np.float32)\n    \n    df[f\"{feature}_cos\"] = np.cos(2 * np.pi * df[feature] / period).astype(np.float32)\n    df[f\"{feature}_cos\"] = df[f\"{feature}_cos\"].astype(np.float32)\n    \n    return df\n\n\ndef add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype(np.uint8)\n    df = add_periodic_features(df, \"hour\", 24)\n\n    df[\"day_of_week\"] = df[\"timestamp\"].dt.weekday.astype(np.uint8)\n    df = add_periodic_features(df, \"day_of_week\", 7)\n\n    df[\"month\"] = df[\"timestamp\"].dt.month.astype(np.uint8)\n    df = add_periodic_features(df, \"month\", 12)\n\n    is_weekend = (df[\"timestamp\"].dt.weekday >= 5)\n    df[\"is_weekend\"] = is_weekend.astype(np.uint8)\n    \n    return df\n\n\ndef add_building_age_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"building_age_years\"] = df[\"timestamp\"].dt.year - df[\"year_built\"]\n    return df\n\n\ndef add_building_area_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df[\"building_area_square_feet\"] = df[\"square_feet\"] * df[\"floor_count\"]\n    return df\n\n\ndef saturation_vapour_pressure(temperature: pd.Series) -> pd.Series:\n    return 6.1094 * np.exp(17.625 * temperature / (temperature + 243.04))\n\n\ndef add_relative_humidity_feature(df: pd.DataFrame) -> pd.DataFrame:\n    svp_air_temp = saturation_vapour_pressure(df[\"air_temperature\"])\n    svp_dew_temp = saturation_vapour_pressure(df[\"dew_temperature\"])\n    rh = 100 * svp_dew_temp / svp_air_temp\n    df[\"relative_humidity\"] = rh.astype(np.float32)\n    return df\n\n\ndef add_cold_chill_feature(df: pd.DataFrame) -> pd.DataFrame:\n    # Cold chill only defined for temps below 10C and wind speeds above 1.3 m/s\n    mask = (df[\"air_temperature\"] <= 10.0) & (df[\"wind_speed\"] >= 1.3)\n    air_temp = df.loc[mask, \"air_temperature\"]\n    wind_speed = df.loc[mask, \"wind_speed\"]\n    cold_chill = (\n        13.12 \n        +  0.6215 * air_temp\n        - 11.37 * (3.6 * wind_speed) ** 0.16\n        + 0.3965 * air_temp * (3.6 * wind_speed) ** 0.16\n    )\n    df.loc[mask, \"cold_chill\"] = cold_chill.astype(np.float32)\n    return df\n\n\ndef add_apparent_temperature_feature(df: pd.DataFrame) -> pd.DataFrame:\n    mask = df[\"air_temperature\"].between(10, 27, inclusive=\"both\")\n    air_temp = df.loc[mask, \"air_temperature\"]\n    wind_speed = df.loc[mask, \"wind_speed\"]\n    humidity = df.loc[mask, \"relative_humidity\"] / 100\n    pressure = humidity * 6.105 * np.exp((17.27 * air_temp) / (air_temp + 237.7))\n    apparent_temp = air_temp + 0.33 * pressure - 0.7 * wind_speed - 4\n    df.loc[mask, \"apparent_temperature\"] = apparent_temp.astype(np.float32)\n    return df\n\n\ndef add_heat_index_feature(df: pd.DataFrame) -> pd.DataFrame:\n    mask = df[\"air_temperature\"] >= 27\n    air_temp = df.loc[mask, \"air_temperature\"]\n    humidity = df.loc[mask, \"relative_humidity\"]\n    heat_index = (\n        - 8.7847 \n        + 1.6114 * air_temp \n        + 2.3385 * humidity\n        - 0.1461 * air_temp * humidity\n        - 0.0123 * air_temp ** 2 \n        - 0.0164 * humidity ** 2\n        + 2.212e-03 * air_temp ** 2 * humidity\n        + 7.255e-04 * air_temp * humidity ** 2\n        - 3.582e-06 * air_temp ** 2 * humidity ** 2\n    )\n    df.loc[mask, \"heat_index\"] = heat_index.astype(np.float32)\n    return df\n\n\ndef cooling_degree_days(df: pd.DataFrame) -> pd.DataFrame:\n    # https://www.investopedia.com/terms/c/colddegreeday.asp\n    ...\n\n\ndef heating_degree_days():\n    ...",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Timestamp features\ntrain_df = add_temporal_features(train_df)\n\n# Meter reading features\ntrain_df = kbtu_to_kwh(train_df)\n\n# Building features\ntrain_df = add_building_age_feature(train_df)\ntrain_df = add_building_area_feature(train_df)\n\n# Weather features\ntrain_df = add_relative_humidity_feature(train_df)\ntrain_df = add_cold_chill_feature(train_df)\ntrain_df = add_apparent_temperature_feature(train_df)\ntrain_df = add_heat_index_feature(train_df)\ntrain_df = add_periodic_features(train_df, \"wind_direction\", 360)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "train_df.to_parquet(\"train_df.parquet\")",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Test data",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Meter readings\nreadings_df_test = pd.read_csv(\n    f\"{INPUT_DATA_PATH}/test.csv\",\n    header=0,\n    names=[\"row_id\", \"building_id\", \"meter_id\", \"timestamp\"],\n)\nreadings_df_test = cast_readings_data(readings_df_test)\nreadings_df_test[\"row_id\"] = readings_df_test[\"row_id\"].astype(np.uint32)\n\n# Weather\nweather_df_test = pd.read_csv(f\"{INPUT_DATA_PATH}/weather_test.csv\")\nweather_df_test = cast_weather_data(weather_df_test)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Reindex weather data\nweather_df_test = reindex_weather_data(\n    weather_df=weather_df_test,\n    start_timestamp=MIN_TEST_TIMESTAMP,\n    end_timestamp=MAX_TEST_TIMESTAMP,\n)\n\n# Concat train and test weather data\ntrain_timestamp_cutoff = MIN_TEST_TIMESTAMP - pd.Timedelta(\"2d\")\ntimestamp_mask = weather_df_train[\"timestamp\"] >= train_timestamp_cutoff\nweather_cols = [\"timestamp\", \"site_id\"] + WEATHER_FEATURE_COLUMNS\nweather_df_test = pd.concat(\n    [\n        weather_df_test,\n        weather_df_train[timestamp_mask][weather_cols]\n        \n    ],\n    axis=0,\n    ignore_index=True\n)\n\n# Fill missing weather data\nweather_df_test = fill_missing_weather_data(weather_df_test)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Compute lagged / rolling weather features\n\n# Air temperature\nfeature_name = \"air_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_test = add_lagged_weather_feature(weather_df_test, feature_name, lag)\nfor window in WEATHER_ROLLING_WINDOWS:\n    weather_df_test = add_rolling_mean_weather_feature(weather_df_test, feature_name, window)\n\n# Dew temperature\nfeature_name = \"dew_temperature\"\nfor lag in WEATHER_LAGS:\n    weather_df_test = add_lagged_weather_feature(weather_df_test, feature_name, lag)\nfor window in WEATHER_ROLLING_WINDOWS:\n    weather_df_test = add_rolling_mean_weather_feature(weather_df_test, feature_name, window)\n\n# Sea level pressure\nfeature_name = \"sea_level_pressure\"\nfor lag in WEATHER_LAGS:\n    weather_df_test = add_lagged_weather_feature(weather_df_test, feature_name, lag)\nfor window in WEATHER_ROLLING_WINDOWS:\n    weather_df_test = add_rolling_mean_weather_feature(weather_df_test, feature_name, window)\n\n\nweather_df_test = weather_df_test.sort_values([\"site_id\", \"timestamp\"]).reset_index(drop=True)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Merge all dfs\ntest_df = merge_dfs(readings_df_test, buildings_df, weather_df_test)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "### Feature engineering",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Timestamp features\ntest_df = add_temporal_features(test_df)\n\n# Building features\ntest_df = add_building_age_feature(test_df)\ntest_df = add_building_area_feature(test_df)\n\n# Weather features\ntest_df = add_relative_humidity_feature(test_df)\ntest_df = add_cold_chill_feature(test_df)\ntest_df = add_apparent_temperature_feature(test_df)\ntest_df = add_heat_index_feature(test_df)\ntest_df = add_periodic_features(test_df, \"wind_direction\", 360)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "test_df.to_parquet(\"test_df.parquet\")",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}