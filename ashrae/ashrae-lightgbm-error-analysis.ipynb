{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9994,"databundleVersionId":752467,"sourceType":"competition"},{"sourceId":10716110,"sourceType":"datasetVersion","datasetId":6473980}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nimport matplotlib.colors as mcolors\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgbm\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Constants\nINPUT_DATA_PATH = \"/kaggle/input/ashrae-energy-prediction\"\n\nMIN_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-01-01 00:00:00\")\nMAX_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-12-31 23:00:00\")\nMIN_TEST_TIMESTAMP = pd.Timestamp(\"2017-01-01 00:00:00\")\nMAX_TEST_TIMESTAMP = pd.Timestamp('2018-12-31 23:00:00')\n\nDATA_RESOLUTION = \"1h\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"CATEGORY_COLS = [\"building_id\", \"meter_id\", \"site_id\", \"primary_use\"]\nUINT8_COLS = [\"hour\", \"day_of_week\", \"month\"]\n\n\ndef drop_cols(df: pd.DataFrame) -> pd.DataFrame:\n    cols_to_drop = [\"hour\", \"day_of_week\", \"month\"]\n    df = df.drop(columns=cols_to_drop)\n    return df\n\n\ndef cast_dtypes(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n\n    # Timestamps\n    try:\n        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    except KeyError:\n        if verbose:\n            print(\"Col 'timestamp' missing from df. Skipping ...\")\n\n    # Categories\n    for col in CATEGORY_COLS:\n        try:\n            df[col] = df[col].astype(\"category\")\n        except KeyError:\n            if verbose:\n                print(f\"Col '{col}' missing from df. Skipping ...\")\n\n    # UINT8\n    for col in UINT8_COLS:\n        try:\n            if df[col].max() > np.iinfo(np.uint8).max:\n                print(f\"Col max for '{col}' exceeds np.uint8 max. Skipping ...\")\n                continue\n            df[col] = df[col].astype(np.uint8)\n        except KeyError:\n            if verbose:\n                print(f\"Col '{col}' missing from df. Skipping ...\")\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = pd.read_parquet(\"/kaggle/input/ashrae-iii/train_df.parquet\")\ntrain_dataset = drop_cols(train_dataset)\ntrain_dataset = cast_dtypes(train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"CATEGORICAL_FEATURES = [\n    \"building_id\",\n    \"meter_id\",\n    \"site_id\",\n    \"primary_use\",\n]\nNUMERICAL_FEATURES = [\n    \"square_feet\",\n    \"floor_count\",\n    \"air_temperature\",\n    \"cloud_coverage\",\n    \"dew_temperature\",\n    \"precip_depth_1_hr\",\n    \"sea_level_pressure\",\n    \"wind_direction_sin\",\n    \"wind_direction_cos\",\n    \"wind_speed\",\n    \"air_temperature_lag_1\",\n    \"air_temperature_lag_2\",\n    \"air_temperature_lag_3\",\n    \"air_temperature_lag_4\",\n    \"air_temperature_lag_5\",\n    \"air_temperature_rolling_mean_12\",\n    \"air_temperature_rolling_mean_24\",\n    \"dew_temperature_lag_1\",\n    \"dew_temperature_lag_2\",\n    \"dew_temperature_lag_3\",\n    \"dew_temperature_lag_4\",\n    \"dew_temperature_lag_5\",\n    \"dew_temperature_rolling_mean_12\",\n    \"dew_temperature_rolling_mean_24\",\n    \"sea_level_pressure_lag_1\",\n    \"sea_level_pressure_lag_2\",\n    \"sea_level_pressure_lag_3\",\n    \"sea_level_pressure_lag_4\",\n    \"sea_level_pressure_lag_5\",\n    \"sea_level_pressure_rolling_mean_12\",\n    \"sea_level_pressure_rolling_mean_24\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"day_of_week_sin\",\n    \"day_of_week_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"is_weekend\",\n    \"building_age_years\",\n    \"building_area_square_feet\",\n    \"relative_humidity\",\n    \"cold_chill\",\n    \"apparent_temperature\",\n    \"heat_index\",\n]\nFEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n\nLABEL = \"meter_reading\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_ITERATIONS = 10\nTRAIN_PARAMETERS = {\n    \"objective\": \"mean_squared_error\",\n    \"learning_rate\": 0.01,\n    \"seed\": 1,\n    \"max_bin\": 255,\n    \"num_leaves\": 2 ** 6 - 1,\n    \"min_data_in_leaf\": 50,\n    \"metric\": [\"rmse\"],\n}\nDATASET_PARAMETERS = {\"categorical_feature\": CATEGORICAL_FEATURES}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_column_transformer() -> ColumnTransformer:\n    ordinal_encoder = OrdinalEncoder(\n        categories=\"auto\",\n        handle_unknown=\"use_encoded_value\",\n        unknown_value=-1,\n        dtype=np.int32,\n    )\n    transformer = ColumnTransformer(\n        transformers=[\n            (\"numerical_features\", \"passthrough\", NUMERICAL_FEATURES),\n            (\"ordinal_encoder\", ordinal_encoder, [\"primary_use\"]),\n        ],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    )\n    transformer.set_output(transform=\"pandas\")\n    return transformer\n\n\ndef target_transform(y: pd.Series) -> pd.Series:\n    return np.log1p(y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_start, val_end = pd.Timestamp(\"2016-11-01 00:00:00\"), pd.Timestamp(\"2016-12-01 00:00:00\")\nval_start, val_end = pd.Timestamp(\"2016-12-01 00:00:00\"), pd.Timestamp(\"2017-01-01 00:00:00\")\n\n# Train / val split\ntrain_df = train_dataset[train_dataset[\"timestamp\"] < val_start]\nvalid_df = train_dataset[(train_dataset[\"timestamp\"].between(val_start, val_end, inclusive=\"left\"))]\n\nX_train, y_train = train_df[FEATURES], train_df[LABEL]\nX_valid, y_valid = valid_df[FEATURES], valid_df[LABEL]\n\n# Feature / target transforms\ny_train = target_transform(y_train)\ny_valid = target_transform(y_valid)\n\n# col_transformer = get_column_transformer()\n# col_tranformer = col_transformer.fit(X_train, y_train)\n# X_train = col_transformer.transform(X_train)\n# X_train = X_train.astype({\"primary_use\": \"category\"})\n# X_valid = col_tranformer.transform(X_valid)\n# X_valid = X_valid.astype({\"primary_use\": \"category\"})\n\n# train_ds = lgbm.Dataset(data=X_train, label=y_train)\n# valid_ds = lgbm.Dataset(data=X_valid, label=y_valid)\n\n# # Train Lightgbm\n# eval_results = {}\n# model = lgbm.train(\n#     TRAIN_PARAMETERS,\n#     num_boost_round=N_ITERATIONS,\n#     train_set=train_ds,\n#     valid_sets=[train_ds, valid_ds],\n#     valid_names=[\"train\", \"valid\"],\n#     callbacks=[\n#         lgbm.log_evaluation(period=10),\n#         lgbm.record_evaluation(eval_results),\n#     ]\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict \n# y_hat = model.predict(X_valid)\ny_hat = np.random.normal(size=y_valid.shape)\n\nvalid_df = valid_df.copy()\nvalid_df[\"y_true\"] = y_valid\nvalid_df[\"y_hat\"] = y_hat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evalute","metadata":{}},{"cell_type":"code","source":"valid_df[\"squared_error\"] = (y_hat - y_valid) ** 2\nvalid_df = valid_df.set_index([\"meter_id\", \"site_id\", \"building_id\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predictions by site, meter, and building","metadata":{}},{"cell_type":"code","source":"# Plot predictions for individual buildings\nmeter_id = 2\nsite_id = 15\n\nn_plots = 5\nn_buildings_per_plot = 3\n\n# Sample some buildings to plot\nall_building_ids = valid_df.loc[(meter_id, site_id)].index.values\nbuilding_ids_to_plot = np.random.choice(\n    all_building_ids,\n    size=n_plots * n_buildings_per_plot,\n    replace=False\n)\n\nfig, ax = plt.subplots(n_plots, 1, figsize=(10, 12), sharex=True)\ncolors = cm.copper(np.linspace(0, 1, n_buildings_per_plot))\nfor plot_idx in range(n_plots):\n    # Get indices of buildings to plot\n    start_b_idx = plot_idx * n_buildings_per_plot\n    end_b_idx = start_b_idx + n_buildings_per_plot\n    building_ids_current_plot = building_ids_to_plot[start_b_idx: end_b_idx]\n    \n    for b_idx, building_id in enumerate(building_ids_current_plot):\n        msb_df = valid_df.loc[(meter_id, site_id, building_id)].sort_values(\"timestamp\")\n        ax[plot_idx].plot(\n            msb_df[\"timestamp\"].values,\n            msb_df[\"y_true\"].values,\n            label=building_id,\n            color=colors[b_idx],\n        )\n        ax[plot_idx].plot(\n            msb_df[\"timestamp\"].values,\n            msb_df[\"y_hat\"].values,\n            color=colors[b_idx],\n            ls=\"--\",\n        )\n\n    ax[plot_idx].legend(fontsize=\"small\", ncols=n_buildings_per_plot, loc=4)\n    ax[plot_idx].set_ylabel(\"log meter_reading\")\n    # for tick in ax[plot_idx].get_xticklabels():\n    #     tick.set_rotation(45)\n    \nfig.tight_layout();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Error heatmaps","metadata":{}},{"cell_type":"code","source":"site_ids_by_meter_type = {\n    0: [2, 3, 14],\n    1: [2, 9, 14],\n    2: [9, 13, 15],\n    3: [1, 2, 14],\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot heatmaps\nmeter_id = 0\nsites_to_plot = site_ids_by_meter_type[meter_id]\n\nfig, ax = plt.subplots(1, len(sites_to_plot), figsize=(5 * len(sites_to_plot), 4)) \n\nfor i, site_id in enumerate(sites_to_plot):\n    site_meter_df = (\n        valid_df.loc[(meter_id, site_id)]\n        .reset_index()\n        .sort_values([\"building_id\", \"timestamp\"])\n        .pivot(index=\"building_id\", columns=\"timestamp\", values=\"squared_error\")\n    )\n    \n    ax[i] = sns.heatmap(site_meter_df, ax=ax[i])\n    \n    # Format x-axis labels\n    timestamps = site_meter_df.columns\n    xtick_locs = range(0, len(timestamps), 72)  # Every third entry\n    xtick_labels = [pd.to_datetime(timestamps[i]).strftime(\"%Y-%m-%d\") for i in xtick_locs]\n    \n    ax[i].set_xticks(xtick_locs)\n    ax[i].set_xticklabels(xtick_labels, rotation=45, ha=\"center\")\n    ax[i].set_xlabel(\"\")\n    \nfig.tight_layout();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def error_scale(e: float, *thresholds):\n    for idx, threshold in enumerate(thresholds, start=1):\n        if e < threshold:\n            return idx\n    return len(thresholds) + 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meter_id = 0\nsites_to_plot = site_ids_by_meter_type[meter_id]\n\nfig, ax = plt.subplots(1, len(sites_to_plot), figsize=(5 * len(sites_to_plot), 4)) \n\nfor i, site_id in enumerate(sites_to_plot):\n    site_meter_df = (\n        valid_df.loc[(meter_id, site_id)]\n        .reset_index()\n        .sort_values([\"building_id\", \"timestamp\"])\n    )\n    \n    # Get error thresholds and scales\n    error_threshold_q75 = np.percentile(site_meter_df[\"squared_error\"], q=75)\n    error_threshold_q90 = np.percentile(site_meter_df[\"squared_error\"], q=90)\n    error_thresholds = [error_threshold_q75, error_threshold_q90]\n    site_meter_df[\"error_scale\"] = site_meter_df[\"squared_error\"].apply(error_scale, args=error_thresholds)\n    \n    site_meter_df = site_meter_df.pivot(index=\"building_id\", columns=\"timestamp\", values=\"error_scale\")\n\n    # Define color map\n    error_scales = [1, 2, 3]\n    colors = [\"lightgrey\", \"green\", \"orange\"]\n    cmap = mcolors.ListedColormap(colors)\n    bounds = [e - 0.5 for e in error_scales + [4]]\n    norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=len(colors))\n    \n    ax[i] = sns.heatmap(site_meter_df, ax=ax[i], cmap=cmap, norm=norm)\n    cbar = ax[i].collections[0].colorbar\n    cbar.set_ticks(error_scales)\n    cbar.set_ticklabels([str(e) for e in error_scales])\n    \n    # Format x-axis labels\n    timestamps = site_meter_df.columns\n    xtick_locs = range(0, len(timestamps), 72)  # Every third entry\n    xtick_labels = [pd.to_datetime(timestamps[i]).strftime(\"%Y-%m-%d\") for i in xtick_locs]\n    \n    ax[i].set_xticks(xtick_locs)\n    ax[i].set_xticklabels(xtick_labels, rotation=45, ha=\"center\")\n    ax[i].set_xlabel(\"\")\n    \nfig.tight_layout();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}