{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9994,"databundleVersionId":752467,"sourceType":"competition"},{"sourceId":10716110,"sourceType":"datasetVersion","datasetId":6473980}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nimport matplotlib.colors as mcolors\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgbm\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Constants\nINPUT_DATA_PATH = \"/kaggle/input/ashrae-energy-prediction\"\n\nMIN_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-01-01 00:00:00\")\nMAX_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-12-31 23:00:00\")\nMIN_TEST_TIMESTAMP = pd.Timestamp(\"2017-01-01 00:00:00\")\nMAX_TEST_TIMESTAMP = pd.Timestamp('2018-12-31 23:00:00')\n\nDATA_RESOLUTION = \"1h\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"CATEGORY_COLS = [\"building_id\", \"meter_id\", \"site_id\", \"primary_use\"]\nUINT8_COLS = [\"hour\", \"day_of_week\", \"month\"]\n\n\ndef drop_cols(df: pd.DataFrame) -> pd.DataFrame:\n    cols_to_drop = [\"hour\", \"day_of_week\", \"month\"]\n    df = df.drop(columns=cols_to_drop)\n    return df\n\n\ndef cast_dtypes(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n\n    # Timestamps\n    try:\n        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    except KeyError:\n        if verbose:\n            print(\"Col 'timestamp' missing from df. Skipping ...\")\n\n    # Categories\n    for col in CATEGORY_COLS:\n        try:\n            df[col] = df[col].astype(\"category\")\n        except KeyError:\n            if verbose:\n                print(f\"Col '{col}' missing from df. Skipping ...\")\n\n    # UINT8\n    for col in UINT8_COLS:\n        try:\n            if df[col].max() > np.iinfo(np.uint8).max:\n                print(f\"Col max for '{col}' exceeds np.uint8 max. Skipping ...\")\n                continue\n            df[col] = df[col].astype(np.uint8)\n        except KeyError:\n            if verbose:\n                print(f\"Col '{col}' missing from df. Skipping ...\")\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = pd.read_parquet(\"/kaggle/input/ashrae-iii/train_df.parquet\")\ntrain_dataset = drop_cols(train_dataset)\ntrain_dataset = cast_dtypes(train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"CATEGORICAL_FEATURES = [\n    \"building_id\",\n    \"meter_id\",\n    \"site_id\",\n    \"primary_use\",\n]\nNUMERICAL_FEATURES = [\n    \"square_feet\",\n    \"floor_count\",\n    \"air_temperature\",\n    \"cloud_coverage\",\n    \"dew_temperature\",\n    \"precip_depth_1_hr\",\n    \"sea_level_pressure\",\n    \"wind_direction_sin\",\n    \"wind_direction_cos\",\n    \"wind_speed\",\n    \"air_temperature_lag_1\",\n    \"air_temperature_lag_2\",\n    \"air_temperature_lag_3\",\n    \"air_temperature_lag_4\",\n    \"air_temperature_lag_5\",\n    \"air_temperature_rolling_mean_12\",\n    \"air_temperature_rolling_mean_24\",\n    \"dew_temperature_lag_1\",\n    \"dew_temperature_lag_2\",\n    \"dew_temperature_lag_3\",\n    \"dew_temperature_lag_4\",\n    \"dew_temperature_lag_5\",\n    \"dew_temperature_rolling_mean_12\",\n    \"dew_temperature_rolling_mean_24\",\n    \"sea_level_pressure_lag_1\",\n    \"sea_level_pressure_lag_2\",\n    \"sea_level_pressure_lag_3\",\n    \"sea_level_pressure_lag_4\",\n    \"sea_level_pressure_lag_5\",\n    \"sea_level_pressure_rolling_mean_12\",\n    \"sea_level_pressure_rolling_mean_24\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"day_of_week_sin\",\n    \"day_of_week_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"is_weekend\",\n    \"building_age_years\",\n    \"building_area_square_feet\",\n    \"relative_humidity\",\n    \"cold_chill\",\n    \"apparent_temperature\",\n    \"heat_index\",\n]\nFEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n\nLABEL = \"meter_reading\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_ITERATIONS = 10\nTRAIN_PARAMETERS = {\n    \"objective\": \"mean_squared_error\",\n    \"learning_rate\": 0.01,\n    \"seed\": 1,\n    \"max_bin\": 255,\n    \"num_leaves\": 2 ** 6 - 1,\n    \"min_data_in_leaf\": 50,\n    \"metric\": [\"rmse\"],\n}\nDATASET_PARAMETERS = {\"categorical_feature\": CATEGORICAL_FEATURES}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_column_transformer() -> ColumnTransformer:\n    ordinal_encoder = OrdinalEncoder(\n        categories=\"auto\",\n        handle_unknown=\"use_encoded_value\",\n        unknown_value=-1,\n        dtype=np.int32,\n    )\n    transformer = ColumnTransformer(\n        transformers=[\n            (\"numerical_features\", \"passthrough\", NUMERICAL_FEATURES),\n            (\"ordinal_encoder\", ordinal_encoder, [\"primary_use\"]),\n        ],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    )\n    transformer.set_output(transform=\"pandas\")\n    return transformer\n\n\ndef target_transform(y: pd.Series) -> pd.Series:\n    return np.log1p(y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_start, val_end = pd.Timestamp(\"2016-11-01 00:00:00\"), pd.Timestamp(\"2016-12-01 00:00:00\")\nval_start, val_end = pd.Timestamp(\"2016-12-01 00:00:00\"), pd.Timestamp(\"2017-01-01 00:00:00\")\n\n# Train / val split\ntrain_df = train_dataset[train_dataset[\"timestamp\"] < val_start]\nvalid_df = train_dataset[(train_dataset[\"timestamp\"].between(val_start, val_end, inclusive=\"left\"))]\n\nX_train, y_train = train_df[FEATURES], train_df[LABEL]\nX_valid, y_valid = valid_df[FEATURES], valid_df[LABEL]\n\n# Feature / target transforms\ny_train = target_transform(y_train)\ny_valid = target_transform(y_valid)\n\ncol_transformer = get_column_transformer()\ncol_tranformer = col_transformer.fit(X_train, y_train)\nX_train = col_transformer.transform(X_train)\nX_train = X_train.astype({\"primary_use\": \"category\"})\nX_valid = col_tranformer.transform(X_valid)\nX_valid = X_valid.astype({\"primary_use\": \"category\"})\n\ntrain_ds = lgbm.Dataset(data=X_train, label=y_train)\nvalid_ds = lgbm.Dataset(data=X_valid, label=y_valid)\n\n# Train Lightgbm\neval_results = {}\nmodel = lgbm.train(\n    TRAIN_PARAMETERS,\n    num_boost_round=N_ITERATIONS,\n    train_set=train_ds,\n    valid_sets=[train_ds, valid_ds],\n    valid_names=[\"train\", \"valid\"],\n    callbacks=[\n        lgbm.log_evaluation(period=10),\n        lgbm.record_evaluation(eval_results),\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict \ny_hat = model.predict(X_valid)\n\nvalid_df = valid_df.copy()\nvalid_df[\"y_true\"] = y_valid\nvalid_df[\"y_hat\"] = y_hat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evalute","metadata":{}},{"cell_type":"code","source":"valid_df[\"squared_error\"] = (y_hat - y_valid) ** 2\nvalid_df = valid_df.set_index([\"meter_id\", \"site_id\", \"building_id\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predictions by site, meter, and building","metadata":{}},{"cell_type":"code","source":"# Plot predictions for individual buildings\nmeter_id = 2\nsite_id = 14\nbuilding_ids = valid_df.loc[(meter_id, site_id)].index.values\n\nn_plots = 5\nn_buildings_per_plot = 3\n\nfig, ax = plt.subplots(1, n_plots, figsize=(20, 3.5))\ncolors = cm.copper(np.linspace(0, 1, n_buildings_per_plot))\nfor plot_idx in range(n_plots):\n    building_ids_to_plot = np.random.choice(building_ids, size=n_buildings_per_plot)\n    for b_idx, building_id in enumerate(building_ids_to_plot):\n        msb_df = valid_df.loc[(meter_id, site_id, building_id)].sort_values(\"timestamp\")\n        ax[plot_idx].plot(\n            msb_df[\"timestamp\"].values,\n            msb_df[\"y_true\"].values,\n            label=f\"B.ID: {building_id}\",\n            color=colors[b_idx],\n        )\n        ax[plot_idx].plot(\n            msb_df[\"timestamp\"].values,\n            msb_df[\"y_hat\"].values,\n            color=colors[b_idx],\n            ls=\"--\",\n        )\n\n    ax[plot_idx].legend(fontsize=\"x-small\")\n    for tick in ax[plot_idx].get_xticklabels():\n        tick.set_rotation(45)\n    \nfig.tight_layout();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"site_ids_by_meter_type = {0: [2, 3, 14]}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot heatmaps\nmeter_id = 0\nsites_to_plot = site_ids_by_meter_type[meter_id]\n\nfig, ax = plt.subplots(1, len(sites_to_plot), figsize=(5 * len(sites_to_plot), 4)) \n\nfor i, site_id in enumerate(sites_to_plot):\n    site_meter_df = (\n        valid_df.loc[(meter_id, site_id)]\n        .reset_index()\n        .sort_values([\"building_id\", \"timestamp\"])\n        .pivot(index=\"building_id\", columns=\"timestamp\", values=\"squared_error\")\n    )\n    \n    ax[i] = sns.heatmap(site_meter_df, ax=ax[i])\n    \n    # Format x-axis labels\n    timestamps = site_meter_df.columns\n    xtick_locs = range(0, len(timestamps), 72)  # Every third entry\n    xtick_labels = [pd.to_datetime(timestamps[i]).strftime(\"%Y-%m-%d\") for i in xtick_locs]\n    \n    ax[i].set_xticks(xtick_locs)\n    ax[i].set_xticklabels(xtick_labels, rotation=45, ha=\"center\")\n    ax[i].set_xlabel(\"\")\n    \nfig.tight_layout();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meter_id = 0\nsites_to_plot = site_ids_by_meter_type[meter_id]\n\nfig, ax = plt.subplots(1, len(sites_to_plot), figsize=(5 * len(sites_to_plot), 4)) \n\nfor i, site_id in enumerate(sites_to_plot):\n    site_meter_df = (\n        valid_df.loc[(meter_id, site_id)]\n        .reset_index()\n        .sort_values([\"building_id\", \"timestamp\"])\n    )\n    \n    # Get error thresholds and scales\n    error_threshold_q75 = np.percentile(site_meter_df[\"squared_error\"], q=75)\n    error_threshold_q90 = np.percentile(site_meter_df[\"squared_error\"], q=90)\n    \n    def error_threshold(e: float):\n        if e < error_threshold_q75:\n            return 1\n        if e < error_threshold_q90:\n            return 2\n        return 3\n    \n    site_meter_df[\"error_scale\"] = site_meter_df[\"squared_error\"].apply(error_threshold)\n    \n    site_meter_df = site_meter_df.pivot(index=\"building_id\", columns=\"timestamp\", values=\"error_scale\")\n    \n    ax[i] = sns.heatmap(site_meter_df, ax=ax[i], cmap=cmap, norm=norm, cbar=True, linewidths=0.5)\n    cbar = ax[i].collections[0].colorbar\n    cbar.set_ticks(unique_values)\n    cbar.set_ticklabels([str(v) for v in unique_values])\n    \n    # Format x-axis labels\n    timestamps = site_meter_df.columns\n    xtick_locs = range(0, len(timestamps), 72)  # Every third entry\n    xtick_labels = [pd.to_datetime(timestamps[i]).strftime(\"%Y-%m-%d\") for i in xtick_locs]\n    \n    ax[i].set_xticks(xtick_locs)\n    ax[i].set_xticklabels(xtick_labels, rotation=45, ha=\"center\")\n    ax[i].set_xlabel(\"\")\n    \nfig.tight_layout();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_values = [0, 1, 2]\n# Define a custom discrete colormap with three colors\ncolors = [\"#1f78b4\", \"#33a02c\", \"#e31a1c\"]  # Blue, Green, Red\ncmap = mcolors.ListedColormap(colors)\n\n# Create a normalization that maps unique values to colors\nbounds = unique_values + [unique_values[-1] + 1]  # Add upper bound\nnorm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=len(colors))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}