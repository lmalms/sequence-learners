{
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "sourceType": "competition",
                    "sourceId": 9994,
                    "databundleVersionId": 752467
                },
                {
                    "sourceType": "datasetVersion",
                    "sourceId": 10716110,
                    "datasetId": 6473980,
                    "databundleVersionId": 11065150
                }
            ],
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": false
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import gc\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgbm\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom tqdm import tqdm",
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "SUBMISSION_RUN = False",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Constants\nINPUT_DATA_PATH = \"/kaggle/input/ashrae-energy-prediction\"\n\nMIN_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-01-01 00:00:00\")\nMAX_TRAIN_TIMESTAMP = pd.Timestamp(\"2016-12-31 23:00:00\")\nMIN_TEST_TIMESTAMP = pd.Timestamp(\"2017-01-01 00:00:00\")\nMAX_TEST_TIMESTAMP = pd.Timestamp('2018-12-31 23:00:00')\n\nDATA_RESOLUTION = \"1h\"\n\nVALIDATION_PERIODS = [\n    (pd.Timestamp(\"2016-10-01 00:00:00\"), pd.Timestamp(\"2016-11-01 00:00:00\")),\n    (pd.Timestamp(\"2016-11-01 00:00:00\"), pd.Timestamp(\"2016-12-01 00:00:00\")),\n    (pd.Timestamp(\"2016-12-01 00:00:00\"), pd.Timestamp(\"2017-01-01 00:00:00\")),\n]",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Data loading",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "CATEGORY_COLS = [\"building_id\", \"meter_id\", \"site_id\", \"primary_use\"]\nUINT8_COLS = [\"hour\", \"day_of_week\", \"month\"]\n\n\ndef drop_cols(df: pd.DataFrame) -> pd.DataFrame:\n    cols_to_drop = [\"hour\", \"day_of_week\", \"month\"]\n    cols_to_drop = cols_to_drop + ([\"timestamp\"] if SUBMISSION_RUN else [])\n    df = df.drop(columns=cols_to_drop)\n    return df\n\n\ndef cast_dtypes(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n\n    # Timestamps\n    try:\n        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    except KeyError:\n        if verbose:\n            print(\"Col 'timestamp' missing from df. Skipping ...\")\n\n    # Categories\n    for col in CATEGORY_COLS:\n        try:\n            df[col] = df[col].astype(\"category\")\n        except KeyError:\n            if verbose:\n                print(f\"Col '{col}' missing from df. Skipping ...\")\n\n    # UINT8\n    for col in UINT8_COLS:\n        try:\n            if df[col].max() > np.iinfo(np.uint8).max:\n                print(f\"Col max for '{col}' exceeds np.uint8 max. Skipping ...\")\n                continue\n            df[col] = df[col].astype(np.uint8)\n        except KeyError:\n            if verbose:\n                print(f\"Col '{col}' missing from df. Skipping ...\")\n    \n    return df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "train_dataset = pd.read_parquet(\"/kaggle/input/ashrae-iii/train_df.parquet\")\ntrain_dataset = drop_cols(train_dataset)\ntrain_dataset = cast_dtypes(train_dataset)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Cross Validation",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "CATEGORICAL_FEATURES = [\n    \"building_id\",\n    \"meter_id\",\n    \"site_id\",\n    \"primary_use\",\n]\nNUMERICAL_FEATURES = [\n    \"square_feet\",\n    \"floor_count\",\n    \"air_temperature\",\n    \"cloud_coverage\",\n    \"dew_temperature\",\n    \"precip_depth_1_hr\",\n    \"sea_level_pressure\",\n    \"wind_direction_sin\",\n    \"wind_direction_cos\",\n    \"wind_speed\",\n    \"air_temperature_lag_1\",\n    \"air_temperature_lag_2\",\n    \"air_temperature_lag_3\",\n    \"air_temperature_lag_4\",\n    \"air_temperature_lag_5\",\n    \"air_temperature_rolling_mean_12\",\n    \"air_temperature_rolling_mean_24\",\n    \"dew_temperature_lag_1\",\n    \"dew_temperature_lag_2\",\n    \"dew_temperature_lag_3\",\n    \"dew_temperature_lag_4\",\n    \"dew_temperature_lag_5\",\n    \"dew_temperature_rolling_mean_12\",\n    \"dew_temperature_rolling_mean_24\",\n    \"sea_level_pressure_lag_1\",\n    \"sea_level_pressure_lag_2\",\n    \"sea_level_pressure_lag_3\",\n    \"sea_level_pressure_lag_4\",\n    \"sea_level_pressure_lag_5\",\n    \"sea_level_pressure_rolling_mean_12\",\n    \"sea_level_pressure_rolling_mean_24\",\n    \"hour_sin\",\n    \"hour_cos\",\n    \"day_of_week_sin\",\n    \"day_of_week_cos\",\n    \"month_sin\",\n    \"month_cos\",\n    \"is_weekend\",\n    \"building_age_years\",\n    \"building_area_square_feet\",\n    \"relative_humidity\",\n    \"cold_chill\",\n    \"apparent_temperature\",\n    \"heat_index\",\n]\nFEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n\nLABEL = \"meter_reading\"",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "N_ITERATIONS = 2000\nTRAIN_PARAMETERS = {\n    \"objective\": \"mean_squared_error\",\n    \"learning_rate\": 0.01,\n    \"seed\": 1,\n    \"max_bin\": 255,\n    \"num_leaves\": 2 ** 6 - 1,\n    \"min_data_in_leaf\": 50,\n    \"metric\": [\"rmse\"],\n}\nDATASET_PARAMETERS = {\"categorical_feature\": CATEGORICAL_FEATURES}",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "def train_valid_split(train_df: pd.DataFrame):\n    for val_start, val_end in VALIDATION_PERIODS:\n        train_mask = train_df[\"timestamp\"] < val_start\n        test_mask = (train_df[\"timestamp\"] >= val_start) & (train_df[\"timestamp\"] < val_end)\n        yield train_df.loc[train_mask], train_df.loc[test_mask]\n\n\ndef get_column_transformer() -> ColumnTransformer:\n    ordinal_encoder = OrdinalEncoder(\n        categories=\"auto\",\n        handle_unknown=\"use_encoded_value\",\n        unknown_value=-1,\n        dtype=np.int32,\n    )\n    transformer = ColumnTransformer(\n        transformers=[\n            (\"numerical_features\", \"passthrough\", NUMERICAL_FEATURES),\n            (\"ordinal_encoder\", ordinal_encoder, [\"primary_use\"]),\n        ],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    )\n    transformer.set_output(transform=\"pandas\")\n    return transformer\n\n\ndef target_transform(y: pd.Series) -> pd.Series:\n    return np.log1p(y)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "if not SUBMISSION_RUN:\n    print(f\"Running cross validation ...\\n\")\n    \n    cv_results = []\n    for i, (train_df, valid_df) in enumerate(train_valid_split(train_dataset)):\n        \n        print(f\"Split {i}\")\n\n        X_train, y_train = train_df[FEATURES], train_df[LABEL]\n        X_valid, y_valid = valid_df[FEATURES], valid_df[LABEL]\n        \n        y_train = target_transform(y_train)\n        y_valid = target_transform(y_valid)\n\n        transformer = get_column_transformer()\n        transformer.fit(X_train, y_train)\n        X_train = transformer.transform(X_train)\n        X_train = X_train.astype({\"primary_use\": \"category\"})\n        X_valid = transformer.transform(X_valid)\n        X_valid = X_valid.astype({\"primary_use\": \"category\"})\n        \n        train_ds = lgbm.Dataset(data=X_train, label=y_train)\n        valid_ds = lgbm.Dataset(data=X_valid, label=y_valid)\n\n        # Train Lightgbm\n        eval_results = {}\n        model = lgbm.train(\n            TRAIN_PARAMETERS,\n            num_boost_round=N_ITERATIONS,\n            train_set=train_ds,\n            valid_sets=[train_ds, valid_ds],\n            valid_names=[\"train\", \"valid\"],\n            callbacks=[\n                lgbm.log_evaluation(period=10),\n                lgbm.record_evaluation(eval_results),\n            ]\n        )\n        cv_results.append(eval_results)\n\n        print(\"=================================\")\n\nelse:\n    print(f\"Training model for submission ...\\n\")\n\n    X_train, y_train = train_dataset[FEATURES], train_dataset[LABEL]\n    \n    transformer = get_column_transformer()\n    y_train = target_transform(y_train)\n    transformer.fit(X_train, y_train)\n    X_train = transformer.transform(X_train)\n    X_train = X_train.astype({\"primary_use\": \"category\"})\n    \n    train_ds = lgbm.Dataset(data=X_train, label=y_train)\n\n    eval_results = {}\n    model = lgbm.train(\n        TRAIN_PARAMETERS,\n        num_boost_round=N_ITERATIONS,\n        train_set=train_ds,\n        valid_sets=[train_ds],\n        valid_names=[\"train\"],\n        callbacks=[\n            lgbm.log_evaluation(period=10),\n            lgbm.record_evaluation(eval_results),\n        ]\n    )\n\n    del train_dataset\n    del X_train, y_train\n    gc.collect()",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Plot training loss\nif not SUBMISSION_RUN:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 3.5), sharex=True, sharey=True)\n    \n    for split_idx, split_scores in enumerate(cv_results):\n        ax[0].plot(\n            split_scores[\"train\"][\"rmse\"],\n            label=f\"split {split_idx}\",\n        )\n        ax[0].legend()\n        \n        ax[1].plot(\n            split_scores[\"valid\"][\"rmse\"],\n            label=f\"split {split_idx}\",\n        )\n        ax[1].legend()\n\n    plt.savefig(\"loss_curves.png\", dpi=300)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Submission",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def inverse_target_transform(y: np.ndarray) -> np.ndarray:\n    return np.clip(np.expm1(y), a_min=0.0, a_max=None)\n\n\ndef kwh_to_kbtu(df: pd.DataFrame) -> pd.DataFrame:\n    mask = (df[\"building_id\"] == 0) & (df[\"meter_id\"] == 0)\n    df.loc[mask, \"meter_reading\"] = df.loc[mask, \"meter_reading\"] * 3.4118\n    return df",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "if SUBMISSION_RUN:\n    \n    # Make predictions in batches\n    parquet_file = \"/kaggle/input/ashrae-iii/test_df.parquet\"\n    test_pq_file = pq.ParquetFile(parquet_file)\n    \n    prediction_dfs = []\n    for batch in tqdm(test_pq_file.iter_batches(100000)):\n        test_data = batch.to_pandas()\n        test_data = drop_cols(test_data)\n        test_data = cast_dtypes(test_data, verbose=False)\n    \n        # Predict\n        X_test = test_data[FEATURES]\n        X_test = transformer.transform(X_test)\n        X_test = X_test.astype({\"primary_use\": \"category\"})\n        y_hat = inverse_target_transform(model.predict(X_test))\n        y_hat = pd.DataFrame(y_hat, columns=[\"meter_reading\"])\n    \n        prediction_df = pd.concat([test_data, y_hat], axis=1)\n        prediction_df = kwh_to_kbtu(prediction_df)\n        prediction_dfs.append(prediction_df[[\"row_id\", \"meter_reading\"]])\n    \n    submission_df = pd.concat(prediction_dfs, axis=0)\n    submission_df = submission_df.sort_values(\"row_id\").reset_index(drop=True)\n    submission_df.to_csv(\"submission.csv\", index=False)",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}