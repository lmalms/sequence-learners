{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e46c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5796e04",
   "metadata": {},
   "source": [
    "# RNNs & LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed579ec",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"Nelson St\"\n",
    "cycle_counts = pd.read_csv(\"cycle_counts.csv\", parse_dates=[\"time\"])\n",
    "cycle_counts = cycle_counts[cycle_counts[\"location\"] == LOCATION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad34e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(cycle_counts[\"time\"], cycle_counts[\"count\"], lw=1.5)\n",
    "ax.set(title=LOCATION, ylabel=\"Count\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39495d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_counts[\"time\"] = pd.to_datetime(cycle_counts[\"time\"])\n",
    "cycle_counts = cycle_counts.set_index(\"time\").drop(columns=[\"location\"])\n",
    "cycle_counts = cycle_counts.resample(\"D\").sum().interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4263fa",
   "metadata": {},
   "source": [
    "## Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397141f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler():\n",
    "    def __init__(self):\n",
    "        self.mean_: float | None = None\n",
    "        self.scale_: float | None = None\n",
    "    \n",
    "    @property\n",
    "    def is_fit(self) -> bool:\n",
    "        return self.mean_ is not None and self.scale_ is not None\n",
    "    \n",
    "    def fit_transform(self, y: pd.Series) -> pd.Series:\n",
    "        self.mean_ = y.mean()\n",
    "        self.scale_ = y.std()\n",
    "        return self.transform(y)\n",
    "\n",
    "    def transform(self, y: pd.Series) -> pd.Series:\n",
    "        assert self.is_fit\n",
    "        return (y - self.mean_) / self.scale_\n",
    "\n",
    "    def inverse_transform(self, y: pd.Series) -> pd.Series: \n",
    "        assert self.is_fit\n",
    "        return y * self.scale_ + self.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(\n",
    "    timeseries: pd.Series,\n",
    "    in_seq_length: int,\n",
    "    out_seq_length: int,\n",
    "    batched_input: bool = False,\n",
    "    batched_output: bool = False\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    X_train, y_train = [], []\n",
    "    \n",
    "    last_ts_idx = len(timeseries) - in_seq_length\n",
    "    last_ts_idx -= out_seq_length if out_seq_length > 1 else 0\n",
    "    for i in range(last_ts_idx):\n",
    "        # Get the features\n",
    "        feat_start, feat_end = i, i + in_seq_length\n",
    "        feat_seq = timeseries.iloc[feat_start: feat_end]\n",
    "        X_train.append(feat_seq.values)\n",
    "\n",
    "        # Get the labels\n",
    "        if batched_output:\n",
    "            # Get an output sequence for each value of the input sequence\n",
    "            for j in range(in_seq_length):\n",
    "                label_start = i + j + 1\n",
    "                label_end = i + j + out_seq_length + 1\n",
    "                label_seq = timeseries.iloc[label_start: label_end]\n",
    "                y_train.append(label_seq.values)\n",
    "        \n",
    "        else:\n",
    "            # Only get a single output seq for each input sequence\n",
    "            label_start = i + in_seq_length\n",
    "            label_end = label_start + out_seq_length\n",
    "            label_seq = timeseries.iloc[label_start: label_end]\n",
    "            y_train.append(label_seq.values)\n",
    "\n",
    "    X_train = torch.tensor(np.array(X_train), dtype=torch.float)\n",
    "    X_train = (\n",
    "        X_train.view(-1, in_seq_length)\n",
    "        if not batched_input \n",
    "        else X_train.view(-1, in_seq_length, 1)\n",
    "    )\n",
    "\n",
    "    y_train = torch.tensor(np.array(y_train), dtype=torch.float32)\n",
    "    y_train = (\n",
    "        y_train.view(-1, out_seq_length)\n",
    "        if not batched_output \n",
    "        else y_train.view(-1, in_seq_length, out_seq_length)\n",
    "    )\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47754cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataset_and_index(\n",
    "    timeseries: pd.Series,\n",
    "    in_seq_length: int,\n",
    "    out_seq_length: int,\n",
    "    batched_input: bool = False,\n",
    "):\n",
    "    \n",
    "    X_test, test_index = [], []\n",
    "    max_X_index = len(timeseries) - in_seq_length - out_seq_length\n",
    "    for i in range(0, max_X_index, out_seq_length):\n",
    "        in_ = timeseries.iloc[i: i + in_seq_length]\n",
    "        X_test.append(in_)\n",
    "\n",
    "        index = timeseries.iloc[i + in_seq_length: i + in_seq_length + out_seq_length].index\n",
    "        test_index.append(index)\n",
    "\n",
    "    X_test = torch.tensor(np.array(X_test), dtype=torch.float32)\n",
    "    X_test = (\n",
    "        X_test.view(-1, in_seq_length)\n",
    "        if not batched_input\n",
    "        else X_test.view(-1, in_seq_length, 1)\n",
    "    )\n",
    "    return X_test, test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b3918",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f1351",
   "metadata": {},
   "source": [
    "### Forecast a single timestep ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_1(nn.Module):\n",
    "    def __init__(self, input_size: int = 1, hidden_size: int = 25):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        self.hh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.ho = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape to (batch_size, seq_len)\n",
    "        x = x.view(-1, 3)\n",
    "        \n",
    "        # Initialise the hidden state\n",
    "        h = torch.zeros(x.shape[0], self.hidden_size, dtype=torch.float32)\n",
    "        \n",
    "        # Hidden state after first input\n",
    "        h = F.relu(self.ih(x[:, 0].view(-1, 1)) + self.hh(h))\n",
    "\n",
    "        # Hidden state after second input\n",
    "        h = F.relu(self.ih(x[:, 1].view(-1, 1)) + self.hh(h))\n",
    "\n",
    "        # Hidden state after thrid input\n",
    "        h = F.relu(self.ih(x[:, 2].view(-1, 1)) + self.hh(h))\n",
    "\n",
    "        # Output based on current hidden state\n",
    "        return self.ho(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hide some of the logic here as an excercise\n",
    "\n",
    "scaler = Scaler()\n",
    "scaled_counts = scaler.fit_transform(cycle_counts[\"count\"])\n",
    "\n",
    "in_seq_len = 3\n",
    "out_seq_len = 1\n",
    "X_train, y_train = [], []\n",
    "for i in range(len(scaled_counts) - in_seq_len):\n",
    "    feat_seq = scaled_counts.iloc[i: i + in_seq_len]\n",
    "    X_train.append(feat_seq.values)\n",
    "\n",
    "    label_seq = scaled_counts.iloc[i + in_seq_len: i + in_seq_len + out_seq_len]\n",
    "    y_train.append(label_seq.values)\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1, in_seq_len)\n",
    "y_train = np.array(y_train).reshape(-1, out_seq_len)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54664b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# TODO: Hide some of the logic here as an excercise\n",
    "\n",
    "model = RNN_1()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-03)\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "batch_size = 30\n",
    "n_batches = n_samples // batch_size + 1\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(n_batches):\n",
    "        batch_start = batch * batch_size\n",
    "        batch_end = batch_start + batch_size\n",
    "        X_batch = X_train[batch_start: batch_end]\n",
    "        y_batch = y_train[batch_start: batch_end]\n",
    "\n",
    "        # Zero gradients at the start of each new batch\n",
    "        # Otherwise gradients are accumulated between batches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        # Backward pass computes gradients of all model weights\n",
    "        loss = loss_fn(y_batch, y_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust weights according to gradients\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the fitted values\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    fitted_values = model(X_train)\n",
    "\n",
    "fitted_values = fitted_values.numpy().reshape(-1)\n",
    "fitted_values = pd.Series(fitted_values, index=scaled_counts.iloc[in_seq_len:].index)\n",
    "fitted_values = scaler.inverse_transform(fitted_values)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cycle_counts.index, cycle_counts[\"count\"], lw=1.5)\n",
    "ax.plot(fitted_values)\n",
    "ax.set(title=LOCATION, ylabel=\"Count\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c73ff",
   "metadata": {},
   "source": [
    "### Forecast multiple timesteps ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove some logic to make an excercise\n",
    "\n",
    "class RNN_2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_seq_length: int,\n",
    "        out_seq_length: int,\n",
    "        input_size: int = 1,\n",
    "        hidden_size: int = 25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_seq_length = in_seq_length\n",
    "        self.out_seq_length = out_seq_length\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        self.hh = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.ho = nn.Linear(in_features=hidden_size, out_features=out_seq_length)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape to (batch_size, seq_len)\n",
    "        x = x.view(-1, self.in_seq_length)\n",
    "        \n",
    "        # Initialise the hidden state\n",
    "        h = torch.zeros(x.shape[0], self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        # Collect out sequence at every timestep of the input seq\n",
    "        outs = []\n",
    "\n",
    "        # Loop over every input in the input seq\n",
    "        for i in range(self.in_seq_length):\n",
    "            h = F.relu(self.ih(x[:, i].view(-1, self.input_size)) + self.hh(h))\n",
    "            outs.append(self.ho(h))\n",
    "\n",
    "        outs = torch.stack(outs, dim=1)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()\n",
    "scaled_counts = scaler.fit_transform(cycle_counts[\"count\"])\n",
    "\n",
    "in_seq_length, out_seq_length = 120, 21\n",
    "X_train, y_train = get_train_dataset(\n",
    "    scaled_counts,\n",
    "    in_seq_length,\n",
    "    out_seq_length,\n",
    "    batched_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af03670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "model = RNN_2(in_seq_length=in_seq_length, out_seq_length=out_seq_length)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-03)\n",
    "\n",
    "n_epochs = 300\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "batch_size = 30\n",
    "n_batches = n_samples // batch_size + 1\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in range(n_batches):\n",
    "        batch_start = batch * batch_size\n",
    "        batch_end = batch_start + batch_size\n",
    "        X_batch = X_train[batch_start: batch_end]\n",
    "        y_batch = y_train[batch_start: batch_end]\n",
    "\n",
    "        # Zero gradients at the start of each new batch\n",
    "        # Otherwise gradients are accumulated between batches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        y_hat = model(X_batch)\n",
    "\n",
    "        # Backward pass computes gradients of all model weights\n",
    "        loss = loss_fn(y_batch, y_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust weights according to gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc64053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted values\n",
    "X_test, test_index = get_test_dataset_and_index(\n",
    "    timeseries=scaled_counts,\n",
    "    in_seq_length=in_seq_length,\n",
    "    out_seq_length=out_seq_length,\n",
    "    batched_input=False,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "outputs = outputs[:, -1, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "fitted_values = []\n",
    "for i, out_seq in enumerate(outputs):\n",
    "    out_values = pd.Series(out_seq, index=test_index[i])\n",
    "    out_values = scaler.inverse_transform(out_values)\n",
    "    fitted_values.append(out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d43ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(cycle_counts.index[100:], cycle_counts[\"count\"][100:], lw=1.5, label=\"Observed\")\n",
    "for i, values in enumerate(fitted_values):\n",
    "    ax.plot(values, color=\"tab:orange\", label=\"Fitted\" if i == 0 else \"\")\n",
    "\n",
    "ax.legend(loc=1)\n",
    "ax.set(title=LOCATION, ylabel=\"Count\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b5b41",
   "metadata": {},
   "source": [
    "### Multilayer RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c03a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove some logic to make an excercise\n",
    "\n",
    "class RNN_3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_seq_length: int,\n",
    "        out_seq_length: int,\n",
    "        input_size: int = 1,\n",
    "        hidden_size: int = 25,\n",
    "        num_layers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_seq_length = in_seq_length\n",
    "        self.out_seq_length = out_seq_length\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.ho = nn.Linear(in_features=hidden_size, out_features=out_seq_length)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out_, _= self.rnn(x)\n",
    "        return self.ho(out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148610e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()\n",
    "scaled_counts = scaler.fit_transform(cycle_counts[\"count\"])\n",
    "\n",
    "in_seq_length, out_seq_length = 120, 21\n",
    "X_train, y_train = get_train_dataset(\n",
    "    scaled_counts,\n",
    "    in_seq_length=in_seq_length,\n",
    "    out_seq_length=out_seq_length,\n",
    "    batched_input=True,\n",
    "    batched_output=True,\n",
    ")\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf410377",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_3(in_seq_length=in_seq_length, out_seq_length=out_seq_length, num_layers=3)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-03)\n",
    "\n",
    "model.train()\n",
    "\n",
    "n_epochs = 300\n",
    "for epoch in range(n_epochs):\n",
    "    for X_train, y_train in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X_train)\n",
    "        loss = loss_fn(y_train, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2749fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted values\n",
    "X_test, test_index = get_test_dataset_and_index(\n",
    "    timeseries=scaled_counts,\n",
    "    in_seq_length=in_seq_length,\n",
    "    out_seq_length=out_seq_length,\n",
    "    batched_input=True,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "outputs = outputs[:, -1, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "fitted_values = []\n",
    "for i, out_seq in enumerate(outputs):\n",
    "    out_values = pd.Series(out_seq, index=test_index[i])\n",
    "    out_values = scaler.inverse_transform(out_values)\n",
    "    fitted_values.append(out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71092ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(cycle_counts.index[100:], cycle_counts[\"count\"][100:], lw=1.5, label=\"Observed\")\n",
    "for i, values in enumerate(fitted_values):\n",
    "    ax.plot(values, color=\"tab:orange\", label=\"Fitted\" if i == 0 else \"\")\n",
    "\n",
    "ax.legend(loc=1)\n",
    "ax.set(title=LOCATION, ylabel=\"Count\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444e81d",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove some logic to make an excercise\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_seq_length: int,\n",
    "        input_size: int = 1,\n",
    "        hidden_size: int = 25,\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_seq_length = out_seq_length\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=out_seq_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "scaler = Scaler()\n",
    "scaled_counts = scaler.fit_transform(cycle_counts[\"count\"])\n",
    "\n",
    "in_seq_length, out_seq_length = 120, 21\n",
    "X_train, y_train = get_train_dataset(\n",
    "    scaled_counts,\n",
    "    in_seq_length=in_seq_length,\n",
    "    out_seq_length=out_seq_length,\n",
    "    batched_input=True,\n",
    "    batched_output=True,\n",
    ")\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = LSTM(out_seq_length=out_seq_length)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-03)\n",
    "\n",
    "model.train()\n",
    "\n",
    "n_epochs = 300\n",
    "for epoch in range(n_epochs):\n",
    "    for X_train, y_train in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X_train)\n",
    "        loss = loss_fn(y_train, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "        print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db440793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted values\n",
    "X_test, test_index = get_test_dataset_and_index(\n",
    "    timeseries=scaled_counts,\n",
    "    in_seq_length=in_seq_length,\n",
    "    out_seq_length=out_seq_length,\n",
    "    batched_input=True,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "outputs = outputs[:, -1, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform\n",
    "fitted_values = []\n",
    "for i, out_seq in enumerate(outputs):\n",
    "    out_values = pd.Series(out_seq, index=test_index[i])\n",
    "    out_values = scaler.inverse_transform(out_values)\n",
    "    fitted_values.append(out_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f327ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(cycle_counts.index[100:], cycle_counts[\"count\"][100:], lw=1.5, label=\"Observed\")\n",
    "for i, values in enumerate(fitted_values):\n",
    "    ax.plot(values, color=\"tab:orange\", label=\"Fitted\" if i == 0 else \"\")\n",
    "\n",
    "ax.legend(loc=1)\n",
    "ax.set(title=LOCATION, ylabel=\"Count\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "fig.tight_layout();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
