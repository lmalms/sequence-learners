{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb\n",
    "# https://github.com/Nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54576077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.distributions import NegativeBinomial, Normal\n",
    "\n",
    "from bikes.preprocess.preprocess import get_tensor_train_dataset, MeanScaler\n",
    "from bikes.evaluate.split import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(actual: pd.Series, predicted: pd.Series):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(actual, label=\"Observed\")\n",
    "    ax.plot(predicted, label=\"Predicted\")\n",
    "\n",
    "    ax.set(ylabel=\"Count\")\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout();\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dc97c",
   "metadata": {},
   "source": [
    "## DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2432261",
   "metadata": {},
   "source": [
    "### Distribution Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_normal_params(\n",
    "    output: torch.Tensor,\n",
    "    loc: torch.Tensor | None = None,\n",
    "    scale: torch.Tensor | None = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Scale model outputs back to original scale using standard scaling\n",
    "    \"\"\"\n",
    "    mean, std = torch.tensor_split(output, 2, dim=2)\n",
    "    std = F.softplus(std)\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        mean = mean * scale + loc\n",
    "        std = (std + 0.2) * scale\n",
    "    return mean, std\n",
    "\n",
    "class NormalDistributionLoss(nn.Module):\n",
    "    def __init__(self, scaling_fn):\n",
    "        super().__init__()\n",
    "        self.scaling_fn = scaling_fn\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        output: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        loc: torch.Tensor | None = None,\n",
    "        scale: torch.Tensor | None = None,\n",
    "    ):\n",
    "        mean, std = self.scaling_fn(output, loc, scale)\n",
    "        loss_dist = Normal(loc=mean, scale=std)\n",
    "        return (-loss_dist.log_prob(y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a4ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_nb_params_standard_scaling(\n",
    "    output: torch.Tensor,\n",
    "    loc: torch.Tensor | None = None,\n",
    "    scale: torch.Tensor | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scale model outputs back to original scale using standard scaling\n",
    "    \"\"\"\n",
    "    mu, alpha = torch.tensor_split(output, 2, dim=2)\n",
    "    mu = F.softplus(mu) + 1e-08\n",
    "    alpha = F.softplus(alpha) + 1e-08\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        mu = mu * scale + loc\n",
    "        alpha = alpha / (scale + 1.)\n",
    "    total_count = 1.0 / alpha\n",
    "    probs = (mu * alpha / (1.0 + mu * alpha)) + 1e-08\n",
    "    return total_count, probs\n",
    "\n",
    "\n",
    "def scale_nb_params_mean_scaling(\n",
    "    output: torch.Tensor,\n",
    "    loc: torch.Tensor | None = None,\n",
    "    scale: torch.Tensor | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scale model outputs back to original scale using mean scaling\n",
    "    \"\"\"\n",
    "    mu, alpha = torch.tensor_split(output, 2, dim=2)\n",
    "    mu = F.softplus(mu) + 1e-08\n",
    "    alpha = F.softplus(alpha) + 1e-08\n",
    "    if scale is not None:\n",
    "        mu = mu * scale\n",
    "        alpha = alpha / scale ** 0.5\n",
    "    total_count = 1.0 / alpha\n",
    "    probs = (mu * alpha / (1.0 + mu * alpha)) + 1e-08\n",
    "    return total_count, probs\n",
    "\n",
    "\n",
    "\n",
    "class NegBinomialDistributionLoss(nn.Module):\n",
    "    def __init__(self, scaling_fn):\n",
    "        super().__init__()\n",
    "        self.scaling_fn = scaling_fn\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        output: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        loc: torch.Tensor | None = None,\n",
    "        scale: torch.Tensor | None = None,\n",
    "    ):\n",
    "        total_count, probs = self.scaling_fn(output, loc, scale)\n",
    "        loss_dist = NegativeBinomial(total_count=total_count, probs=probs)\n",
    "        return (-loss_dist.log_prob(y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int = 2,\n",
    "        hidden_size: int = 25,\n",
    "        n_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_layers <= 1:\n",
    "            # Linear transformation only\n",
    "            layers = [nn.Linear(in_features=in_features, out_features=out_features)]\n",
    "        else:\n",
    "            # Input layer\n",
    "            layers = [nn.Linear(in_features=in_features, out_features=hidden_size), nn.ReLU()]\n",
    "            # Hidden layers\n",
    "            for _ in range(n_layers - 2):\n",
    "                layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size), nn.ReLU()]\n",
    "            # Output layer\n",
    "            layers += [nn.Linear(in_features=hidden_size, out_features=out_features)]\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class DeepAR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 1,\n",
    "        encoder_n_layers: int = 1,\n",
    "        encoder_hidden_size: int = 128,\n",
    "        encoder_dropout: float = 0.1,\n",
    "        decoder_output_size: int = 2,\n",
    "        decoder_hidden_size: int = 25,\n",
    "        decoder_n_layers: int = 1,\n",
    "        trajectory_samples: int = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=encoder_hidden_size,\n",
    "            num_layers=encoder_n_layers,\n",
    "            dropout=encoder_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Sequential decoder\n",
    "        self.n_samples = trajectory_samples\n",
    "        self.decoder = DeepARDecoder(\n",
    "            in_features=encoder_hidden_size,\n",
    "            out_features=decoder_output_size,\n",
    "            hidden_size=decoder_hidden_size,\n",
    "            n_layers=decoder_n_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_state, _ = self.encoder(x)\n",
    "        output = self.decoder(hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba971aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_counts = pd.read_csv(\"cycle_counts.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fda8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = \"Quay Street Eco Display Classic\"\n",
    "\n",
    "location_df = cycle_counts.loc[cycle_counts[\"location\"] == LOCATION].copy()\n",
    "location_df = location_df.set_index(\"date\").sort_index()\n",
    "train_df, test_df = train_test_split(location_df)\n",
    "y_train, y_test = train_df[\"count\"], test_df[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe86373",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_train.iloc[-500:], label=\"Observed\", lw=2)\n",
    "ax.set(ylabel=\"Count\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "ax.legend()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f87a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "scaler = MeanScaler()\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "\n",
    "out_seq_length = 1      # DeepAR is trained with 1-step ahead forecasts\n",
    "in_seq_length = 6 * 30  # 6 months of input sequence length\n",
    "\n",
    "ts = get_tensor_train_dataset(y_train, in_seq_length=in_seq_length, out_seq_length=out_seq_length)\n",
    "ts_scaled = get_tensor_train_dataset(y_train_scaled, in_seq_length=in_seq_length, out_seq_length=out_seq_length)\n",
    "dataloader = DataLoader(TensorDataset(*ts.tensors, *ts_scaled.tensors), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99af2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "deepar = DeepAR()\n",
    "loss_fn = NegBinomialDistributionLoss(scaling_fn=scale_nb_params_mean_scaling)\n",
    "optimizer = Adam(params=deepar.parameters(), lr=1e-03)\n",
    "\n",
    "deepar.train()\n",
    "iteration_loss = []\n",
    "n_epochs = 100\n",
    "pgbar = tqdm(range(n_epochs))\n",
    "for epoch in pgbar:\n",
    "    for X, y, X_scaled, y_scaled in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = deepar(X_scaled)\n",
    "        loss = loss_fn(output=y_hat, y=y, loc=None, scale=scaler.mean_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    iteration_loss.append(float(loss.detach()))\n",
    "    pgbar.set_description(f\"Epoch [{epoch + 1} / {n_epochs}] - Loss = {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iteration_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "horizon = len(y_test)\n",
    "n_samples = 100\n",
    "\n",
    "deepar.eval()\n",
    "with torch.no_grad():\n",
    "    forecast_distribution = []\n",
    "    for s in tqdm(range(n_samples)):\n",
    "        \n",
    "        X_test = y_train_scaled.iloc[-in_seq_length:].values\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "        X_test = X_test.view(-1, in_seq_length, 1)\n",
    "    \n",
    "        trajectory = []\n",
    "        for h in range(horizon):\n",
    "            output = deepar(X_test)\n",
    "            output = output[:, -1, :].view(-1, 1, 2)  # Only keep next out-of-sample prediction\n",
    "            \n",
    "            # Forecast by sampling from NB distribution\n",
    "            total_count, probs = scale_nb_params_mean_scaling(output, loc=None, scale=scaler.mean_)\n",
    "            distr = NegativeBinomial(total_count=total_count, probs=probs)\n",
    "            y_hat = distr.sample()\n",
    "            \n",
    "            # Save forecast\n",
    "            trajectory.append(y_hat)\n",
    "\n",
    "            # Append scaled forecast to input for next step\n",
    "            y_hat_scaled = y_hat / scaler.mean_\n",
    "            X_test = torch.cat((X_test, y_hat_scaled), dim=1)\n",
    "\n",
    "        trajectory = torch.cat(trajectory, dim=1)\n",
    "        forecast_distribution.append(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43280166",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = torch.cat(forecast_distribution, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e46c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_mean = torch.mean(forecasts, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_series = pd.Series(data=forecast_mean.flatten().numpy(), index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test, label=\"Observed\")\n",
    "plt.plot(forecast_series, label=\"Forecast\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
