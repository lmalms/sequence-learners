{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a89f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb\n",
    "# https://github.com/Nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506b73c",
   "metadata": {},
   "source": [
    "# DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3970eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import NegativeBinomial, Normal, Poisson\n",
    "from torch.distributions import constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f3ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_to_outputs(level):\n",
    "    qs = sum([[50-l/2, 50+l/2] for l in level], [])\n",
    "    output_names = sum([[f'-lo-{l}', f'-hi-{l}'] for l in level], [])\n",
    "\n",
    "    # Sort in increasing order\n",
    "    sort_idx = np.argsort(qs)\n",
    "    quantiles = np.array(qs)[sort_idx]\n",
    "    output_names = list(np.array(output_names)[sort_idx])\n",
    "    \n",
    "    # Add median by default\n",
    "    quantiles = np.concatenate([np.array([50]), quantiles])\n",
    "    quantiles = torch.Tensor(quantiles) / 100\n",
    "    output_names.insert(0, '-median')\n",
    "\n",
    "    return quantiles, output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84080ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_scale_decouple(output, loc=None, scale=None, eps: float = 0.2):\n",
    "    mean, std = output\n",
    "    std = F.softplus(std)\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        mean = (mean * scale) + loc\n",
    "        std = (std + eps) * scale\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def nbinomial_scale_decouple(output, loc=None, scale=None):\n",
    "    mu, alpha = output\n",
    "    mu = F.softplus(mu) + 1e-08\n",
    "    alpha = F.softplus(alpha) + 1e-08\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        mu = mu * scale + loc\n",
    "        alpha = alpha / (scale + 1.)\n",
    "\n",
    "    total_count = 1.0 / alpha\n",
    "    probs = (mu * alpha / (1.0 + mu * alpha)) + 1e-8\n",
    "    return total_count, probs\n",
    "\n",
    "\n",
    "def poisson_scale_decouple(output, loc=None, scale=None):\n",
    "    eps = 1e-10\n",
    "    rate, _ = output\n",
    "    if (loc is not None) and (scale is not None):\n",
    "        rate = (rate * scale) + loc\n",
    "    rate = F.softplus(rate) + eps\n",
    "    return (rate, )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d680b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistributionLoss\n",
    "\n",
    "class DistributionLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        distribution: str,\n",
    "        level=[80, 90],  # Confidence levels of prediction intervals\n",
    "        num_samples=1000,\n",
    "        return_params=False,\n",
    "        **distribution_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        qs, output_names = level_to_outputs(level)\n",
    "        qs = torch.Tensor(qs)\n",
    "        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n",
    "        self.output_names = output_names\n",
    "\n",
    "        available_distributions = dict(\n",
    "            Normal=Normal,\n",
    "            NegativeBinomial=NegativeBinomial,\n",
    "            Poisson=Poisson\n",
    "        )\n",
    "        scale_decouples = dict(\n",
    "            Normal=normal_scale_decouple,\n",
    "            NegativeBinomial=nbinomial_scale_decouple,\n",
    "            Poisson=poisson_scale_decouple,\n",
    "        )\n",
    "        param_names = dict(\n",
    "            Normal=[\"-loc\", \"-scale\"],\n",
    "            NegativeBinomial=[\"-total_count\", \"-logits\"], \n",
    "            Poisson=[\"-loc\"],\n",
    "        )\n",
    "        \n",
    "        assert distribution in available_distributions\n",
    "        self.distribution = distribution\n",
    "        self._base_distribution = available_distributions[distribution]\n",
    "        self.scale_decouple = scale_decouples[distribution]\n",
    "        self.param_names = param_names[distribution]\n",
    "        self.outputsize_multiplier = len(self.param_names)\n",
    "        self.num_samples = num_samples\n",
    "        self.return_params = return_params\n",
    "        if self.return_params:\n",
    "            self.output_names = self.output_names + self.param_names\n",
    "        self.distribution_kwargs = distribution_kwargs\n",
    "        \n",
    "    \n",
    "    def _domain_map(self, input: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Maps output of neural network to domain of distribution loss\n",
    "        \"\"\"\n",
    "        output = torch.tensor_split(input, self.outputsize_multiplier, dim=2)\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def get_distribution(self, distr_args, **distribution_kwargs) -> Distribution:\n",
    "        distr = self._base_distribution(*distr_args, **distribution_kwargs)\n",
    "        self.distr_mean = distr.mean\n",
    "        \n",
    "        if self.distribution in ('Poisson', 'NegativeBinomial'):\n",
    "              distr.support = constraints.nonnegative\n",
    "        return distr\n",
    "    \n",
    "    def sample(self, distr_args: torch.Tensor, num_samples=None):\n",
    "        if num_samples is None:\n",
    "            num_samples = self.num_samples\n",
    "        \n",
    "        # Instantiate Scale Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args, **self.distribution_kwargs)\n",
    "        samples = distr.sample(sample_shape=(num_samples,))\n",
    "        samples = samples.permute(1, 2, 3, 0)  # [samples, B, H, N] -> [B, H, N, samples]\n",
    "        \n",
    "        # Compute mean and quantiles\n",
    "        sample_mean = torch.mean(samples, dim=-1, keepdim=True)\n",
    "        quants = torch.quantile(samples, self.quantiles, dim=-1)\n",
    "        quants = quants.permute(1, 2, 3, 0)  # [Q, B, H, N] -> [B, H, N, Q]\n",
    "\n",
    "        return samples, sample_mean, quants\n",
    "    \n",
    "    def __call__(self, y: torch.Tensor, distr_args: torch.Tensor):\n",
    "        # Instantiate Scale Decoupled Distribution\n",
    "        distr = self.get_distribution(distr_args=distr_args, **self.distribution_kwargs)\n",
    "        loss_values = -distr.log_prob(y)\n",
    "        return loss_values.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1733b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_size: int,\n",
    "        hidden_layers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        if hidden_layers == 0:\n",
    "            # Linear transformation only\n",
    "            layers = [nn.Linear(in_features=in_features, out_features=out_features)]\n",
    "        else:\n",
    "            # Input layer\n",
    "            layers = [nn.Linear(in_features=in_features, out_features=hidden_size), nn.ReLU()]\n",
    "            # Hidden layers\n",
    "            for _ in range(hidden_layers - 2):\n",
    "                layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size), nn.ReLU()]\n",
    "            # Output layer\n",
    "            layers += [nn.Linear(in_features=hidden_size, out_features=out_features)]\n",
    "\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class DeepARNormal(nn.Module):\n",
    "    def __init__(\n",
    "        self,    \n",
    "        out_seq_length: int,\n",
    "        input_seq_length: int = 1,\n",
    "        input_size: int = 1,\n",
    "        encoder_n_layers: int = 2,\n",
    "        encoder_hidden_size: int = 128,\n",
    "        encoder_dropout: float = 0.1,\n",
    "        decoder_hidden_layers: int = 0,\n",
    "        decoder_hidden_size: int = 0,\n",
    "        decoder_output_size: int = 2,\n",
    "        trajectory_samples: int = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_seq_length = out_seq_length\n",
    "        self.inp_seq_length = input_seq_length\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # LSTM encoder\n",
    "        self.encoder_n_layers = encoder_n_layers\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.encoder_dropout = encoder_dropout\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.encoder_hidden_size,\n",
    "            num_layers=self.encoder_n_layers,\n",
    "            dropout=self.encoder_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.n_samples = trajectory_samples\n",
    "        self.decoder = DeepARDecoder(\n",
    "            in_features=encoder_hidden_size,\n",
    "            out_features=decoder_output_size,\n",
    "            hidden_size=decoder_hidden_size,\n",
    "            hidden_layers=decoder_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_state, _ = self.encoder(x)\n",
    "        output = self.decoder(hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17e6f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DeepARDecoder(in_features=5, out_features=3, hidden_size=20, hidden_layers=3)\n",
    "input = torch.rand((32, 6, 5))\n",
    "output = decoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c219d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar = DeepARNormal(out_seq_length=16)\n",
    "input = torch.randn(32, 21, 1)\n",
    "output = deepar(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f23815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
