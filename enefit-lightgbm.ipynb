{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57236,"databundleVersionId":6957715,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport lightgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\nDATA = pd.read_csv(f\"{DATA_PATH}/train.csv\", parse_dates=[\"datetime\"])\nDATA = DATA.astype(\n    {\n        \"county\": int, \n        \"product_type\": int, \n        \"is_business\": float, \n        \"is_consumption\": float\n    }\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and preprocess training data","metadata":{}},{"cell_type":"code","source":"# Feature engineering\ndef temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.assign(\n        month=df[\"datetime\"].dt.month.astype(float),\n        day=df[\"datetime\"].dt.day.astype(float),\n        weekday=df[\"datetime\"].dt.weekday.astype(float),\n        hour=df[\"datetime\"].dt.hour.astype(float),\n        is_weekend=(df[\"datetime\"].dt.weekday >= 5).astype(float)\n    )\n    return df\n\n\ndef fourier_features(df: pd.DataFrame) -> pd.DataFrame:\n    def fft(df: pd.DataFrame, column: str, period: float, max_freq: int = 1):\n        for freq in range(1, max_freq + 1):\n            frac_period = 2 * np.pi * np.array(df[column] / period)\n            df[f\"{column}_sin_{freq}\"] = np.sin(frac_period * freq)\n            df[f\"{column}_cos_{freq}\"] = np.cos(frac_period * freq)\n\n        return df\n    \n    df = fft(df, \"month\", 12, 2)\n    df = fft(df, \"weekday\", 7, 1)\n    df = fft(df, \"hour\", 24, 2)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Temporal features\nDATA = temporal_features(DATA)\n\n# Fourier features\nDATA = fourier_features(DATA)\n\n# Drop irrelevant cols\ncols_to_drop = [\"data_block_id\", \"row_id\", \"prediction_unit_id\"]\nDATA = DATA.drop(cols_to_drop, axis=1)\n\n# Drop missing target values\nDATA = DATA.dropna(subset=[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline models","metadata":{"execution":{"iopub.status.busy":"2023-11-06T21:24:35.720571Z","iopub.execute_input":"2023-11-06T21:24:35.720935Z","iopub.status.idle":"2023-11-06T21:24:36.038935Z","shell.execute_reply.started":"2023-11-06T21:24:35.720903Z","shell.execute_reply":"2023-11-06T21:24:36.037647Z"}}},{"cell_type":"code","source":"# Feature selection\nCATEGORICAL_FEATURES = [\"county\", \"product_type\"]\nFEATURES = [c for c in DATA.columns if c not in [\"target\", \"datetime\"]]\nLABEL = \"target\"\n\nprint(DATA[FEATURES].shape, DATA[LABEL].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://lightgbm.readthedocs.io/en/latest/Parameters.html\nTRAINING_PARAMS = {\n    'objective': 'regression',\n    'metric': 'mae',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = lightgbm.Dataset(\n    data=DATA[FEATURES],\n    label=DATA[LABEL],\n    feature_name=\"auto\",\n    categorical_feature=CATEGORICAL_FEATURES\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lightgbm.train(\n    TRAINING_PARAMS, \n    train_dataset,\n    num_boost_round=250,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train / val split\n# start_datetime, stop_datetime = DATASET[\"datetime\"].min(), DATASET[\"datetime\"].max()\n# total_days = int((stop_datetime - start_datetime).total_seconds() / 3600 / 24)\n\n# n_folds = 10\n# evaluation_days = 30\n# max_training_days = 180\n\n# start_evaluation_set = stop_datetime.floor(\"1D\") - n_folds * pd.Timedelta(days=evaluation_days)\n# start_training_set = start_evaluation_set - pd.Timedelta(days=max_training_days)\n\n# scores, iterations = [], []\n# for i in range(n_folds):\n#     val_start = start_evaluation_set + i * pd.Timedelta(days=evaluation_days)\n#     val_end = val_start + pd.Timedelta(days=evaluation_days)\n#     train_start = val_end - pd.Timedelta(days=max_training_days)\n    \n#     # Select and transform training data\n#     train = DATASET[(DATASET[\"datetime\"] >= train_start) & (DATASET[\"datetime\"] < val_start)]\n#     X_train, y_train = train[FEATURES], train[LABEL]\n#     train_lgbm = lightgbm.Dataset(data=X_train, label=y_train)\n    \n#     # Select and transform validation data\n#     val = DATASET[(DATASET[\"datetime\"] >= val_start) & (DATASET[\"datetime\"] <= val_end)]\n#     X_val, y_val = val[FEATURES], val[LABEL]\n#     val_lgbm = lightgbm.Dataset(X_val, y_val)\n    \n#     model = lightgbm.train(\n#         TRAINING_PARAMS, \n#         train_lgbm,\n#         num_boost_round=1000,\n#         valid_sets=[val_lgbm],\n#         callbacks=[lightgbm.callback.early_stopping(10)]\n#     )\n#     y_hat = model.predict(X_val)\n#     scores.append(np.mean(np.abs(y_hat - np.array(y_val))))\n#     iterations.append(model.best_iteration)\n\n# mean_score = np.mean(scores)\n# mean_iterations = np.mean(iterations)\n\n# print(mean_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Model","metadata":{}},{"cell_type":"code","source":"# %%time\n\n# # Maybe consider not training on the full dataset and only the last few months?\n# # Check what timestamps I actually need to make predictions for.\n# model = lightgbm.train(\n#     TRAINING_PARAMS, \n#     lightgbm.Dataset(DATASET[FEATURES], DATASET[LABEL]),\n#     num_boost_round=int(mean_iterations),\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"import enefit\n\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test_data(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.astype({\"county\": int, \"product_type\": int, \"is_business\": float, \"is_consumption\": float})\n    df = df.rename(columns={\"prediction_datetime\": \"datetime\"})\n    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n    df = df.drop([\"row_id\", \"prediction_unit_id\"], axis=1)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (test, _, _, _, _, _, _, sample_prediction) in iter_test:\n    # Preprocessing and feature selection\n    X_test = preprocess_test_data(test)\n\n    X_test = temporal_features(X_test)\n    X_test = fourier_features(X_test)\n    \n    # Predict on test set\n    y_hat = model.predict(X_test[FEATURES])\n    y_hat = np.maximum(y_hat, 0.0)\n    sample_prediction['target'] = y_hat\n    env.predict(sample_prediction)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}